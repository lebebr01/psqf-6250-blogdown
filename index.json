[{"authors":["admin"],"categories":null,"content":"\nThis course aims to give students an in depth exploration of using R for data science and statistical analysis. The course is hands on where students will spend time practicing what they learn with real data to explore real problems. Students will gain experience cleaning, manipulating, visualizing, describing, exploring, and analyzing data from various perspectives. Students will also explore the benefits of reproducible analyses using R markdown documents to weave statistical code with text. Additional topics such as version control, markdown, and bootstrap will be discussed. This course will not teach you exactly what to do for every analysis, rather will attempt to give you tools to accomplish general data tasks and practice answering questions with data.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://psqf6250.brandonlebeau.org/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"This course aims to give students an in depth exploration of using R for data science and statistical analysis. The course is hands on where students will spend time practicing what they learn with real data to explore real problems.","tags":null,"title":"","type":"authors"},{"authors":["brandon"],"categories":null,"content":"\nI\u0026rsquo;m interested in computational methods, longitudinal data, and statistical software development with R. You can see more about my interests on my website: https://brandonlebeau.org/.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a6b366d06474d85d9f788b8d18e8310d","permalink":"https://psqf6250.brandonlebeau.org/authors/brandon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/brandon/","section":"authors","summary":"I\u0026rsquo;m interested in computational methods, longitudinal data, and statistical software development with R. You can see more about my interests on my website: https://brandonlebeau.org/.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"Links directly to R code topics\n Reproducible Document Graphics Syntax R Basics ggplot2 Extensions Graphics Tips  ","date":1642032000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1642032000,"objectID":"2b0562cceadd5b9e8843464c13310145","permalink":"https://psqf6250.brandonlebeau.org/rcode/","publishdate":"2022-01-13T00:00:00Z","relpermalink":"/rcode/","section":"rcode","summary":"R Course Syntax","tags":null,"title":"R Syntax","type":"book"},{"authors":null,"categories":null,"content":"A list of the data used within the course.\n UFO Data EDU01 LongitudinalEx ECLS_6250  Data Description   ECLS_6250_school  Data Description    ","date":1613606400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1613606400,"objectID":"77c2d2ba0979ac9954ceb502de85c1ce","permalink":"https://psqf6250.brandonlebeau.org/data/","publishdate":"2021-02-18T00:00:00Z","relpermalink":"/data/","section":"data","summary":"Data for the course","tags":null,"title":"Data","type":"book"},{"authors":null,"categories":null,"content":"The course content will be organized by weeks. Each week will contain some text to:\n discuss the goals of the week the content to be covered relevant R syntax/notebook files pre-recorded videos.  Each week may also contain some information about assignments and links directly to those.\n","date":1611532800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611532800,"objectID":"d5be68294f12f9cfecf81ad87009adc6","permalink":"https://psqf6250.brandonlebeau.org/content/","publishdate":"2021-01-25T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Course Content","tags":null,"title":"Content","type":"book"},{"authors":null,"categories":null,"content":"Here you can view all of the course assignments for the semester. This will include the hands on assignments and the quizzes. The quizzes will provide a link to ICON to complete.\n","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610064000,"objectID":"5d370553e45c580541e007200292c8d8","permalink":"https://psqf6250.brandonlebeau.org/assignments/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/assignments/","section":"assignments","summary":"Course Requirements","tags":null,"title":"Course Requirements","type":"book"},{"authors":null,"categories":null,"content":"The course schedule for the semester. There will be a single page for each week of the course. These weeks will include all materials, highlight the course objectives for that week, include pre-recorded video lectures, include links to any assignments/quizzes that can be completed, lecture notes, and other external readings.\nOverview of Weekly Content  Week 1 - Rmarkdown / Reproducible Documents Week 2 - Graphics with ggplot2 Week 3 - Graphics with ggplot2 extensions  ","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610064000,"objectID":"ee8dfa775d2ca48042bf50fe0819e70c","permalink":"https://psqf6250.brandonlebeau.org/schedule/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/schedule/","section":"schedule","summary":"Course schedule","tags":null,"title":"Schedule","type":"book"},{"authors":null,"categories":null,"content":"This page contains the syllabus for the course. This syllabus is an attempt early in the semester to plan for the course. This syllabus is subject to change at the Instructors discretion.\nCourse Information PSQF 6250: Computer Packages for Statistical Analysis - Spring 2022\nInstructor Information  Brandon LeBeau, Ph.D. E-mail: brandon-lebeau at uiowa.edu Virtual Office Hours (Zoom): Tues 11 am to 12:30 pm or by appointment  See ICON for office hours zoom link   Department: Psychological and Quantitative Foundations, 361 LC  DEO: Dr. Foley Nicpon, 361 LC}    Course Quote Data does not give up their secrets easily. They must be tortured to confess. \u0026ndash; Jeff Hooper, Bell Labs\nCourse Description This course aims to give students an in depth exploration of using R for data science and statistical analysis. The course is hands on where students will spend time practicing what they learn with real data to explore real problems. Students will gain experience cleaning, manipulating, visualizing, describing, exploring, and analyzing data from various perspectives. Students will also explore the benefits of reproducible analyses using R markdown documents to weave statistical code with text. Additional topics such as version control, markdown, and Monte Carlo simulation will be discussed. This course will not teach you exactly what to do for every analysis, rather will attempt to give you tools to accomplish general data tasks and practice answering questions with data.\nCourse Objectives By the end of the course, students should be comfortable doing the following with R:\n data visualization data manipulation data joining descriptive analysis fitting linear models reproducible data analysis  Textbook No required textbook for purchase. There will be numerous online resources that will be used for the course. These are listed below and are posted on the ICON site. Note: You can click the links below and it will take you directly to the source on ICON.\nR  R for Data Science R Markdown: The Definitive Guide  Markdown  Markdown Cheatsheet R Markdown Cheatsheet  git  git - the simple guide or git cheatsheet  Course Requirements  Online Quizzes (50 pts): Online quizzes through ICON will be given roughly every week. These will test basic knowledge of statistical programs covered in the course. Each quiz will be worth 5 points with 10 total quizzes. Quizzes will generally be due on Sunday evenings. Assignments (50 pts): Homework assignments will be used to give hands on experience with the software. These homework assignments will give you an opportunity to answer questions with data, interpret results, and receive feedback on them. Each assignment will be worth 20 points with 5 total assignments. Students are able to work in groups of up to 3 for the course assignments. If students work in groups, all students will receive the same grade. Homework, including the R source code and written responses, will be submitted electronically on ICON. Final Project Proposal (5 pts): This is a short, one page, proposal for the type of project you hope to complete. The goal of the final project proposal is to give you a chance to think about the final project earlier in the semester to get started on it. More details on what is intended in this proposal will be shared on the course website. Final Project (25 pts): The final project gives you an opportunity to use data you are interested in to answer a question of interest. You will be expected to use descriptive statistics, fit a statistical model, and create at least one figure. More specific details will be shared on the course website.  Grades   Grading: Final grades will be based on the following weighting scheme:\n Quizzes: 50 points (5 points each \u0026ndash; 10 quizzes) Assignments: 50 points (10 points each \u0026ndash; 5 assignments) Final Project Proposal: 5 pts Final Project: 25 pts    Percentage Breakdown: Guidelines are given below, plus and minus grades will be given as well. If you wish to be graded on an S/U basis, please send me an email before the last day of classes (May 8, 2022).\n  A: 90% or better\n  B: 80% up to 90%\n  C: 70% up to 80%\n  D: 60% up to 70%\n  Course and University Policies  Absences: Absences happen. Therefore, I ask you to be as transparent as possible with me. I promise to be compassionate and understanding. If at any point in the semester you are having difficulties, please reach out to me and I will do my best to be accommodating and provide support, which could include an extension on course deadlines as necessary. Announcements and Communication: Any announcements regarding the course will be communicated via e-mail so please check it daily. Course materials will be posted to ICON. Go to icon.uiowa.edu for access to the ICON site. Adaptations and Modifications: Please inform me during the first two weeks if you require special adaptations or modifications to any assignment or due dates because of special circumstances such as learning disabilities, religious observances, or other appropriate needs. Contesting a Grade: To contest a grade, please send me an e-mail detailing your reason within 48 hours of receiving the grade. This allows both of us time to think, reflect, and discuss the matter without taking class time from other students. When contesting a grade, provide a copy of the graded assignment. Academic Misconduct: Plagiarism and cheating may result in grade reduction and/or serious penalties. Unless you are otherwise instructed, your work should be entirely your own. Please take care in writing your final project. You should always be writing in your own words, citing others' ideas, and quoting text as appropriate. This link provides the College of Education policy on student academic misconduct (plagiarism and cheating) https://education.uiowa.edu/coe-policies/student-academic-misconduct. Free Speech and Expression: The University of Iowa supports and upholds the First Amendment protection of freedom of speech and the principles of academic and artistic freedom. We are committed to open inquiry, vigorous debate, and creative expression inside and outside of the classroom. Visit the Free Speech at Iowa website for more information on the university’s policies on free speech and academic freedom. Accommodations for Students with Disabilities: The University is committed to providing an educational experience that is accessible to all students. If a student has a diagnosed disability or other disabling condition that may impact the student’s ability to complete the course requirements as stated in the syllabus, the student may seek accommodations through Student Disability Services (SDS). SDS is responsible for making Letters of Accommodation (LOA) available to the student. The student must provide a LOA to the instructor as early in the semester as possible, but requests not made at least two weeks prior to the scheduled activity for which an accommodation is sought may not be accommodated. The LOA will specify what reasonable course accommodations the student is eligible for and those the instructor should provide. Additional information can be found on the SDS website. Absences for Religious Holy Days: The University is prepared to make reasonable accommodations for students whose religious holy days coincide with their classroom assignments, test schedules, and classroom attendance expectations. Students must notify their instructors in writing of any such Religious Holy Day conflicts or absences within the first few days of the semester or session, and no later than the third week of the semester. If the conflict or absence will occur within the first three weeks of the semester, the student should notify the instructor as soon as possible. See Operations Manual 8.2 Absences for Religious Holy Days for additional information. Classroom Expectations: Students are expected to comply with University policies regarding appropriate classroom behavior as outlined in the Code of Student Life. While students have the right to express themselves and participate freely in class, it is expected that students will behave with the same level of courtesy and respect in the virtual class setting (whether asynchronous or synchronous) as they would in an in-person classroom. Failure to follow behavior expectations as outlined in the Code of Student Life may be addressed by the instructor and may also result in discipline under the Code of Student Life policies governing E.5 Disruptive Behavior or E.6 Failure to Comply with University Directive. Non-Discrimination Statement: The University of Iowa prohibits discrimination in employment, educational programs, and activities on the basis of race, creed, color, religion, national origin, age, sex, pregnancy, disability, genetic information, status as a U.S. veteran, service in the U.S. military, sexual orientation, gender identity, associational preferences, or any other classification that deprives the person of consideration as an individual. The university also affirms its commitment to providing equal opportunities and equal access to university facilities. For additional information on nondiscrimination policies, contact the Director, Office of Institutional Equity, the University of Iowa, 202 Jessup Hall, Iowa City, IA 52242-1316, 319-335-0705, oie-ui@uiowa.edu. Students may share their pronouns and chosen/preferred names in MyUI, which is accessible to instructors and advisors. Sexual Harassment/Sexual Misconduct and Supportive Measures: The University of Iowa prohibits all forms of sexual harassment, sexual misconduct, and related retaliation. The Policy on Sexual Harassment and Sexual Misconduct governs actions by students, faculty, staff and visitors. Incidents of sexual harassment or sexual misconduct can be reported to the Title IX and Gender Equity Office or to the Department of Public Safety. Students impacted by sexual harassment or sexual misconduct may be eligible for academic supportive measures and can learn more by contacting the Title IX and Gender Equity Office. Information about confidential resources can be found here. Watch the video for an explanation of these resources. Mental Health: Students are encouraged to be mindful of their mental health and seek help as a preventive measure or if feeling overwhelmed and/or struggling to meet course expectations. Students are encouraged to talk to their instructor for assistance with specific class-related concerns. For additional support and counseling, students are encouraged to contact University Counseling Service (UCS). Information about UCS, including resources and how to schedule an appointment, can be found at http://counseling.uiowa.edu. Find out more about UI mental health services at: http://mentalhealth.uiowa.edu. Basic Needs and Support for Students: Student Care \u0026amp; Assistance provides assistance to University of Iowa students experiencing a variety of crisis and emergency situations, including but not limited to medical issues, family emergencies, unexpected challenges, and sourcing basic needs such as food and shelter. More information on the resources related to basic needs can be found at: https://basicneeds.uiowa.edu/resources/. Students are encouraged to contact Student Care \u0026amp; Assistance in the Office of the Dean of Students (Room 135 IMU, dos-assistance@uiowa.edu or 319-335-1162) for support and assistance with resources.  ","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610064000,"objectID":"5538b9800f06ab7b29edaa22ab7e63cb","permalink":"https://psqf6250.brandonlebeau.org/syllabus/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/syllabus/","section":"syllabus","summary":"Course syllabus","tags":null,"title":"Syllabus","type":"book"},{"authors":null,"categories":null,"content":"Note: No penalty for late submissions, all assignments due by May 8th, 2022.\n Assignment 1 - Due around February 13th. Assignment 2 - Due around February 28th. Assignment 3 - Due around March 27th. Assignment 4 - Due around April 10th.  ","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610064000,"objectID":"148b0563112c2a9006e85ec08153b0ce","permalink":"https://psqf6250.brandonlebeau.org/assignments/assignment/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/assignments/assignment/","section":"assignments","summary":"Course Assignments","tags":null,"title":"Assignments","type":"book"},{"authors":null,"categories":null,"content":"Below are the course quizzes.\n Quiz 1 - Due January 30th Quiz 2 - Due February 6th Quiz 3 - Due February 20th Quiz 4 - Due March 13th Quiz 5 - Due March 28th  ","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610064000,"objectID":"9ee2f1578158bc73334dba690602f1a2","permalink":"https://psqf6250.brandonlebeau.org/assignments/quizzes/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/assignments/quizzes/","section":"assignments","summary":"Course Quizzes","tags":null,"title":"Quizzes","type":"book"},{"authors":null,"categories":null,"content":"The course project are detailed in the following pages, one for the project proposal and another for the final project requirements.\n Project Proposal Course Project  ","date":1610064000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610064000,"objectID":"99f302b9e9cee92740f853ade9eda986","permalink":"https://psqf6250.brandonlebeau.org/assignments/project/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/assignments/project/","section":"assignments","summary":"Course Project","tags":null,"title":"Project","type":"book"},{"authors":null,"categories":null,"content":"   We are going to start by exploring graphics with R using the midwest data. To access this data, run the following commands:\ninstall.packages(\u0026quot;tidyverse\u0026quot;) library(tidyverse) Suppose we were interested in exploring the question: How does population density influence the percentage of the population with at least a college degree? Let’s explore these data closer.\nmidwest ## # A tibble: 437 × 28 ## PID county state area poptotal popdensity popwhite popblack popamerindian ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 561 ADAMS IL 0.052 66090 1271. 63917 1702 98 ## 2 562 ALEXAN… IL 0.014 10626 759 7054 3496 19 ## 3 563 BOND IL 0.022 14991 681. 14477 429 35 ## 4 564 BOONE IL 0.017 30806 1812. 29344 127 46 ## 5 565 BROWN IL 0.018 5836 324. 5264 547 14 ## 6 566 BUREAU IL 0.05 35688 714. 35157 50 65 ## 7 567 CALHOUN IL 0.017 5322 313. 5298 1 8 ## 8 568 CARROLL IL 0.027 16805 622. 16519 111 30 ## 9 569 CASS IL 0.024 13437 560. 13384 16 8 ## 10 570 CHAMPA… IL 0.058 173025 2983. 146506 16559 331 ## # … with 427 more rows, and 19 more variables: popasian \u0026lt;int\u0026gt;, popother \u0026lt;int\u0026gt;, ## # percwhite \u0026lt;dbl\u0026gt;, percblack \u0026lt;dbl\u0026gt;, percamerindan \u0026lt;dbl\u0026gt;, percasian \u0026lt;dbl\u0026gt;, ## # percother \u0026lt;dbl\u0026gt;, popadults \u0026lt;int\u0026gt;, perchsd \u0026lt;dbl\u0026gt;, percollege \u0026lt;dbl\u0026gt;, ## # percprof \u0026lt;dbl\u0026gt;, poppovertyknown \u0026lt;int\u0026gt;, percpovertyknown \u0026lt;dbl\u0026gt;, ## # percbelowpoverty \u0026lt;dbl\u0026gt;, percchildbelowpovert \u0026lt;dbl\u0026gt;, percadultpoverty \u0026lt;dbl\u0026gt;, ## # percelderlypoverty \u0026lt;dbl\u0026gt;, inmetro \u0026lt;int\u0026gt;, category \u0026lt;chr\u0026gt; This will bring up the first 10 rows of the data (hiding the additional 8,592) rows. A first common step to explore our research question is to plot the data. To do this we are going to use the R package, ggplot2, which was installed when running the install.packages command above. You can explore the midwest data by calling up the help file as well with ?midwest.\nCreate a ggplot To plot these two variables from the midwest data, we will use the function ggplot and geom_point to add a layer of points. We will treat popdensity as the x variable and percollege as the y variable.\nggplot(data = midwest) + geom_point(mapping = aes(x = popdensity, y = percollege)) Examples Try plotting popdensity by state. Try plotting county by state. Does this plot work? Bonus: Try just using the ggplot(data = midwest) from above. What do you get? Does this make sense?  Note: You should be able to modify the structure of the code above to do this.\n  Add Aesthetics Aesthetics are a way to explore more complex interactions within the data. Particularly, from the above example, lets add in the state variable to the plot via an aesthetic.\nggplot(data = midwest) + geom_point(mapping = aes(x = popdensity, y = percollege, color = state)) As you can see, we simply colored the points by the state they belong in. Does there appear to be a trend?\nExamples Using the same aesthetic structure as above, instead of using colors, make the shape of the points different for each state. Instead of color, use alpha instead. What does this do to the plot?    Global Aesthetics Above, we specified a variable to an aesthetic, which is a common use of aesthetics. However, the aesthetics can also be assigned globally. Here are two examples using the first scatterplot created.\nggplot(data = midwest) + geom_point(mapping = aes(x = popdensity, y = percollege), color = \u0026#39;pink\u0026#39;) ggplot(data = midwest) + geom_point(mapping = aes(x = popdensity, y = percollege), shape = 15) These two plots changed the aesthetics for all of the points. Notice, the suttle difference between the code for these plots and that for the plot above. The placement of the aesthetic is crucial, if it is within the parentheses for aes() then it should be assigned a variable. If it is outside, as in the last two examples, it will define the aesthetic for all the data.\nExamples Try the following command: colors(). This will print a vector of all the color names within R, try a few to find your favorites. What happens if you use the following code:  ggplot(data = midwest) + geom_point(mapping = aes(x = popdensity, y = percollege, color = \u0026#39;green\u0026#39;)) What is the problem?\n  Facets Instead of defining an aesthetic to change the color or shape of points by a third variable, we can also plot each groups data in a single plot and combine them. The process is easy with ggplot2 by using facets.\nggplot(data = midwest) + geom_point(mapping = aes(x = popdensity, y = percollege)) + facet_grid(. ~ state) You can also use facet_wrap.\nggplot(data = midwest) + geom_point(mapping = aes(x = popdensity, y = percollege)) + facet_wrap(~ state) Examples Can you facet with a continuous variable? Try it!    Geoms ggplot2 uses a grammar of graphics which makes it easy to switch different plot types (called geoms) once you are comfortable with the basic syntax. For example, how does the following plot differ from the scatterplot first generated above? What is similar?\nggplot(data = midwest) + geom_smooth(mapping = aes(x = popdensity, y = percollege)) We can also do this plot by states\nggplot(data = midwest) + geom_smooth(mapping = aes(x = popdensity, y = percollege, linetype = state), se = FALSE) What about the code above gave me the different lines for each state? Note, I also removed the standard error shading from the plot as well.\nExamples It is possible to combine geoms, which we will do next, but try it first. Try to recreate this plot.     Combining multiple geoms Combining more than one geom into a single plot is relatively straightforward, but a few considerations are important. Essentially to do the task, we just simply need to combine the two geoms we have used:\nggplot(data = midwest) + geom_point(aes(x = popdensity, y = percollege, color = state)) + geom_smooth(mapping = aes(x = popdensity, y = percollege, color = state), se = FALSE) A couple points about combining geoms, first, the order matters. In the above example, we called geom_point first, then geom_smooth. When plotting these data, the points will then be plotted first followed by the lines. Try flipping the order of the two geoms to see how the plot differs.\nWe can also simplify this code to not duplicate typing:\nggplot(data = midwest, mapping = aes(x = popdensity, y = percollege, color = state)) + geom_point() + geom_smooth(se = FALSE) Examples Can you recreate the following figure?     Other geom examples There are many other geoms available to use. To see them all, visit https://ggplot2.tidyverse.org/reference/index.html which gives examples of all the possibilities. This is a handy resource that I keep going back to.\nGeoms for single variables The introduction to plotting has been with two variables, but lets take a step back and focus on one variable with a bar chart.\nggplot(data = midwest, mapping = aes(x = state)) + geom_bar() You can also easily add aesthetics this base plot as shown before.\nggplot(data = midwest, mapping = aes(x = state)) + geom_bar(aes(fill = factor(inmetro))) A few additions can help interpretation of this plot:\nggplot(data = midwest, mapping = aes(x = state)) + geom_bar(aes(fill = factor(inmetro)), position = \u0026#39;fill\u0026#39;) ggplot(data = midwest, mapping = aes(x = state)) + geom_bar(aes(fill = factor(inmetro)), position = \u0026#39;dodge\u0026#39;) It is also possible to do a histrogram of a quantitative variable:\nggplot(data = midwest, mapping = aes(x = popdensity)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. You can adjust the binwidth directly:\nggplot(data = midwest, mapping = aes(x = popdensity)) + geom_histogram(binwidth = 1000)  Examples With more than two groups, histograms are difficult to interpret due to overlap. Instead, use the geom_density to create a density plot for popdensity for each state. The final plot should look similar to this:  Using geom_boxplot, create boxplots with popdensity as the y variable and state as the x variable. Bonus: facet this plot by the variable inmetro.     Plot Customization There are many many ways to adjust the look of the plot, I will discuss a few that are common.\nChange axes Axes are something that are commonly altered, particularly to give them a good name and also to alter the values shown on the axes. These are generally done with scale_x_* and scale_y_* where * is a filler based on the type of variable on the axes.\nFor example:\nggplot(data = midwest, mapping = aes(x = popdensity, y = percollege, color = state)) + geom_point() + scale_x_continuous(\u0026quot;Population Density\u0026quot;) + scale_y_continuous(\u0026quot;Percent College Graduates\u0026quot;) To change the legend title, the scale_color_discrete command can be used to adjust the color aesthetic and the variable is discrete.\nggplot(data = midwest, mapping = aes(x = popdensity, y = percollege, color = state)) + geom_point() + scale_x_continuous(\u0026quot;Population Density\u0026quot;) + scale_y_continuous(\u0026quot;Percent College Graduates\u0026quot;) + scale_color_discrete(\u0026quot;State\u0026quot;) we can also alter the breaks showing on the x-axis.\nggplot(data = midwest, mapping = aes(x = popdensity, y = percollege, color = state)) + geom_point() + scale_x_continuous(\u0026quot;Population Density\u0026quot;, breaks = seq(0, 80000, 20000)) + scale_y_continuous(\u0026quot;Percent College Graduates\u0026quot;) + scale_color_discrete(\u0026quot;State\u0026quot;)   Zoom in on plot You’ll notice that there are outliers in this scatterplot due to larger population density values for some counties. It may be of interest to zoom in on the plot. The plot can be zoomed in by using the coord_cartesian command as follows.\nggplot(data = midwest, mapping = aes(x = popdensity, y = percollege, color = state)) + geom_point() + scale_x_continuous(\u0026quot;Population Density\u0026quot;) + scale_y_continuous(\u0026quot;Percent College Graduates\u0026quot;) + scale_color_discrete(\u0026quot;State\u0026quot;) + coord_cartesian(xlim = c(0, 15000)) Note: This can also be achieved using the xlim argument to scale_x_continuous above, however this will cause some points to not be plotted. In this case it would not be a huge deal, however, if we plotted the smooth lines from before you can see the difference.\nggplot(data = midwest, mapping = aes(x = popdensity, y = percollege, color = state)) + geom_point() + geom_smooth(se = FALSE) + scale_x_continuous(\u0026quot;Population Density\u0026quot;) + scale_y_continuous(\u0026quot;Percent College Graduates\u0026quot;) + scale_color_discrete(\u0026quot;State\u0026quot;) + coord_cartesian(xlim = c(0, 15000)) ggplot(data = midwest, mapping = aes(x = popdensity, y = percollege, color = state)) + geom_point() + geom_smooth(se = FALSE) + scale_x_continuous(\u0026quot;Population Density\u0026quot;, limits = c(0, 15000)) + scale_y_continuous(\u0026quot;Percent College Graduates\u0026quot;) + scale_color_discrete(\u0026quot;State\u0026quot;) ## Warning: Removed 16 rows containing non-finite values (stat_smooth). ## Warning: Removed 16 rows containing missing values (geom_point).  ","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"5cd35ce41d8b7dd43e2b3a7c1a480ee0","permalink":"https://psqf6250.brandonlebeau.org/rcode/graphics/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/rcode/graphics/","section":"rcode","summary":"using ggplot2","tags":null,"title":"Graphics","type":"book"},{"authors":null,"categories":null,"content":"   In an attempt to get you “doing things” in R quickly, I’ve omitted a lot of discussion surrounding internal R workings. R is an object oriented language, this is much different than many other software languages.\nR works as a calculator R can be used as a calculator to do any type of addition, subtraction, multiplication, or division (among other things).\n1 + 2 - 3 ## [1] 0 5 * 7 ## [1] 35 2/1 ## [1] 2 sqrt(4) ## [1] 2 2^2 ## [1] 4 Being an object oriented system, values can directly saved within an object to be used later. As an example:\nx \u0026lt;- 1 + 3 x ## [1] 4 This can then be used later in other calculations:\nx * 3 ## [1] 12 This simplistic example is a bit too simple to show all the benefits of this approach, but will become more apparent when we start reading in data and doing more complicated data munging type tasks.\nNaming conventions This is a topic in which you will not get a single answer, but rather a different answer for everyone you ask. I prefer something called snake_case using underscores to separate words in an object. Others use titleCase as a way to distinguish words others yet use period.to.separate words in object names.\nThe most important thing is to be consistent. Pick a convention that works for you and stick with it through out. Avoiding this Mixed.TypeOf_conventions at all costs.\n  R is case sensitive This can cause problems and make debugging a bit more difficult. Be careful with typos and with case. Here is an example:\ncase_sensitive \u0026lt;- 10 Case_sensitive ## Error in eval(expr, envir, enclos) : object \u0026#39;Case_sensitive\u0026#39; not found  Functions We have already been using functions when working through creating graphics with R. A function consists of at least two parts, the function name and the arguments as follows: function_name(arg1 = num, arg2 = num). The arguments are always inside of parentheses, take on some value, and are always named. To call a function, use the function_name followed by parentheses with the arguments inside the parentheses. For example, using the rnorm function to generate values from a random normal distribution:\nset.seed(1) rnorm(n = 10, mean = 0, sd = 1) ## [1] -0.6264538 0.1836433 -0.8356286 1.5952808 0.3295078 -0.8204684 ## [7] 0.4874291 0.7383247 0.5757814 -0.3053884 Notice I called the arguments by name directly, this is good practice, however, this code will generate the same values (the values are the same because I’m using set.seed here):\nset.seed(1) rnorm(10, 0, 1) ## [1] -0.6264538 0.1836433 -0.8356286 1.5952808 0.3295078 -0.8204684 ## [7] 0.4874291 0.7383247 0.5757814 -0.3053884 The key when arguments are not called via their names is the order of the arguments. Look at ?rnorm to see that the first three arguments are indeed n, mean, and sd. When you name arguments, they can be specified in any order (generally bad practice).\nset.seed(1) rnorm(sd = 1, n = 10, mean = 0) ## [1] -0.6264538 0.1836433 -0.8356286 1.5952808 0.3295078 -0.8204684 ## [7] 0.4874291 0.7383247 0.5757814 -0.3053884 You can save this result to an object to be used later.\nset.seed(1) norm_values \u0026lt;- rnorm(n = 10, mean = 0, sd = 1) Notice the result is no longer printed to the screen, but rather is saved to the object norm_values. To see the result, you could just type norm_values in the console.\n Errors Lastly, I want to discuss errors. Errors are going to happen. Even the best programmers encounter errors that they did not anticipate and debugging needs to happen. If you encounter an error I recommend doing the following few things first:\nUse ?function_name to explore the details of the function. The examples at the bottom of every R help page can be especially helpful.\n If this does not help, copy and paste the error and search on the internet. Chances are someone else has had this error and has asked how to fix it. This is how I fix most errors I am unable to figure out with the R help.\n If these two steps still do not help, feel free to email me, but take the time to do steps 1 and 2. If you do email me, please include the following things:\n The error message directly given from R A reproducible example of the code. The reproducible example is one in which I can run the code directly with no modifications. Without this, it is much more difficult if not impossible for me to help without asking for more information.    ","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"98d43ea826bd8e4faa445fd73e95bb1b","permalink":"https://psqf6250.brandonlebeau.org/rcode/r-basics/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/rcode/r-basics/","section":"rcode","summary":"R Basics","tags":null,"title":"R Basics","type":"book"},{"authors":null,"categories":null,"content":"Getting Started  Review the syllabus Review the schedule Install R (or use the RStudio Cloud link) - RStudio Cloud Optionally, install RSTudio  Weekly Videos  Course Overview    Course Logistics   ","date":1611532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611532800,"objectID":"1573444049fbe3c4964b394aa4c5174c","permalink":"https://psqf6250.brandonlebeau.org/content/00-getting-started/","publishdate":"2021-01-25T00:00:00Z","relpermalink":"/content/00-getting-started/","section":"content","summary":"Getting Started","tags":null,"title":"Welcome","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week will introduce you to Rmarkdown documents, markdown syntax, and discussion of installing R/RStudio.\nObjectives After completing this module, students will be able to:\n Define reproducible research Demonstrate code and markdown chunks Create a reproducible document template  Activities  Read R for Data Science Textbook - chapter 27 Optional reading, Reproducible Research in Education - LeBeau, Ellison \u0026amp; Aloe, in press Basic Markdown Guide Extended Markdown Syntax  Weekly Videos  Introduction to Reproducible Research    Dynamic Documents (part 1)    Dynamic Documents (part 2)   Syntax  Raw syntax for reproducible document (Rmd file)  Assignments None this week.\n","date":1611532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611532800,"objectID":"d1fbf25a3d7006b7a626817f4e8207b2","permalink":"https://psqf6250.brandonlebeau.org/content/01-week1/","publishdate":"2021-01-25T00:00:00Z","relpermalink":"/content/01-week1/","section":"content","summary":"Reproducible Research","tags":null,"title":"Week 1","type":"book"},{"authors":null,"categories":null,"content":"   The ggplot2 package has a robust ecosystem of many other packages that extend the functionality of ggplot2. This week, we are going to explore some of these packages in more detail, highlighting a few packages that give you additional ways to create stunning visualizations. You can see all of the extensions packages in the following ggplot2 extension website.\nWe are going to spend some time with the following packages:\n ggrepel ggforce patchwork  I also plan to discuss, gganimate, but we are going to come back to this later in the course when talking about interactive graphics.\nAll of these packages are on CRAN and you can install with the following command:\ninstall.packages(c(\u0026quot;ggrepel\u0026quot;, \u0026quot;ggforce\u0026quot;, \u0026quot;patchwork\u0026quot;)) ggrepel Let’s start by exploring the ggrepel package. This package is particularly useful when working with text labels and provides some algorithms to help with text label placement automatically. One challenge when placing text labels in a figure is that they often overlap and they also often are placed on top of the data too. ggrepel helps to solve this problem.\nTo show a motivating example, we are going to use data in this section based on penguins. To do this, we first need to install this data package.\ninstall.packages(\u0026quot;palmerpenguins\u0026quot;) The data include three different species of penguins originally collected by Dr. Kristen Gorman at the Palmer Station in Antarctica. There are a total of 344 penguins collected from 3 islands in Antarctica and include information about the species, which island, penguin measurements, and the sex of the penguin. More information about the data including artwork about the species and penguin measurements are on this page.\nHere are the penguin species and what the measurements mean, “artwork by @allison_horst”.\nlibrary(palmerpenguins) library(ggplot2) penguins ## # A tibble: 344 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## 7 Adelie Torgersen 38.9 17.8 181 3625 ## 8 Adelie Torgersen 39.2 19.6 195 4675 ## 9 Adelie Torgersen 34.1 18.1 193 3475 ## 10 Adelie Torgersen 42 20.2 190 4250 ## # … with 334 more rows, and 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt; Suppose we wanted to explore the bill length and flipper length with a scatter plot. We can do that with ggplot2 using the geom_point() function. I’m also using the theme_set() function to set the theme to be theme_bw() for the remainder of the notebook. I’ve also altered the theme settings by increasing the base font size from 12 to 16 so hopefully it is a bit easier to read the figure.\ntheme_set(theme_bw(base_size = 16)) ggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 4) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) Suppose we wished to add the species to this figure. More specifically, we want to add the species information to the points in the figure to label which points below to each penguin species. There are a few ways we could do this, we could do this by color, shape, or both.\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 4, aes(color = species, shape = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) Another potential option would be to add the text labels directly to the figure and not use color. Adding text to a figure is typically done with the geom_text() function.\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 4, aes(shape = species)) + geom_text(aes(label = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) Notice how the text labels overlap and the word is centered with the data point? This makes the plot unusable. We could fiddle with some settings to the geom_text() function, but the ggrepel package helps to fix this issue for us without having to guess and test. The primary difference in the code below is to use geom_text_repel() instead of geom_text(). Note, I shrunk the data point slightly in the following figure.\nlibrary(ggrepel) ggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 3, aes(shape = species)) + geom_text_repel(aes(label = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) This isn’t actually better, but you can see the points were moved away. The issue here is that there are too many text labels to show in a single plot. I’m going to plot only 30 points, 10 from each species.\nlibrary(dplyr) set.seed(100) penguins %\u0026gt;% group_by(species) %\u0026gt;% sample_n(10) %\u0026gt;% ggplot(., aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 3, aes(shape = species)) + geom_text_repel(aes(label = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) To see exactly what was done, I’m going to generate the same figure using geom_text().\nset.seed(100) penguins %\u0026gt;% group_by(species) %\u0026gt;% sample_n(10) %\u0026gt;% ggplot(., aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 3, aes(shape = species)) + geom_text(aes(label = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) geom_label_repel() The ggrepel package only has two functions, the first we saw, geom_text_repel(). The second is geom_label_repel(). This works the same as geom_text_repel(), but creates a box around the text attribute.\nset.seed(100) penguins %\u0026gt;% group_by(species) %\u0026gt;% sample_n(10) %\u0026gt;% ggplot(., aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 3, aes(shape = species)) + geom_label_repel(aes(label = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;)  ggforce The ggforce package has a few powerful additions. One of these helps to solve the problem of too many text labels when using the entire penguin data and is the problem I’d like to start with.\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 3, aes(shape = species)) + geom_text_repel(aes(label = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) Way too many text labels and for this example, there would be too many duplicate text labels. Since there are only three species, other ways of showing the text and groups would be helpful. ggforce helps with this problem using a series of functions that enclose data within different shapes. These functions are geom_mark_rect(), geom_mark_circle(), geom_mark_ellipse(), and geom_mark_hull() for rectangle, circle, ellipse, and hulls respectively. For an example, let’s try geom_mark_ellipse() instead of the text labels.\nlibrary(ggforce) library(tidyr) penguins %\u0026gt;% drop_na(flipper_length_mm, bill_length_mm) %\u0026gt;% ggplot(., aes(x = flipper_length_mm, y = bill_length_mm)) + geom_mark_ellipse(aes(fill = species)) + geom_point(size = 3, aes(shape = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) To take this one step further, we can add a text label to this figure by setting a label aesthetic to geom_mark_ellipse().\npenguins %\u0026gt;% drop_na(flipper_length_mm, bill_length_mm) %\u0026gt;% ggplot(., aes(x = flipper_length_mm, y = bill_length_mm)) + geom_mark_ellipse(aes(fill = species, label = species)) + geom_point(size = 3, aes(shape = species)) + scale_x_continuous(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + scale_y_continuous(\u0026quot;Penguin Bill Length (in mm)\u0026quot;, limits = c(25, 70)) Another cool feature of ggforce is the ability to use something called facet zoom. Essentially, this will create a zoomed in element of a portion of your figure. For example, suppose we wanted to zoom in on the Gentoo penguins to explore their relationship between bill length and flipper length. This creates a picture in picture plotting effect.\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 3, aes(shape = species, color = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) + facet_zoom(x = species == \u0026#39;Gentoo\u0026#39;)   patchwork The patchwork package is particularly helpful to combine multiple ggplot2 figures into a single figure, but you don’t want to facet. This can be useful to show multiple different relationships of attributes and combine these into a single figure element to include in a document to share.\nTo combine figure elements, basic math notation is used, including +, /, or |. There are other operators as well, but these are the primary ones we will explore and will also use parentheses to group plots together.\nFirst, let’s create a few plots that we may want to combine.\np1 \u0026lt;- ggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 4, aes(color = species, shape = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) p1  p2 \u0026lt;- ggplot(penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + geom_point(size = 4, aes(color = species, shape = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Depth (in mm)\u0026quot;) p2 Now, we will start by using the + operator to combine plots.\nlibrary(patchwork) p1 + p2 As you can see, the plots are combined directly as generated. In the above example, we’d likely want to only have one legend instead of two. We can do this by modifying the first figure to remove the legend.\np1 \u0026lt;- ggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) + geom_point(size = 4, aes(color = species, shape = species)) + xlab(\u0026quot;Penguin Flipper Length (in mm)\u0026quot;) + ylab(\u0026quot;Penguin Bill Length (in mm)\u0026quot;) + theme(legend.position = \u0026#39;none\u0026#39;) p1  p1 + p2 We can use the / operator to stack plots into multiple rows.\np1 / p2 The + operator has one issue with it, it tries to keep things in a square grid, similar to how facet_wrap() works. For more advanced layout, the | operator separates columns whereas we saw above that the / operator will stack plots. Combined with parentheses, you can get more advanced layouts. First, let’s add one more figure.\np3 \u0026lt;- ggplot(drop_na(penguins, sex), aes(x = sex, y = body_mass_g)) + geom_violin(aes(fill = species), draw_quantiles = c(0.1, .5, 0.9)) + xlab(\u0026quot;Penguin Sex\u0026quot;) + ylab(\u0026quot;Penguin Body Mass (in g)\u0026quot;) + theme(legend.position = \u0026#39;none\u0026#39;) p3 p3 | (p1 / p2) Note, without parentheses, the figures may not turn out as you want.\np1 | p2 / p3 (p1 + p2) / p3  ","date":1612310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612310400,"objectID":"6ca2e667791f64c28f5edf60988df181","permalink":"https://psqf6250.brandonlebeau.org/rcode/ggplot2_extensions/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/rcode/ggplot2_extensions/","section":"rcode","summary":"ggplot2 extensions","tags":null,"title":"ggplot2 extensions","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week will explore data visualization using ggplot2 in R.\nObjectives After completing this module, students will be able to:\n Classify different figure types by their usage Interpret different figure types Evaluate if the figure is appropriate for a given research question Create new figures  Activities  Read R for Data Science Textbook - chapters 1 \u0026ndash; 4  Weekly Videos  Introduction to Graphics with R    Adding Aesthetics    More geoms    Plot Customization    R Basics   R Syntax  Graphics Syntax R Basics  Assignments  Quiz 1 - Due January 30th, 2022  ","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"52d024887264e25c0a808c70b28505ad","permalink":"https://psqf6250.brandonlebeau.org/content/02-week2/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/content/02-week2/","section":"content","summary":"Graphics with R","tags":null,"title":"Week 2","type":"book"},{"authors":null,"categories":null,"content":"   Graphics Basics What is the story you want to tell? Is the figure misleading? Could other figure types be more effective? Does the figure show variation? Is the figure self-contained?   Misleading Graphs Data visualization is hard and it is easy to mislead, intentionally or unintentionally.\n Better Approach  Axis Labels  Axis labels are often placed on the x-axis, but for long labels this can be less effective.   Axis Labels 2  Often, the labels are rotated. Works, but is ugly and difficult to read in my opinion.   Axis Labels 3  The solution, flip x and y axis using coord_flip()!   Showing Variation  Figures depicting statistics, should show variation.   Showing Variation 2  There are multiple values for each major category, the mean is useful, but simplifies too much and could mislead.   ","date":1612310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612310400,"objectID":"dc1a0cf78c6a546bd632853fc62d4f3a","permalink":"https://psqf6250.brandonlebeau.org/rcode/graphics-tips/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/rcode/graphics-tips/","section":"rcode","summary":"Graphics Tips","tags":null,"title":"Graphics Tips","type":"book"},{"authors":null,"categories":null,"content":"Introduction The following week the course will continue discussing data visualization using ggplot2 by exploring a handful of useful extension packages that allows new functionality to be implemented. In addition to the new extension packages, some graphical creation tips will be shared.\nObjectives After completing this module, students will be able to:\n Engage with the ggplot2 extension packages Create new figures using the ggplot2 extension packages Evaluate and implement appropriate data visualization standards  Activities  Fundamentals of Data Visualization - chapters 17 \u0026ndash; 26  Weekly Videos  ggrepel    ggforce    patchwork    Graphics Tips   R Syntax  ggplot2 extensions Graphics Tips  Assignments  Quiz 2 - To come \u0026hellip;  ","date":1612310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612310400,"objectID":"a45a23bf966f2b7c885958051528505d","permalink":"https://psqf6250.brandonlebeau.org/content/03-week3/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/content/03-week3/","section":"content","summary":"ggplot2 Extension Packages","tags":null,"title":"Week 3","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week the course will turn to skills that are often not covered much in a statistics course, working with data to create new variables, arrange data, select columns, or filter data. These will make use of the dplyr package from the tidyverse to explore verbs to manipulate data.\nObjectives After completing this module, students will be able to:\n Explore benefits of script based analyses Create new data attributes Evaluate dplyr code for data manipulation  Activities  R for Data Science - chapters 5 \u0026ndash; 6  Weekly Videos  Data Munging Introduction    Arranging Data    Selecting Columns of Data    Add Variables to Data    Summarise Data    Chaining Commands Together    R Scripts   R Syntax  ggplot2 extensions R Basics  Assignments  Assignment 1 - Due around February 13th  ","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643673600,"objectID":"e315f03cac4ec7010fcc961beef42232","permalink":"https://psqf6250.brandonlebeau.org/content/04-week4/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/content/04-week4/","section":"content","summary":"Data Munging/Manipulation","tags":null,"title":"Week 4","type":"book"},{"authors":null,"categories":null,"content":"   Data munging (i.e. data transformations, variable creation, filtering) is a common task that is often overlooked in traditional statistics textbooks and courses. Even though it is omitted, the task of cleaning and organizing the data (coming in week 5 of the course)\nData from the fivethirtyeight package is used in this set of notes to show the use of the dplyr verbs for data munging. This package can be installed with the following command:\ninstall.packages(\u0026quot;fivethirtyeight\u0026quot;) To get started with this set of notes, you will need the following packages loaded:\nlibrary(fivethirtyeight) library(tidyverse) We are going to explore the congress_age data set in more detail. Take a few minutes to familiarize yourself with the data.\nView(congress_age) ?congress_age congress_age ## # A tibble: 18,635 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house M000112 Joseph Jefferson Mansfield \u0026lt;NA\u0026gt; 1861-02-09 ## 2 80 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 3 80 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 4 80 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 5 80 house L000296 William \u0026lt;NA\u0026gt; Lewis \u0026lt;NA\u0026gt; 1868-09-22 ## 6 80 house G000017 James A. Gallagher \u0026lt;NA\u0026gt; 1869-01-16 ## 7 80 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 8 80 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 9 80 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 10 80 house G000169 Charles Laceille Gifford \u0026lt;NA\u0026gt; 1871-03-15 ## # … with 18,625 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; Using dplyr for data munging The dplyr package uses verbs for common data manipulation tasks. These include:\n filter() arrange() select() mutate() summarise()  The great aspect of these verbs are that they all take a similar data structure, the first argument is always the data, the other arguments are unquoted column names. These functions also always return a data frame in which the rows are observations and the columns are variables.\n Examples with filter() The filter function selects rows that match a specified condition(s). For example, suppose we wanted to select only the rows in the data that are a part of the 80th congress. The following code will do this action:\nfilter(congress_age, congress == 80) ## # A tibble: 555 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house M000112 Joseph Jefferson Mansfield \u0026lt;NA\u0026gt; 1861-02-09 ## 2 80 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 3 80 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 4 80 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 5 80 house L000296 William \u0026lt;NA\u0026gt; Lewis \u0026lt;NA\u0026gt; 1868-09-22 ## 6 80 house G000017 James A. Gallagher \u0026lt;NA\u0026gt; 1869-01-16 ## 7 80 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 8 80 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 9 80 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 10 80 house G000169 Charles Laceille Gifford \u0026lt;NA\u0026gt; 1871-03-15 ## # … with 545 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; Notice from above two things, first, the function returned a new data frame. Therefore, if this subsetted data is to be saved, we need to save it to an object, for example, as follows:\ncongress_80 \u0026lt;- filter(congress_age, congress == 80) Notice now that the data were not automatically printed, instead it was saved into the object called congress_80. If you wish to preview the data and save it to an object in a single step, you need to wrap the command above in parentheses. Take a second to try this yourself.\nSecondly, notice from the above commands that equality in R is done with == not just a single =. The single = is used for named arguments, therefore when testing for equality you need to be sure to use ==, this is a common frustration and source of bugs when getting started with R.\nSelecting values based on a character vector are similar to numeric values. For example, suppose we wanted to select only those rows pertaining to those from the senate. The following code will do that:\nsenate \u0026lt;- filter(congress_age, chamber == \u0026#39;senate\u0026#39;) Combining Logical Operations The filter function becomes much more useful with more complex operations. For example, suppose we were interested in selecting the rows that belong to the 80th senate.\nfilter(congress_age, congress == 80, chamber == \u0026#39;senate\u0026#39;) ## # A tibble: 102 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 senate C000133 Arthur \u0026lt;NA\u0026gt; Capper \u0026lt;NA\u0026gt; 1865-07-14 ## 2 80 senate G000418 Theodore Francis Green \u0026lt;NA\u0026gt; 1867-10-02 ## 3 80 senate M000499 Kenneth Douglas McKellar \u0026lt;NA\u0026gt; 1869-01-29 ## 4 80 senate R000112 Clyde Martin Reed \u0026lt;NA\u0026gt; 1871-10-19 ## 5 80 senate M000895 Edward Hall Moore \u0026lt;NA\u0026gt; 1871-11-19 ## 6 80 senate O000146 John Holmes Overton \u0026lt;NA\u0026gt; 1875-09-17 ## 7 80 senate M001108 James Edward Murray \u0026lt;NA\u0026gt; 1876-05-03 ## 8 80 senate M000308 Patrick Anthony McCarran \u0026lt;NA\u0026gt; 1876-08-08 ## 9 80 senate T000165 Elmer \u0026lt;NA\u0026gt; Thomas \u0026lt;NA\u0026gt; 1876-09-08 ## 10 80 senate W000021 Robert Ferdinand Wagner \u0026lt;NA\u0026gt; 1877-06-08 ## # … with 92 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; By default, the filter function uses AND when combining multiple arguments. Therefore, the above command returned only the 102 rows belonging to senators from the 80th congress. The figure on section 5.2.2 of R for Data Science shows all the possible boolean operators.\nUsing an example of the OR operator using | to select the 80th and 81st congress:\nfilter(congress_age, congress == 80 | congress == 81) ## # A tibble: 1,112 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house M000112 Joseph Jefferson Mansfield \u0026lt;NA\u0026gt; 1861-02-09 ## 2 80 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 3 80 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 4 80 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 5 80 house L000296 William \u0026lt;NA\u0026gt; Lewis \u0026lt;NA\u0026gt; 1868-09-22 ## 6 80 house G000017 James A. Gallagher \u0026lt;NA\u0026gt; 1869-01-16 ## 7 80 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 8 80 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 9 80 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 10 80 house G000169 Charles Laceille Gifford \u0026lt;NA\u0026gt; 1871-03-15 ## # … with 1,102 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; Note that to do the OR operator, you need to name the variable twice. When selecting multiple values in the same variable, a handy shortcut is %in%. The same command can be run with the following shorthand: handy shortcut is %in%. The same command can be run with the following shorthard\nfilter(congress_age, congress %in% c(80, 81)) ## # A tibble: 1,112 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house M000112 Joseph Jefferson Mansfield \u0026lt;NA\u0026gt; 1861-02-09 ## 2 80 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 3 80 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 4 80 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 5 80 house L000296 William \u0026lt;NA\u0026gt; Lewis \u0026lt;NA\u0026gt; 1868-09-22 ## 6 80 house G000017 James A. Gallagher \u0026lt;NA\u0026gt; 1869-01-16 ## 7 80 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 8 80 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 9 80 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 10 80 house G000169 Charles Laceille Gifford \u0026lt;NA\u0026gt; 1871-03-15 ## # … with 1,102 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt;  Not Operator Another useful operator that deserves a bit more discussion is the not operator, !. For example, suppose we wanted to omit the 80th congress:\nfilter(congress_age, congress != 80) ## # A tibble: 18,080 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 81 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 2 81 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 3 81 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 4 81 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 5 81 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 6 81 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 7 81 house B000545 Schuyler Otis Bland \u0026lt;NA\u0026gt; 1872-05-04 ## 8 81 house K000138 John Hosea Kerr \u0026lt;NA\u0026gt; 1873-12-31 ## 9 81 house C000932 Robert \u0026lt;NA\u0026gt; Crosser \u0026lt;NA\u0026gt; 1874-06-07 ## 10 81 house K000039 John \u0026lt;NA\u0026gt; Kee \u0026lt;NA\u0026gt; 1874-08-22 ## # … with 18,070 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; It is also possible to do not with an AND operator as follows:\nfilter(congress_age, congress == 80 \u0026amp; !chamber == \u0026#39;senate\u0026#39;) ## # A tibble: 453 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house M000112 Joseph Jefferson Mansfield \u0026lt;NA\u0026gt; 1861-02-09 ## 2 80 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 3 80 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 4 80 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 5 80 house L000296 William \u0026lt;NA\u0026gt; Lewis \u0026lt;NA\u0026gt; 1868-09-22 ## 6 80 house G000017 James A. Gallagher \u0026lt;NA\u0026gt; 1869-01-16 ## 7 80 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 8 80 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 9 80 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 10 80 house G000169 Charles Laceille Gifford \u0026lt;NA\u0026gt; 1871-03-15 ## # … with 443 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; Exercises Using the congress data, select the rows belonging to the democrats (party = D) from the senate of the 100th congress. Select all congress members who are older than 80 years old.    Note on Missing Data Missing data within R are represented with NA which stands for not available.\nThere are no missing data in the congress data, however, by default the filter function will not return any missing values. In order to select missing data, you need to use the is.na function.\nExercise Given the following simple vector, run one filter that selects all values greater than 100. Write a second filter command that selects all the rows greater than 100 and also the NA value.  df \u0026lt;- tibble(x = c(200, 30, NA, 45, 212))    Examples with arrange() The arrange function is used for ordering rows in the data. For example, suppose we wanted to order the rows in the congress data by the state the members of congress lived in. This can be done using the arrange function as follows:\narrange(congress_age, state) ## # A tibble: 18,635 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 2 81 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 3 82 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 4 83 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 5 84 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 6 85 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 7 86 house R000282 Ralph Julian Rivers \u0026lt;NA\u0026gt; 1903-05-23 ## 8 86 senate G000508 Ernest \u0026lt;NA\u0026gt; Gruening \u0026lt;NA\u0026gt; 1887-02-06 ## 9 86 senate B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 10 87 house R000282 Ralph Julian Rivers \u0026lt;NA\u0026gt; 1903-05-23 ## # … with 18,625 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; Similar to the filter function, additional arguments can be added to add more layers to the ordering. For example, if we were interested in ordering the rows by state and then by party affiliation.\narrange(congress_age, state, party) ## # A tibble: 18,635 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 2 81 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 3 82 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 4 83 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 5 84 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 6 85 house B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 7 86 house R000282 Ralph Julian Rivers \u0026lt;NA\u0026gt; 1903-05-23 ## 8 86 senate G000508 Ernest \u0026lt;NA\u0026gt; Gruening \u0026lt;NA\u0026gt; 1887-02-06 ## 9 86 senate B000201 Edward Lewis Bartlett \u0026lt;NA\u0026gt; 1904-04-20 ## 10 87 house R000282 Ralph Julian Rivers \u0026lt;NA\u0026gt; 1903-05-23 ## # … with 18,625 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; More variables can easily be added to the arrange function. Notice from the above two commands that the ordering of the rows is in ascending order, if descending order is desired, the desc function. For example, to order the data starting with the latest congress first:\narrange(congress_age, desc(congress)) ## # A tibble: 18,635 × 13 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 113 house H000067 Ralph M. Hall \u0026lt;NA\u0026gt; 1923-05-03 ## 2 113 house D000355 John D. Dingell \u0026lt;NA\u0026gt; 1926-07-08 ## 3 113 house C000714 John \u0026lt;NA\u0026gt; Conyers Jr. 1929-05-16 ## 4 113 house S000480 Louise McIntosh Slaughter \u0026lt;NA\u0026gt; 1929-08-14 ## 5 113 house R000053 Charles B. Rangel \u0026lt;NA\u0026gt; 1930-06-11 ## 6 113 house J000174 Sam Robert Johnson \u0026lt;NA\u0026gt; 1930-10-11 ## 7 113 house Y000031 C. W. Bill Young \u0026lt;NA\u0026gt; 1930-12-16 ## 8 113 house C000556 Howard \u0026lt;NA\u0026gt; Coble \u0026lt;NA\u0026gt; 1931-03-18 ## 9 113 house L000263 Sander M. Levin \u0026lt;NA\u0026gt; 1931-09-06 ## 10 113 house Y000033 Don E. Young \u0026lt;NA\u0026gt; 1933-06-09 ## # … with 18,625 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt;  Examples with select() The select function is used to select columns (i.e. variables) from the data but keep all the rows. For example, maybe we only needed the congress number, the chamber, the party affiliation, and the age of the members of congress. We can reduce the data to just these variables using select.\nselect(congress_age, congress, chamber, party, age) ## # A tibble: 18,635 × 4 ## congress chamber party age ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 80 house D 85.9 ## 2 80 house D 83.2 ## 3 80 house D 80.7 ## 4 80 house R 78.8 ## 5 80 house R 78.3 ## 6 80 house R 78 ## 7 80 house R 77.9 ## 8 80 house D 76.8 ## 9 80 house R 76 ## 10 80 house R 75.8 ## # … with 18,625 more rows Similar to the arrange functions, the variables that you wish to keep are separated by commas and come after the data argument.\nFor more complex selection, the dplyr package has additional functions that are helpful for variable selection. These include: - starts_with() - ends_with() - contains() - matches() - num_range()\nThese helper functions can be useful for selecting many variables that match a specific pattern. For example, suppose we were interested in selecting all the name variables, this can be accomplished using the contains function as follows:\nselect(congress_age, contains(\u0026#39;name\u0026#39;)) ## # A tibble: 18,635 × 3 ## firstname middlename lastname ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Joseph Jefferson Mansfield ## 2 Robert Lee Doughton ## 3 Adolph Joachim Sabath ## 4 Charles Aubrey Eaton ## 5 William \u0026lt;NA\u0026gt; Lewis ## 6 James A. Gallagher ## 7 Richard Joseph Welch ## 8 Sol \u0026lt;NA\u0026gt; Bloom ## 9 Merlin \u0026lt;NA\u0026gt; Hull ## 10 Charles Laceille Gifford ## # … with 18,625 more rows Another useful shorthand to select multiple columns in succession is the : operator. For example, suppose we wanted to select all the variables between congress and bithday.\nselect(congress_age, congress:birthday) ## # A tibble: 18,635 × 8 ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house M000112 Joseph Jefferson Mansfield \u0026lt;NA\u0026gt; 1861-02-09 ## 2 80 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 3 80 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 4 80 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 5 80 house L000296 William \u0026lt;NA\u0026gt; Lewis \u0026lt;NA\u0026gt; 1868-09-22 ## 6 80 house G000017 James A. Gallagher \u0026lt;NA\u0026gt; 1869-01-16 ## 7 80 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 8 80 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 9 80 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 10 80 house G000169 Charles Laceille Gifford \u0026lt;NA\u0026gt; 1871-03-15 ## # … with 18,625 more rows Rename variables The select function does allow you to rename variables, however, using the select function to rename variables is not usually advised as you may end up missing a variable that you wish to keep during the renaming operation. Instead, using the rename function is better practice.\nrename(congress_age, first_name = firstname, last_name = lastname) ## # A tibble: 18,635 × 13 ## congress chamber bioguide first_name middlename last_name suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house M000112 Joseph Jefferson Mansfield \u0026lt;NA\u0026gt; 1861-02-09 ## 2 80 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 3 80 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 4 80 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 5 80 house L000296 William \u0026lt;NA\u0026gt; Lewis \u0026lt;NA\u0026gt; 1868-09-22 ## 6 80 house G000017 James A. Gallagher \u0026lt;NA\u0026gt; 1869-01-16 ## 7 80 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 8 80 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 9 80 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 10 80 house G000169 Charles Laceille Gifford \u0026lt;NA\u0026gt; 1871-03-15 ## # … with 18,625 more rows, and 5 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt; By default, the rename function will not save changes to the object, if you wish to save the name differences (very likely), be sure to save this new step to an object.\nExercises Using the dplyr helper functions, select all the variables that start with the letter ‘c’. Rename the first three variables in the congress data to ‘x1’, ‘x2’, ‘x3’. After renaming the first three variables, use this new data (ensure you saved the previous step to an object) to select these three variables with the num_range function.     Examples with mutate() mutate is a useful verb that allows you to add new columns to the existing data set. Actions done with mutate include adding a column of means, counts, or other transformations of existing variables. Suppose for example, we wished to convert the party affiliation of the members of congress into a dummy (indicator) variable. This may be useful to more easily compute a proportion or count for instance.\nThis can be done with the mutate function. Below, I’m first going to use select to reduce the number of columns to make it easier to see the operation.\ncongress_red \u0026lt;- select(congress_age, congress, chamber, state, party) mutate(congress_red, democrat = ifelse(party == \u0026#39;D\u0026#39;, 1, 0), num_democrat = sum(democrat) ) ## # A tibble: 18,635 × 6 ## congress chamber state party democrat num_democrat ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 80 house TX D 1 10290 ## 2 80 house NC D 1 10290 ## 3 80 house IL D 1 10290 ## 4 80 house NJ R 0 10290 ## 5 80 house KY R 0 10290 ## 6 80 house PA R 0 10290 ## 7 80 house CA R 0 10290 ## 8 80 house NY D 1 10290 ## 9 80 house WI R 0 10290 ## 10 80 house MA R 0 10290 ## # … with 18,625 more rows You’ll notice that the number of rows in the data are the same (18635) as it was previously, but now the two new columns have been added to the data. One converted the party affiliation to a series of 0/1 values and the other variable counted up the number of democrats elected since the 80th congress. Notice how this last variable is simply repeated for all values in the data. The operation done here is not too exciting, however, we will learn another utility later that allows us to group the data to calculate different values for each group.\nLastly, from the output above, notice that I was able to reference a variable that I created previously in the mutate command. This is unique to the dplyr package and allows you to create a single mutate command to add many variables, even those that depend on prior calculations. Obviously, if you need to reference a calculation in another calculation, they need to be done in the proper order.\nCreation Functions There are many useful operators to use when creating additional variables. The R for Data Science text has many examples shown in section 5.5.1. In general useful operators include addition, subtraction, multiplication, division, descriptive statistics (we will talk more about these in week 4), ranks, logical comparisons, and many more. The exercises will have you explore some of these operations in more detail.\nExercises Using the diamonds data, use ?diamonds for more information on the data, use the mutate function to calculate the price per carat. Hint, this operation would involve standardizing the price variable so that all are comparable at 1 carat. Calculate the rank of the original price variable and the new price variable calculated above using the min_rank function. Are there differences in the ranking of the prices? Hint, it may be useful to test if the two ranks are equal to explore this.     Examples with summarise() summarise is very similar to the mutate function, except instead of adding additional columns to the data, it collapses data down to a single row. For instance, doing the same operation as the example with mutate above:\ncongress_2 \u0026lt;- mutate(congress_age, democrat = ifelse(party == \u0026#39;D\u0026#39;, 1, 0) ) summarise(congress_2, num_democrat = sum(democrat) ) ## # A tibble: 1 × 1 ## num_democrat ## \u0026lt;dbl\u0026gt; ## 1 10290 Notice now, instead of repeating the same value for all the rows as with mutate, summarise collapsed the data into a single numeric summary. Normally this is not a very interesting data activity, however, used in tandem with another function, group_by, interesting summary statistics can be calculated.\nSuppose we were interested in calculating the number of democrats in each congress. This can be achieved with similar code to above, but first by grouping the data as follows:\ncongress_grp \u0026lt;- group_by(congress_2, congress) summarise(congress_grp, num_democrat = sum(democrat), total = n(), prop_democrat = num_democrat / total ) ## # A tibble: 34 × 4 ## congress num_democrat total prop_democrat ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 80 247 555 0.445 ## 2 81 330 557 0.592 ## 3 82 292 555 0.526 ## 4 83 274 557 0.492 ## 5 84 288 544 0.529 ## 6 85 295 547 0.539 ## 7 86 356 554 0.643 ## 8 87 339 559 0.606 ## 9 88 332 552 0.601 ## 10 89 371 548 0.677 ## # … with 24 more rows Notice above, the use of the group_by function to group the data first by congress. Then this new grouped data is passed to the summarise command. As you can see from the output, the operations performed with the summarise function are done for each unique level of the congress variable. You could now easily plot these to see the trend in proportion of democrats has changed over time.\nlibrary(ggplot2) num_dem \u0026lt;- summarise(congress_grp, num_democrat = sum(democrat), total = n(), prop_democrat = num_democrat / total ) ggplot(num_dem, aes(x = congress, y = prop_democrat)) + geom_line() Exercises Suppose we wanted to calculate the number and proportion of republicans instead of democrats, assuming these are the only two parties, edit the summarise command above to calculate these values. Suppose instead of using sum(democrat) above, we used mean(democrat), what does this value return? Why does it return this value?   Extending group_by() in other places The group_by function is also useful with the mutate function and works in a similar way as summarise above. For example, if we wanted to keep the values calculated above in the original data, we could use mutate instead of summarise. This would look like the following:\nmutate(congress_grp, num_democrat = sum(democrat), total = n(), prop_democrat = num_democrat / total ) ## # A tibble: 18,635 × 17 ## # Groups: congress [34] ## congress chamber bioguide firstname middlename lastname suffix birthday ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 80 house M000112 Joseph Jefferson Mansfield \u0026lt;NA\u0026gt; 1861-02-09 ## 2 80 house D000448 Robert Lee Doughton \u0026lt;NA\u0026gt; 1863-11-07 ## 3 80 house S000001 Adolph Joachim Sabath \u0026lt;NA\u0026gt; 1866-04-04 ## 4 80 house E000023 Charles Aubrey Eaton \u0026lt;NA\u0026gt; 1868-03-29 ## 5 80 house L000296 William \u0026lt;NA\u0026gt; Lewis \u0026lt;NA\u0026gt; 1868-09-22 ## 6 80 house G000017 James A. Gallagher \u0026lt;NA\u0026gt; 1869-01-16 ## 7 80 house W000265 Richard Joseph Welch \u0026lt;NA\u0026gt; 1869-02-13 ## 8 80 house B000565 Sol \u0026lt;NA\u0026gt; Bloom \u0026lt;NA\u0026gt; 1870-03-09 ## 9 80 house H000943 Merlin \u0026lt;NA\u0026gt; Hull \u0026lt;NA\u0026gt; 1870-12-18 ## 10 80 house G000169 Charles Laceille Gifford \u0026lt;NA\u0026gt; 1871-03-15 ## # … with 18,625 more rows, and 9 more variables: state \u0026lt;chr\u0026gt;, party \u0026lt;chr\u0026gt;, ## # incumbent \u0026lt;lgl\u0026gt;, termstart \u0026lt;date\u0026gt;, age \u0026lt;dbl\u0026gt;, democrat \u0026lt;dbl\u0026gt;, ## # num_democrat \u0026lt;dbl\u0026gt;, total \u0026lt;int\u0026gt;, prop_democrat \u0026lt;dbl\u0026gt;  Useful summary functions There are many useful summary functions, many of which we will explore in more detail in week 4 of the course during exploratory data analysis (EDA). However, I want to show a few here with the summarise function to ease you in. Suppose for instance we were interested in the knowing the youngest and oldest member of congress for each congress. There are actually two ways of doing this, one is using the min and max functions on the grouped data.\nsummarise(congress_grp, youngest = min(age), oldest = max(age) ) ## # A tibble: 34 × 3 ## congress youngest oldest ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 80 25.9 85.9 ## 2 81 27.2 85.2 ## 3 82 27.9 87.2 ## 4 83 26.7 85.3 ## 5 84 28.5 87.3 ## 6 85 30.5 89.3 ## 7 86 31 91.3 ## 8 87 28.9 86 ## 9 88 29 85.3 ## 10 89 25 87.3 ## # … with 24 more rows This could also be done by using the first and last functions after arranging the data:\nsummarise(arrange(congress_grp, age), youngest = first(age), oldest = last(age) ) ## # A tibble: 34 × 3 ## congress youngest oldest ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 80 25.9 85.9 ## 2 81 27.2 85.2 ## 3 82 27.9 87.2 ## 4 83 26.7 85.3 ## 5 84 28.5 87.3 ## 6 85 30.5 89.3 ## 7 86 31 91.3 ## 8 87 28.9 86 ## 9 88 29 85.3 ## 10 89 25 87.3 ## # … with 24 more rows This goes to show that there are commonly many different ways to calculate descriptive statistics. I would argue two strong virtues when writing code is to make it as clear, expressive, and ensure accuracy. Speed and grace in writing code can come later.\nExercises For each congress, calculate a summary using the following command: n_distinct(state). What does this value return? What happens when you use a logical expression within a sum function call? For example, what do you get in a summarise when you do: sum(age \u0026gt; 75)? What happens when you try to use sum or mean on the variable incumbent?     Chaining together multiple operations Now that you have seen all of the basic dplyr data manipulation verbs, it is useful to chain these together to create more complex operations. So far, I have shown you how to do it by saving intermediate steps, for example, saving the grouped data after using the group_by function. In many instances, these intermediate steps are not useful to us. In these cases you can chain operations together.\nSuppose we are interested in calculating the proportion of democrats for each chamber of congress, but only since the 100th congress? There are two ways to do this, the difficult to read and the easier to read. I first shown the difficult to read.\nsummarise( group_by( mutate( filter( congress_age, congress \u0026gt;= 100 ), democrat = ifelse(party == \u0026#39;D\u0026#39;, 1, 0) ), congress, chamber ), num_democrat = sum(democrat), total = n(), prop_democrat = num_democrat / total ) ## `summarise()` has grouped output by \u0026#39;congress\u0026#39;. You can override using the ## `.groups` argument. ## # A tibble: 28 × 5 ## # Groups: congress [14] ## congress chamber num_democrat total prop_democrat ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 100 house 263 443 0.594 ## 2 100 senate 55 101 0.545 ## 3 101 house 266 445 0.598 ## 4 101 senate 56 101 0.554 ## 5 102 house 272 443 0.614 ## 6 102 senate 59 104 0.567 ## 7 103 house 261 443 0.589 ## 8 103 senate 58 105 0.552 ## 9 104 house 206 441 0.467 ## 10 104 senate 47 103 0.456 ## # … with 18 more rows How difficult do you find the code above to read? This is valid R code, but the first operation done is nested in the middle (it is the filter function that is run first). This makes for difficult code to debug and write in my opinion. In my opinion, the better way to write code is through the pipe operator, %\u0026gt;%. The same code above can be achieved with the following much easier to read code:\ncongress_age %\u0026gt;% filter(congress \u0026gt;= 100) %\u0026gt;% mutate(democrat = ifelse(party == \u0026#39;D\u0026#39;, 1, 0)) %\u0026gt;% group_by(congress, chamber) %\u0026gt;% summarise( num_democrat = sum(democrat), total = n(), prop_democrat = num_democrat / total ) ## `summarise()` has grouped output by \u0026#39;congress\u0026#39;. You can override using the ## `.groups` argument. ## # A tibble: 28 × 5 ## # Groups: congress [14] ## congress chamber num_democrat total prop_democrat ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 100 house 263 443 0.594 ## 2 100 senate 55 101 0.545 ## 3 101 house 266 445 0.598 ## 4 101 senate 56 101 0.554 ## 5 102 house 272 443 0.614 ## 6 102 senate 59 104 0.567 ## 7 103 house 261 443 0.589 ## 8 103 senate 58 105 0.552 ## 9 104 house 206 441 0.467 ## 10 104 senate 47 103 0.456 ## # … with 18 more rows The pipe allows for more readable code by humans and progresses from top to bottom, left to right. The best word to substitute when translating the %\u0026gt;% code above is ‘then’. So the code above says, using the congress_age data, then filter, then mutate, then group_by, then summarise.\nThis is much easier to read and follow the chain of commands. I highly recommend using the pipe in your code. For more details on what is actually happening, the R for Data Science book has a good explanation in Section 5.6.1.\nExercises Look at the following nested code and determine what is being done. Then translate this code to use the pipe operator.  summarise( group_by( mutate( filter( diamonds, color %in% c(\u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;) \u0026amp; cut %in% c(\u0026#39;Fair\u0026#39;, \u0026#39;Good\u0026#39;, \u0026#39;Very Good\u0026#39;) ), f_color = ifelse(color == \u0026#39;F\u0026#39;, 1, 0), vg_cut = ifelse(cut == \u0026#39;Very Good\u0026#39;, 1, 0) ), clarity ), avg = mean(carat), sd = sd(carat), avg_p = mean(price), num = n(), summary_f_color = mean(f_color), summary_vg_cut = mean(vg_cut) )   ","date":1612742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612742400,"objectID":"b0f659bcc0129e01ff2cba6181b735c0","permalink":"https://psqf6250.brandonlebeau.org/rcode/data_munging/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/rcode/data_munging/","section":"rcode","summary":"Data Manipulation","tags":null,"title":"Data Manipulation","type":"book"},{"authors":null,"categories":null,"content":"Introduction This module will give some tools to use when performing exploratory data analysis, a framework that helps you to better understand your data prior to doing any more formal analysis.\nObjectives After completing this module, students will be able to:\n Evaluate exploratory figures Define exploratory data analysis Interpret exploratory data analysis output Create exploratory data analysis code  Activities  R for Data Science - chapter 7  Weekly Videos  EDA - Missing Data    EDA - Variation    EDA - Covariation    EDA - Rare/Common Cases    R Projects   R Syntax  Exploratory Data Analysis R Projects - RStudio  Assignments To come \u0026hellip;\n","date":1644364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644364800,"objectID":"7b26937bb8dacd1f2a59f61a2d255d6b","permalink":"https://psqf6250.brandonlebeau.org/content/05-week5/","publishdate":"2022-02-09T00:00:00Z","relpermalink":"/content/05-week5/","section":"content","summary":"Exploratory Data Analysis","tags":null,"title":"Week 5","type":"book"},{"authors":null,"categories":null,"content":"   I want to talk very briefly about R scripts. You may have been using these already within your workflow for this course, but these are best practice instead of simply running code in the console. Creating R scripts are a crucial step to ensure the data analyses are reproducible, the script will act as a log of all the things that are done to the data to go from data import to any outputs (model results, tables, figures, etc.).\nTo create an R script with RStudio, the short cut is CTRL/CMD + SHIFT + N. You can also create a new script by going to File \u0026gt; New File \u0026gt; R Script. Both of these commands will open up a blank script window.\nIn this script window, I would recommend loading any R packages first at the top of the file. Then proceed with the analysis. Commands can be sent to the console using CRTL/CMD + ENTER. By default RStudio will run any commands that span more than one line with a single CRTL/CMD + ENTER call.\nFor more details about R Scripts, the R for Data Science text has detail with screenshots in Chapter 6. I recommend trying to create a simple script and sending these commands from the script to the console to be run with R.\n","date":1612742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612742400,"objectID":"4a1498dfe51443e4f8fbba8d0304ae58","permalink":"https://psqf6250.brandonlebeau.org/rcode/r_scripts/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/rcode/r_scripts/","section":"rcode","summary":"R Scripts","tags":null,"title":"R Scripts","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week will cover functionality to import, restructure, and join data in the tidyverse. These are common tasks to perform to get the data into the appropriate format for data analysis.\nObjectives After completing this module, students will be able to:\n Define different data join methods Define different data formats Create join and import code  Activities  R for Data Science - chapters 11, 12, 13  Weekly Videos  Data Import    Data Restructuring    Joining Data   R Syntax  Data Import Data Restructuring Data Joins  Assignments To come \u0026hellip;\n","date":1644796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644796800,"objectID":"a1ba50a25029964cc75a26ac15871b71","permalink":"https://psqf6250.brandonlebeau.org/content/06-week6/","publishdate":"2022-02-14T00:00:00Z","relpermalink":"/content/06-week6/","section":"content","summary":"Data Import, Restructuring, Joins","tags":null,"title":"Week 6","type":"book"},{"authors":null,"categories":null,"content":"   Exploratory data analysis (EDA) is an important step in exploring and understanding your data. In addition, EDA does not suffer from problems with inferential statistics related to multiple (correlated) models on the same data. Instead, EDA is a great way to visualize, summarize, and prod your data without any consequences.\nFor this set of notes, we are going to use the following packages:\nlibrary(nycflights13) library(tidyverse) We will use a few different data sets, but the one we are going to start with is the flights data from the nycflights13 package. Below is the first 10 rows of the data.\nflights ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 336,766 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, ## # carrier \u0026lt;chr\u0026gt;, flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, ## # air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt; This data contains information on all flights that departed from the three airports in NYC in 2013. As you can see, the data has a total of 336776 rows and 19. For additional information use ?flights.\nExploratory Data Analysis The general process for proceeding with exploratory data analysis as summarized in the R for Data Science text are:\nAsk questions about your data Search for answers Refine or ask new questions about the data.  There are no bad questions when performing EDA, but some common questions worth exploring are:\n Missing Data Variation Covariation Rare cases Common cases Distributions  Missing Data A first step in exploring the data is to explore if there are any missing data present in the data (likely). This is not an easy step, but determining the amount of missing data and, if possible, why these values are missing are important first steps. One quick way to get a view of this information for the entire data is to use the summary command. An example is given with the flights data below.\nsummary(flights) ## year month day dep_time sched_dep_time ## Min. :2013 Min. : 1.000 Min. : 1.00 Min. : 1 Min. : 106 ## 1st Qu.:2013 1st Qu.: 4.000 1st Qu.: 8.00 1st Qu.: 907 1st Qu.: 906 ## Median :2013 Median : 7.000 Median :16.00 Median :1401 Median :1359 ## Mean :2013 Mean : 6.549 Mean :15.71 Mean :1349 Mean :1344 ## 3rd Qu.:2013 3rd Qu.:10.000 3rd Qu.:23.00 3rd Qu.:1744 3rd Qu.:1729 ## Max. :2013 Max. :12.000 Max. :31.00 Max. :2400 Max. :2359 ## NA\u0026#39;s :8255 ## dep_delay arr_time sched_arr_time arr_delay ## Min. : -43.00 Min. : 1 Min. : 1 Min. : -86.000 ## 1st Qu.: -5.00 1st Qu.:1104 1st Qu.:1124 1st Qu.: -17.000 ## Median : -2.00 Median :1535 Median :1556 Median : -5.000 ## Mean : 12.64 Mean :1502 Mean :1536 Mean : 6.895 ## 3rd Qu.: 11.00 3rd Qu.:1940 3rd Qu.:1945 3rd Qu.: 14.000 ## Max. :1301.00 Max. :2400 Max. :2359 Max. :1272.000 ## NA\u0026#39;s :8255 NA\u0026#39;s :8713 NA\u0026#39;s :9430 ## carrier flight tailnum origin ## Length:336776 Min. : 1 Length:336776 Length:336776 ## Class :character 1st Qu.: 553 Class :character Class :character ## Mode :character Median :1496 Mode :character Mode :character ## Mean :1972 ## 3rd Qu.:3465 ## Max. :8500 ## ## dest air_time distance hour ## Length:336776 Min. : 20.0 Min. : 17 Min. : 1.00 ## Class :character 1st Qu.: 82.0 1st Qu.: 502 1st Qu.: 9.00 ## Mode :character Median :129.0 Median : 872 Median :13.00 ## Mean :150.7 Mean :1040 Mean :13.18 ## 3rd Qu.:192.0 3rd Qu.:1389 3rd Qu.:17.00 ## Max. :695.0 Max. :4983 Max. :23.00 ## NA\u0026#39;s :9430 ## minute time_hour ## Min. : 0.00 Min. :2013-01-01 05:00:00 ## 1st Qu.: 8.00 1st Qu.:2013-04-04 13:00:00 ## Median :29.00 Median :2013-07-03 10:00:00 ## Mean :26.23 Mean :2013-07-03 05:22:54 ## 3rd Qu.:44.00 3rd Qu.:2013-10-01 07:00:00 ## Max. :59.00 Max. :2013-12-31 23:00:00 ##  This summary can be a bit difficult to digest at first, but can give some useful insight into the variables, including the amount of missing data for each variable.\nYou can dive into looking at specific rows that are missing with the filter command and the is.na function. An example pulling out rows with a missing dep_time values is illustrated:\nfilter(flights, is.na(dep_time)) ## # A tibble: 8,255 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2013 1 1 NA 1630 NA NA 1815 ## 2 2013 1 1 NA 1935 NA NA 2240 ## 3 2013 1 1 NA 1500 NA NA 1825 ## 4 2013 1 1 NA 600 NA NA 901 ## 5 2013 1 2 NA 1540 NA NA 1747 ## 6 2013 1 2 NA 1620 NA NA 1746 ## 7 2013 1 2 NA 1355 NA NA 1459 ## 8 2013 1 2 NA 1420 NA NA 1644 ## 9 2013 1 2 NA 1321 NA NA 1536 ## 10 2013 1 2 NA 1545 NA NA 1910 ## # … with 8,245 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, ## # carrier \u0026lt;chr\u0026gt;, flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, ## # air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt; We can also build more complex operations to look at data that is missing for one variable, but not another. For instance, if you look at the summary information above, you may notice there are more missing values for the arr_delay variable compared to the arr_time variable. To look at these values you can use the following command:\nfilter(flights, is.na(arr_delay) \u0026amp; !is.na(arr_time)) ## # A tibble: 717 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2013 1 1 1525 1530 -5 1934 1805 ## 2 2013 1 1 1528 1459 29 2002 1647 ## 3 2013 1 1 1740 1745 -5 2158 2020 ## 4 2013 1 1 1807 1738 29 2251 2103 ## 5 2013 1 1 1939 1840 59 29 2151 ## 6 2013 1 1 1952 1930 22 2358 2207 ## 7 2013 1 2 905 822 43 1313 1045 ## 8 2013 1 2 1125 925 120 1445 1146 ## 9 2013 1 2 1848 1840 8 2333 2151 ## 10 2013 1 2 1849 1724 85 2235 1938 ## # … with 707 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, carrier \u0026lt;chr\u0026gt;, ## # flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, ## # distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt; This may actually be a data error, as there is an arr_time value, a scheduled_arr_time value, but no arr_delay value. This could then be calculated manually to reduce the number of missing values with this variable.\nViewing missing data graphically It may be useful to view missing data graphically. This may be useful to see if there are specific trends in the data in relation to the missing values. A few ways to plot these data may be useful.\nFirst, it is always a good rule to explore missing data in relation to other variables in the data. If there is evidence that another variable is influencing whether a value is missing, additional statistical controls are needed to adjust for these concerns.\nFor example, we could explore if the scheduled arrival time is related to whether the actual arrival flight time is missing.\nflights %\u0026gt;% mutate( miss_arrival = is.na(arr_time) ) %\u0026gt;% ggplot(mapping = aes(sched_arr_time)) + geom_freqpoly(aes(color = miss_arrival)) + theme_bw() Notice that the count metric masks much of what is being visualized here as there are many more flights that arrived compared to those with missing times. To adjust this, we simply need to change the y-axis from counts to density.\nflights %\u0026gt;% mutate( miss_arrival = is.na(arr_time) ) %\u0026gt;% ggplot(mapping = aes(x = sched_arr_time, y = ..density..)) + geom_freqpoly(aes(color = miss_arrival)) + theme_bw() The two curves are now standardized so that the area under each curve equals 1, which in turn makes comparison between the two groups easier.\nOne other special note, for EDA internally, there is no need to spend much time worrying about formatting of the graphics. However, if this plot above would be included in a report or manuscript, this figure would need additional polish to be included.\n  Variation Another common EDA question is related to variation. Variation is important for statistics, without variation there is no need to do statistics. The best way to explore variation of any type of variable is through visulization. This section will be broken into two sub areas, one that explores qualitative and another that explores quantitative.\nQualitative Variables Bar graphs (frequency tables) are commonly used to explore variation in qualitative variables. For example, if we wished to explore the number of flights that took off for each month of the year from NYC:\nggplot(flights, aes(factor(month))) + geom_bar() + theme_bw() One special note about the above code, I used the factor() function so that ggplot specifically added all the values of the variable to the x-axis. By default since the month variable is being treated as an integer (a number), it would not show all the values for month.\nThese counts could be calculated manually with the use of dplyr using the count function. The count function basically creates a frequency table. More complex tables can be created by passing additional variables to the count function.\nflights %\u0026gt;% count(month) ## # A tibble: 12 × 2 ## month n ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 27004 ## 2 2 24951 ## 3 3 28834 ## 4 4 28330 ## 5 5 28796 ## 6 6 28243 ## 7 7 29425 ## 8 8 29327 ## 9 9 27574 ## 10 10 28889 ## 11 11 27268 ## 12 12 28135  Exercises Copy the code from the bar graph above, but instead of wrapping the month variable in factor, try it without it. What is different? Extra, using the scale_x_continuous function, can you manually add each of the 12 numeric month values to the plot? Using dplyr, manually calculate the number of flights that took off for every day of every month. In other words, how many flights took off everyday of the year. Which day had the most flights?   Quantitative Variables Histrograms, frequency polygons, or density curves are three common options to explore variation with quantitative variables. Within the flights data, suppose we were interested in exploring the variation in the distance traveled, this could easily be done with a histrogram.\nggplot(flights, aes(distance)) + geom_histogram() + theme_bw() We could also use a frequency polygon:\nggplot(flights, aes(distance)) + geom_freqpoly() + theme_bw() We could also use a density curve:\nggplot(flights, aes(distance)) + geom_density() + theme_bw() When exploring the variation for a single variable overall, I tend to use histograms. However, when attempting to see if the variation changes across values of a categorical variable, histograms are difficult as the groups likely overlap. These are instances when using the frequency polygon or density curves are useful. Here are examples of both when exploring variation differences by month.\nggplot(flights, aes(distance)) + geom_freqpoly(aes(color = factor(month))) + theme_bw() ggplot(flights, aes(distance)) + geom_density(aes(color = factor(month))) + theme_bw() You can also calculate the counts plotted in the histograms and frequency polygons using the count as with qualitative variables. We just now need to use the cut_width function to specify bins.\nflights %\u0026gt;% count(cut_width(distance, 100)) ## # A tibble: 28 × 2 ## `cut_width(distance, 100)` n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 [-50,50] 1 ## 2 (50,150] 2514 ## 3 (150,250] 36839 ## 4 (250,350] 18442 ## 5 (350,450] 15233 ## 6 (450,550] 29149 ## 7 (550,650] 11688 ## 8 (650,750] 33482 ## 9 (750,850] 19482 ## 10 (850,950] 19644 ## # … with 18 more rows Note, these counts may differ from above as the binwidth was not specifically stated when creating the histrogram or frequency polygon.\nIt may also be useful to calculate the variance, standard deviation, or the range. These can be calculated using the summarize function.\nflights %\u0026gt;% summarize( var_dist = var(distance, na.rm = TRUE), sd_dist = sd(distance, na.rm = TRUE), min_dist = min(distance, na.rm = TRUE), max_dist = max(distance, na.rm = TRUE) ) ## # A tibble: 1 × 4 ## var_dist sd_dist min_dist max_dist ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 537631. 733. 17 4983 You could pair this with the group_by function to calculate these values for different groups (e.g. by month).\n Exercises Explore variation in the air_time variable. Does the variation in the air_time variable differ by month?    Distributions Exploring distributions for variables is a very similar process to exploring the variation, the question is just different. Most often we are interested in exploring if the shape of the distribution is approximately normal. This will become more interesting when we start fitting models to explore potential assumption violations in the residuals. We leave these discussions until then.\n Covariation Covariation is the process of comparing how two (or more) variables are related. The most common method for exploring covariation is through scatterplots. However, these are most natural for two continuous variables. Other plots are useful for a mixture of variable types or for two qualitative variables. We will explore each in turn.\nTwo Qualitative Variables Covariation in two qualitative variables is more difficult to view visually due to the restricted possible values in each variable. Suppose for example, we wished to explore covariation in the origin of the flight and the carrier.\nggplot(flights, aes(origin, carrier)) + geom_count() This plot is okay, however, I think a more useful plot is to use a tile plot to explore these differences with color.\nflights %\u0026gt;% count(origin, carrier) %\u0026gt;% ggplot(aes(origin, carrier)) + geom_tile(aes(fill = n)) + theme_bw() Note, that holes mean missing values (i.e. no flights from that airport from that carrier).\n Exercises Explore the covariation between the month and day variables. Note, these are treated as continuous in the data, but in reality they are likely best represented as qualitative.   Two Quantitative Variables Scatterplots are useful for two quantitative variables. Suppose for example that we wish to explore the relationship between the air_time variable and the arr_delay variable. This could be done with a scatterplot.\nggplot(flights, aes(air_time, arr_delay)) + geom_point() + theme_bw() ## Warning: Removed 9430 rows containing missing values (geom_point). One problem with the plot above, is overplotting. There are two fixes for this, one is to use transparent points using the alpha argument to the geom_point function.\nggplot(flights, aes(air_time, arr_delay)) + geom_point(alpha = .05) + theme_bw() ## Warning: Removed 9430 rows containing missing values (geom_point). Another approach that will simplify the exploration is to use boxplots. This will involve grouping the air_time variable into “bins.”\nggplot(flights, aes(x = air_time, y = arr_delay)) + geom_boxplot(aes(group = cut_width(air_time, 50))) + theme_bw() ## Warning: Removed 9430 rows containing missing values (stat_boxplot). Even another more sophisticated graphic is to do the quantitative alternative to geom_tile. Note, the code below uses the hexbin package, but a similar function is geom_bin2d.\n# install.packages(\u0026quot;hexbin\u0026quot;) library(hexbin) ggplot(flights, aes(x = air_time, y = arr_delay)) + geom_hex() ## Warning: Removed 9430 rows containing non-finite values (stat_binhex).  Adding a Third Variable Adding a third variable is often useful, but can be difficult to think about procedurally. The type of plot that is useful depends on the type the third variable is. For example, if the third variable is also quantitative, the visualization is more difficult, however, if the third variable is qualitative, there are two main options. These will be explored in more detail below.\nThe main two approaches for adding a third variable when it is qualitative is to use a different color/shape for the values of this variable or to facet the plot. Suppose we wished to explore the covariation between the following three variables: air_time, arr_delay, and origin. The two different options are shown below.\nggplot(flights, aes(air_time, arr_delay)) + geom_hex() + facet_grid(. ~ origin) + theme_bw() ## Warning: Removed 9430 rows containing non-finite values (stat_binhex). ggplot(flights, aes(air_time, arr_delay)) + geom_point(aes(color = origin), alpha = .05) + theme_bw() ## Warning: Removed 9430 rows containing missing values (geom_point). Plotting three quantitative variables commonly involves binning one one the variables to turn it into an ordinal variable with different levels. For example, see the example with two variables and the boxplot, a similar approach could be used to facet by this third variable. Below is a simple example:\nggplot(flights, aes(x = air_time, y = arr_delay)) + geom_hex() + theme_bw() + facet_wrap(~ cut_width(dep_time, 250)) ## Warning: Removed 9430 rows containing non-finite values (stat_binhex).  One Quantitative, One Qualitative This was actually already discussed in the discussion of variation by exploring differences in variation for different levels of a qualitative variable (see above). If the variation differs by groups, there is then evidence of covariation.\n Correlations It is also useful to calculate and visualize raw correlations. To calculate raw correlations (assuming only quantitative variables), the cor function is useful.\nflights %\u0026gt;% select(air_time, arr_delay, dep_time) %\u0026gt;% cor(use = \u0026#39;pairwise.complete.obs\u0026#39;) ## air_time arr_delay dep_time ## air_time 1.00000000 -0.03529709 -0.01461948 ## arr_delay -0.03529709 1.00000000 0.23230573 ## dep_time -0.01461948 0.23230573 1.00000000 To visualize a correlation matrix, the GGally package is useful. Note, the ggpairs function can take some time to run.\n# install.package(\u0026quot;GGally\u0026quot;) library(GGally) flights %\u0026gt;% select(air_time, arr_delay, dep_time) %\u0026gt;% na.omit() %\u0026gt;% sample_n(1000) %\u0026gt;% ggpairs()  Exercises Explore covariation in the dep_delay and arr_delay variables. What type of relationship, if any, appears to be present? Explore the relationship between dep_delay, arr_delay, and origin. What type of relationship is present. Does the relationship between dep_delay and arr_delay differ by origin? Finally, calculate the correlation matrix for dep_delay, arr_delay, and dep_time.    Rare/Common Cases The last question of use to explore when performing EDA is looking for the presence of rare or common cases. In other words, an exploration of any outliers and the central tendency of the distribution.\nWhen we explored variation in the distance variable earlier, there may have been extreme values we’d want to explore in more detail.\nggplot(flights, aes(distance)) + geom_histogram() + theme_bw() Notice the large distance value, to get a better view of how many there are here, we can use coord_cartesian to zoom in.\nggplot(flights, aes(distance)) + geom_histogram() + theme_bw() + coord_cartesian(ylim = c(0, 5000)) Note, that in the above plot, coord_cartesian does not remove any points, simply changes the coordinates that are plotted. We could also pull these out using filter as well.\nflights %\u0026gt;% filter(distance \u0026gt; 3000) %\u0026gt;% arrange(distance) ## # A tibble: 715 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2013 7 6 1629 1615 14 1954 1953 ## 2 2013 7 13 1618 1615 3 1955 1953 ## 3 2013 7 20 1618 1615 3 2003 1953 ## 4 2013 7 27 1617 1615 2 1906 1953 ## 5 2013 8 3 1615 1615 0 2003 1953 ## 6 2013 8 10 1613 1615 -2 1922 1953 ## 7 2013 8 17 1740 1625 75 2042 2003 ## 8 2013 8 24 1633 1625 8 1959 2003 ## 9 2013 1 1 1344 1344 0 2005 1944 ## 10 2013 1 2 1344 1344 0 1940 1944 ## # … with 705 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, carrier \u0026lt;chr\u0026gt;, ## # flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, ## # distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt; Measures of Central Tendency Exploring measures of central tendency or simply common values/repeated of common values can also be important.\nggplot(flights, aes(arr_time)) + geom_histogram(binwidth = 50) + theme_bw() ## Warning: Removed 8713 rows containing non-finite values (stat_bin). Measures of central tendency can be directly calculated using the summarise function. For example, exploring central tendency of the arr_delay variable.\nflights %\u0026gt;% summarise( avg_arrdelay = mean(arr_delay, na.rm = TRUE), med_arrdelay = median(arr_delay, na.rm = TRUE) ) ## # A tibble: 1 × 2 ## avg_arrdelay med_arrdelay ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 6.90 -5 More interesting computations can be performed by using adding in the group_by function.\n Exercises Using the txhousing data, explore rare/common cases in the median sale price for the following 3 cities: Austin, Dallas, and Houston. Using the data from #1, explore measures of central tendency in the median sale price of these three cities. How have these changed over time (year)? Create an effective visualization that explores differences in the median sale price over time for these three cities.     ","date":1613347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613347200,"objectID":"d51221318d1ce5961433027d6b37a824","permalink":"https://psqf6250.brandonlebeau.org/rcode/eda/","publishdate":"2021-02-15T00:00:00Z","relpermalink":"/rcode/eda/","section":"rcode","summary":"Exploratory Data Analysis","tags":null,"title":"Exploratory Data Analysis","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week will get you oriented for working with factor variables in R which can be useful for linear modeling (to come next week). Then the week will also explore how to manipulate character vectors to extract information you may want. As part of this, special functionality for extracting dates will be explored.\nObjectives After completing this module, students will be able to:\n Identify character and factor data attributes Manipulate factor data attributes Manipulate character vectors Extract dates using the lubridate package  Activities  R for Data Science - chapters 15, 16  Weekly Videos  Intro to Factors    Factor Manipulations    Rename Factor Levels    Basic Character String Tasks    Introduction to Regular Expressions    Regular Expression Functions    Real-world Examples - working with dates   R Syntax  Factors Strings  Assignments To come \u0026hellip;\n","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"edf57c5339859dc84c8243e38d10813b","permalink":"https://psqf6250.brandonlebeau.org/content/07-week7/","publishdate":"2022-02-21T00:00:00Z","relpermalink":"/content/07-week7/","section":"content","summary":"Factors and String Manipulations","tags":null,"title":"Week 7","type":"book"},{"authors":null,"categories":null,"content":"   I want to talk briefly about RStudio projects. These are a great way to structure each individual project. Chapter 8 in the R for Data Science text will provide a more thorough discussion of RStudio projects: http://r4ds.had.co.nz/workflow-projects.html\nWorking Directory As we move into reading in data files, the idea of a working directory will become even more important. RStudio projects makes this discussion much easier as the root of the project directory is treated as the working directory. The nice aspect of this is that all paths to data files are in reference to this root project directory (more on this coming soon).\n Project Structure The last aspect I want to share is a common project directory structure. Everyone can have slightly different versions, but I want to share what I have come to commonly use. The following structure is how I tend to structure most or my projects.\nProject Structure\n Another way to visualize this structure is as follows:\n Project Root:  Data example paper R    ","date":1613347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613347200,"objectID":"ad26be1d8c7fb8b89ce5463a40d8d96d","permalink":"https://psqf6250.brandonlebeau.org/rcode/projects/","publishdate":"2021-02-15T00:00:00Z","relpermalink":"/rcode/projects/","section":"rcode","summary":"R Projects -  RStudio","tags":null,"title":"R Projects -  RStudio","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week will explore fitting statistical models within R. More specifically, regression models will be explored with categorical predictors. Focus will be on interpretation and evaluating model fit. Future weeks will explore more complicated models and also visualizing and exploring assumptions.\nObjectives After completing this module, students will be able to:\n Fit a regression model in R Interpret model parameters Evaluate model fitting  Weekly Videos  Introduction to Models    lm() Behind the Scenes    Single Categorical Predictor    Evaluating Model Fit   R Syntax  Model Intro  Assignments To come \u0026hellip;\n","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"61b59fb25c9c9655e1869c626f92dc17","permalink":"https://psqf6250.brandonlebeau.org/content/08-week8/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/content/08-week8/","section":"content","summary":"Introduction to Regression Models","tags":null,"title":"Week 8","type":"book"},{"authors":null,"categories":null,"content":"   So far we have solely used data that is already found within R by way of packages. Obviously, we will want to use our own data and this involves importing data directly into R. We are going to focus on two types of data structures to read in, text files and excel files.\nThe following two packages will be used in this section.\nlibrary(tidyverse) # install.packages(\u0026quot;readxl\u0026quot;) library(readxl) Text Files Through the use of the readr package, we are going to read in flat text files. In many cases, these text files are saved as csv files. The csv stands for comma separated values files meaning that columns in the data are separated by columns. As a side note, this is the most common way that I save data and read in data. The nice aspect of csv files is that if needed, they can be opened in programs like Excel for viewing, but are still just text files which are simple and lightweight.\nTo read in a csv file, we are going to use the read_csv function from the readr package. We are going to read in some UFO data (the data can be found on the course website). The code below is going to read the data directly from the GitHub where the data are currently being stored.\nufo \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf-6250-blogdown/main/data/ufo.csv\u0026quot;) ## Rows: 8031 Columns: 7 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (7): Date / Time, City, State, Shape, Duration, Summary, Posted ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. A special note here, if you downloaded the data, I recommend putting the downloaded data within an RStudio project in a folder named “Data” or “data”. The updated code (not run here), would look like (assuming the data are in the “data” folder within an RStudio project):\nufo \u0026lt;- read_csv(\u0026#39;data/ufo.csv\u0026#39;) Note again, similar to dplyr, when saving the data to an object, it will not be printed. We can now view the first 10 rows by typing the object name.\nufo ## # A tibble: 8,031 × 7 ## `Date / Time` City State Shape Duration Summary Posted ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 12/12/14 17:30 North Wales PA Triang… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; ## 2 12/12/14 12:40 Cartersville GA Unknown 3.6 min… \u0026quot;Looki… 12/12… ## 3 12/12/14 06:30 Isle of Man (UK/England) \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… ## 4 12/12/14 01:00 Miamisburg OH Changi… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… ## 5 12/12/14 00:00 Spotsylvania VA Unknown 1 minute \u0026quot;White… 12/12… ## 6 12/11/14 23:25 Kenner LA Chevron ~1 minu… \u0026quot;Stran… 12/12… ## 7 12/11/14 23:15 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… ## 8 12/11/14 20:04 Phoenix AZ Chevron 3 minut… \u0026quot;4 Ora… 12/12… ## 9 12/11/14 20:00 Franklin NC Disk 5 minut… \u0026quot;There… 12/12… ## 10 12/11/14 18:30 Longview WA Cylind… 10 seco… \u0026quot;Two c… 12/12… ## # … with 8,021 more rows By default, the read_csv function uses the first row of the data file as the names of the variables. To override this behavior, set col_names = FALSE or better yet, specify the names with the col_names argument. In addition, if the file has header metadata, rows of the data can be skipped with the skip argument. For example, reading in the same data as above, but skipping the first row and specifying the names manually would look as follows:\nread_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf-6250-blogdown/main/data/ufo.csv\u0026quot;, skip = 1, col_names = c(\u0026#39;Date/Time\u0026#39;, \u0026#39;City\u0026#39;, \u0026#39;State\u0026#39;, \u0026#39;Shape\u0026#39;, \u0026#39;Duration\u0026#39;, \u0026#39;Summary\u0026#39;, \u0026#39;Posted\u0026#39;)) ## Rows: 8031 Columns: 7 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (7): Date/Time, City, State, Shape, Duration, Summary, Posted ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 8,031 × 7 ## `Date/Time` City State Shape Duration Summary Posted ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 12/12/14 17:30 North Wales PA Triang… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; ## 2 12/12/14 12:40 Cartersville GA Unknown 3.6 min… \u0026quot;Looki… 12/12… ## 3 12/12/14 06:30 Isle of Man (UK/England) \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… ## 4 12/12/14 01:00 Miamisburg OH Changi… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… ## 5 12/12/14 00:00 Spotsylvania VA Unknown 1 minute \u0026quot;White… 12/12… ## 6 12/11/14 23:25 Kenner LA Chevron ~1 minu… \u0026quot;Stran… 12/12… ## 7 12/11/14 23:15 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… ## 8 12/11/14 20:04 Phoenix AZ Chevron 3 minut… \u0026quot;4 Ora… 12/12… ## 9 12/11/14 20:00 Franklin NC Disk 5 minut… \u0026quot;There… 12/12… ## 10 12/11/14 18:30 Longview WA Cylind… 10 seco… \u0026quot;Two c… 12/12… ## # … with 8,021 more rows Manually Specifying Column Types You may have noticed above that we just needed to give the read_csv function the path to the data file, we did not need to tell the function the types of columns. Instead, the function guessed the type from the first 1000 rows. This can be useful for interactive work, but for truly reproducible code, it is best to specify these manually. There are two ways to specify the column types, one is verbose and the other is simpler, but both use the argument col_types.\nFirst the verbose solution:\nread_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf-6250-blogdown/main/data/ufo.csv\u0026quot;, col_types = c( \u0026#39;Date/Time\u0026#39; = col_character(), City = col_character(), State = col_character(), Shape = col_character(), Duration = col_character(), Summary = col_character(), Posted = col_character() )) ## # A tibble: 8,031 × 7 ## `Date / Time` City State Shape Duration Summary Posted ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 12/12/14 17:30 North Wales PA Triang… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; ## 2 12/12/14 12:40 Cartersville GA Unknown 3.6 min… \u0026quot;Looki… 12/12… ## 3 12/12/14 06:30 Isle of Man (UK/England) \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… ## 4 12/12/14 01:00 Miamisburg OH Changi… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… ## 5 12/12/14 00:00 Spotsylvania VA Unknown 1 minute \u0026quot;White… 12/12… ## 6 12/11/14 23:25 Kenner LA Chevron ~1 minu… \u0026quot;Stran… 12/12… ## 7 12/11/14 23:15 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… ## 8 12/11/14 20:04 Phoenix AZ Chevron 3 minut… \u0026quot;4 Ora… 12/12… ## 9 12/11/14 20:00 Franklin NC Disk 5 minut… \u0026quot;There… 12/12… ## 10 12/11/14 18:30 Longview WA Cylind… 10 seco… \u0026quot;Two c… 12/12… ## # … with 8,021 more rows As all variables are being read in as characters, there is a simple shortcut to use.\nread_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf-6250-blogdown/main/data/ufo.csv\u0026quot;, col_types = c(\u0026#39;ccccccc\u0026#39;)) ## # A tibble: 8,031 × 7 ## `Date / Time` City State Shape Duration Summary Posted ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 12/12/14 17:30 North Wales PA Triang… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; ## 2 12/12/14 12:40 Cartersville GA Unknown 3.6 min… \u0026quot;Looki… 12/12… ## 3 12/12/14 06:30 Isle of Man (UK/England) \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… ## 4 12/12/14 01:00 Miamisburg OH Changi… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… ## 5 12/12/14 00:00 Spotsylvania VA Unknown 1 minute \u0026quot;White… 12/12… ## 6 12/11/14 23:25 Kenner LA Chevron ~1 minu… \u0026quot;Stran… 12/12… ## 7 12/11/14 23:15 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… ## 8 12/11/14 20:04 Phoenix AZ Chevron 3 minut… \u0026quot;4 Ora… 12/12… ## 9 12/11/14 20:00 Franklin NC Disk 5 minut… \u0026quot;There… 12/12… ## 10 12/11/14 18:30 Longview WA Cylind… 10 seco… \u0026quot;Two c… 12/12… ## # … with 8,021 more rows To show the reason the more verbose is useful, suppose we wished to convert the ‘Data/Time’ variable to the correct type, a date time variable.\nread_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf-6250-blogdown/main/data/ufo.csv\u0026quot;, col_types = c( \u0026#39;Date / Time\u0026#39; = col_datetime(), City = col_character(), State = col_character(), Shape = col_character(), Duration = col_character(), Summary = col_character(), Posted = col_character() )) ## Error: Unknown shortcut: Here we get an error, which is caused by the fact that the date time variable specification needs a format statement. We can directly specify this.\nufo_date \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf-6250-blogdown/main/data/ufo.csv\u0026quot;, col_types = list( \u0026#39;Date / Time\u0026#39; = col_datetime(format = \u0026quot;%m/%d/%y %H:%M\u0026quot;), City = col_character(), State = col_character(), Shape = col_character(), Duration = col_character(), Summary = col_character(), Posted = col_character() )) ## Warning: One or more parsing issues, see `problems()` for details ufo_date ## # A tibble: 8,031 × 7 ## `Date / Time` City State Shape Duration Summary Posted ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2014-12-12 17:30:00 North Wales PA Tria… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; ## 2 2014-12-12 12:40:00 Cartersville GA Unkn… 3.6 min… \u0026quot;Looki… 12/12… ## 3 2014-12-12 06:30:00 Isle of Man (UK/Engl… \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… ## 4 2014-12-12 01:00:00 Miamisburg OH Chan… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… ## 5 2014-12-12 00:00:00 Spotsylvania VA Unkn… 1 minute \u0026quot;White… 12/12… ## 6 2014-12-11 23:25:00 Kenner LA Chev… ~1 minu… \u0026quot;Stran… 12/12… ## 7 2014-12-11 23:15:00 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… ## 8 2014-12-11 20:04:00 Phoenix AZ Chev… 3 minut… \u0026quot;4 Ora… 12/12… ## 9 2014-12-11 20:00:00 Franklin NC Disk 5 minut… \u0026quot;There… 12/12… ## 10 2014-12-11 18:30:00 Longview WA Cyli… 10 seco… \u0026quot;Two c… 12/12… ## # … with 8,021 more rows Notice even though I was careful in the column specification, there was still issues when parsing this column as a date/time column. The data is still returned, but there are issues. These issues can be viewed using the problems function.\nproblems(ufo_date) ## # A tibble: 56 × 5 ## row col expected actual file ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 120 1 date like %m/%d/%y %H:%M 12/1/14 \u0026quot;\u0026quot; ## 2 195 1 date like %m/%d/%y %H:%M 11/27/14 \u0026quot;\u0026quot; ## 3 237 1 date like %m/%d/%y %H:%M 11/24/14 \u0026quot;\u0026quot; ## 4 408 1 date like %m/%d/%y %H:%M 11/15/14 \u0026quot;\u0026quot; ## 5 666 1 date like %m/%d/%y %H:%M 10/31/14 \u0026quot;\u0026quot; ## 6 798 1 date like %m/%d/%y %H:%M 10/25/14 \u0026quot;\u0026quot; ## 7 947 1 date like %m/%d/%y %H:%M 10/19/14 \u0026quot;\u0026quot; ## 8 1082 1 date like %m/%d/%y %H:%M 10/14/14 \u0026quot;\u0026quot; ## 9 1123 1 date like %m/%d/%y %H:%M 10/12/14 \u0026quot;\u0026quot; ## 10 1124 1 date like %m/%d/%y %H:%M 10/12/14 \u0026quot;\u0026quot; ## # … with 46 more rows  Other Text Formats There are other text formats used to read in data. They are listed below with the function used to read in that type. Note, that the function calls are identical to those specified above.\n tsv - tab separated files - read_tsv fixed width files - read_fwf white space generally - read_table delimiter generally - read_delim  Exercises There is a tsv file posted on icon called “lotr_clean.tsv”. Download this and read this data file into R. Instead of specifying the path, use the function file.choose(). For example, read_tsv(file.choose()). What does this function use? Would you recommend this to be used in a reproducible document? Run the getwd() function from the R console. What does this function return?     Excel Files Although I commonly use text files (e.g. csv) files, reality is that many people still use Excel for storing of data files. There are good and bad aspects of this, but reading in Excel files may be needed. The readxl package is useful for this task.\nSuppose we wished to read in the Excel file found on the US Census Bureau website related to Education: https://www.census.gov/support/USACdataDownloads.html\nTo do this, we can do this directly with the read_excel function with the data already downloaded and posted to the course website. Note, the read_excel() function does not allow for reading in data from the web, instead the data need to be downloaded to a temp file, then this file is loaded into R. If you downloaded the data, I recommend placing it within a “Data” or “data” folder.\ntf \u0026lt;- tempfile(fileext = \u0026quot;.xls\u0026quot;) curl::curl_download(\u0026quot;https://github.com/lebebr01/psqf-6250-blogdown/blob/main/data/EDU01.xls?raw=true\u0026quot;, tf) read_excel(tf) ## # A tibble: 3,198 × 42 ## Area_name STCOU EDU010187F EDU010187D EDU010187N1 EDU010187N2 EDU010188F ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 UNITED STATES 00000 0 40024299 0000 0000 0 ## 2 ALABAMA 01000 0 733735 0000 0000 0 ## 3 Autauga, AL 01001 0 6829 0000 0000 0 ## 4 Baldwin, AL 01003 0 16417 0000 0000 0 ## 5 Barbour, AL 01005 0 5071 0000 0000 0 ## 6 Bibb, AL 01007 0 3557 0000 0000 0 ## 7 Blount, AL 01009 0 7319 0000 0000 0 ## 8 Bullock, AL 01011 0 2014 0000 0000 0 ## 9 Butler, AL 01013 0 4640 0000 0000 0 ## 10 Calhoun, AL 01015 0 20939 0000 0000 0 ## # … with 3,188 more rows, and 35 more variables: EDU010188D \u0026lt;dbl\u0026gt;, ## # EDU010188N1 \u0026lt;chr\u0026gt;, EDU010188N2 \u0026lt;chr\u0026gt;, EDU010189F \u0026lt;dbl\u0026gt;, EDU010189D \u0026lt;dbl\u0026gt;, ## # EDU010189N1 \u0026lt;chr\u0026gt;, EDU010189N2 \u0026lt;chr\u0026gt;, EDU010190F \u0026lt;dbl\u0026gt;, EDU010190D \u0026lt;dbl\u0026gt;, ## # EDU010190N1 \u0026lt;chr\u0026gt;, EDU010190N2 \u0026lt;chr\u0026gt;, EDU010191F \u0026lt;dbl\u0026gt;, EDU010191D \u0026lt;dbl\u0026gt;, ## # EDU010191N1 \u0026lt;chr\u0026gt;, EDU010191N2 \u0026lt;chr\u0026gt;, EDU010192F \u0026lt;dbl\u0026gt;, EDU010192D \u0026lt;dbl\u0026gt;, ## # EDU010192N1 \u0026lt;chr\u0026gt;, EDU010192N2 \u0026lt;chr\u0026gt;, EDU010193F \u0026lt;dbl\u0026gt;, EDU010193D \u0026lt;dbl\u0026gt;, ## # EDU010193N1 \u0026lt;chr\u0026gt;, EDU010193N2 \u0026lt;chr\u0026gt;, EDU010194F \u0026lt;dbl\u0026gt;, EDU010194D \u0026lt;dbl\u0026gt;, … By default, the function will read in the first sheet and will treat the first row as the column names. If you wish to read in another sheet, you can use the sheet argument. For example:\nread_excel(tf, sheet = 2) ## # A tibble: 3,198 × 42 ## Area_name STCOU EDU010197F EDU010197D EDU010197N1 EDU010197N2 EDU010198F ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 UNITED STATES 00000 0 44534459 0000 0000 0 ## 2 ALABAMA 01000 0 737386 0000 0000 0 ## 3 Autauga, AL 01001 0 8099 0000 0000 0 ## 4 Baldwin, AL 01003 0 21410 0000 0000 0 ## 5 Barbour, AL 01005 0 5100 0000 0000 0 ## 6 Bibb, AL 01007 0 3717 0000 0000 0 ## 7 Blount, AL 01009 0 7816 0000 0000 0 ## 8 Bullock, AL 01011 0 2010 0000 0000 0 ## 9 Butler, AL 01013 0 4119 0000 0000 0 ## 10 Calhoun, AL 01015 0 19721 0000 0000 0 ## # … with 3,188 more rows, and 35 more variables: EDU010198D \u0026lt;dbl\u0026gt;, ## # EDU010198N1 \u0026lt;chr\u0026gt;, EDU010198N2 \u0026lt;chr\u0026gt;, EDU010199F \u0026lt;dbl\u0026gt;, EDU010199D \u0026lt;dbl\u0026gt;, ## # EDU010199N1 \u0026lt;chr\u0026gt;, EDU010199N2 \u0026lt;chr\u0026gt;, EDU010200F \u0026lt;dbl\u0026gt;, EDU010200D \u0026lt;dbl\u0026gt;, ## # EDU010200N1 \u0026lt;chr\u0026gt;, EDU010200N2 \u0026lt;chr\u0026gt;, EDU010201F \u0026lt;dbl\u0026gt;, EDU010201D \u0026lt;dbl\u0026gt;, ## # EDU010201N1 \u0026lt;chr\u0026gt;, EDU010201N2 \u0026lt;chr\u0026gt;, EDU010202F \u0026lt;dbl\u0026gt;, EDU010202D \u0026lt;dbl\u0026gt;, ## # EDU010202N1 \u0026lt;chr\u0026gt;, EDU010202N2 \u0026lt;chr\u0026gt;, EDU015203F \u0026lt;dbl\u0026gt;, EDU015203D \u0026lt;dbl\u0026gt;, ## # EDU015203N1 \u0026lt;chr\u0026gt;, EDU015203N2 \u0026lt;chr\u0026gt;, EDU015204F \u0026lt;dbl\u0026gt;, EDU015204D \u0026lt;dbl\u0026gt;, … read_excel(tf, sheet = \u0026#39;EDU01B\u0026#39;) ## # A tibble: 3,198 × 42 ## Area_name STCOU EDU010197F EDU010197D EDU010197N1 EDU010197N2 EDU010198F ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 UNITED STATES 00000 0 44534459 0000 0000 0 ## 2 ALABAMA 01000 0 737386 0000 0000 0 ## 3 Autauga, AL 01001 0 8099 0000 0000 0 ## 4 Baldwin, AL 01003 0 21410 0000 0000 0 ## 5 Barbour, AL 01005 0 5100 0000 0000 0 ## 6 Bibb, AL 01007 0 3717 0000 0000 0 ## 7 Blount, AL 01009 0 7816 0000 0000 0 ## 8 Bullock, AL 01011 0 2010 0000 0000 0 ## 9 Butler, AL 01013 0 4119 0000 0000 0 ## 10 Calhoun, AL 01015 0 19721 0000 0000 0 ## # … with 3,188 more rows, and 35 more variables: EDU010198D \u0026lt;dbl\u0026gt;, ## # EDU010198N1 \u0026lt;chr\u0026gt;, EDU010198N2 \u0026lt;chr\u0026gt;, EDU010199F \u0026lt;dbl\u0026gt;, EDU010199D \u0026lt;dbl\u0026gt;, ## # EDU010199N1 \u0026lt;chr\u0026gt;, EDU010199N2 \u0026lt;chr\u0026gt;, EDU010200F \u0026lt;dbl\u0026gt;, EDU010200D \u0026lt;dbl\u0026gt;, ## # EDU010200N1 \u0026lt;chr\u0026gt;, EDU010200N2 \u0026lt;chr\u0026gt;, EDU010201F \u0026lt;dbl\u0026gt;, EDU010201D \u0026lt;dbl\u0026gt;, ## # EDU010201N1 \u0026lt;chr\u0026gt;, EDU010201N2 \u0026lt;chr\u0026gt;, EDU010202F \u0026lt;dbl\u0026gt;, EDU010202D \u0026lt;dbl\u0026gt;, ## # EDU010202N1 \u0026lt;chr\u0026gt;, EDU010202N2 \u0026lt;chr\u0026gt;, EDU015203F \u0026lt;dbl\u0026gt;, EDU015203D \u0026lt;dbl\u0026gt;, ## # EDU015203N1 \u0026lt;chr\u0026gt;, EDU015203N2 \u0026lt;chr\u0026gt;, EDU015204F \u0026lt;dbl\u0026gt;, EDU015204D \u0026lt;dbl\u0026gt;, … If there is metadata or no column names, these can be added in the same fashion as discussed above with the read_csv function. Finally, it should be noted, to use these data within R, you would want to save these data to an object within R.\nedu_data \u0026lt;- read_excel(tf)  Writing Files Most of the read_* functions also come with functions that allow you to write out files as well. I’m only going to cover the write_csv function, however, there are others that may be of use. Similarly to reading in files, the functionality is the same across the write_* functions.\nSuppose we created a new column with the ufo data and wished to save this data to a csv file, this can be accomplished with the following series of commands.\nufo_count \u0026lt;- ufo %\u0026gt;% group_by(State) %\u0026gt;% mutate(num_state = n()) write_csv(ufo_count, path = \u0026#39;path/to/save/file.csv\u0026#39;) Notice there are two arguments to the write_csv function, the first argument is the object you wish to save. The second is the path to the location to save the object. You must specify path = otherwise the write_csv function will look for that object in the R session.\n Other Data Formats There are still other data formats, particularly from proprietary statistical software such as Stata, SAS, or SPSS. To read these files in the haven function would be useful. I leave this as an exercise for you if you have these types of files to read into R.\n ","date":1613606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613606400,"objectID":"f633a13cd9d9bf8127b5d41db07233dc","permalink":"https://psqf6250.brandonlebeau.org/rcode/data_import/","publishdate":"2021-02-18T00:00:00Z","relpermalink":"/rcode/data_import/","section":"rcode","summary":"Data Import","tags":null,"title":"Data Import","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week will explore how to work and fit models that have more than 2 groups.\nObjectives After completing this module, students will be able to:\n Create indicator/dummy variables in R for more than 2 groups Interpret parameters from more than 2 groups  Weekly Videos  More than 2 Groups    Adjusting the Reference Groups    Post Hoc Tests    Interactions    Visualizing Model Results   R Syntax  Model Part 2 Advanced Modeling  Assignments To come \u0026hellip;\n","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"11d56da541945c59c742283c504dcb63","permalink":"https://psqf6250.brandonlebeau.org/content/09-week9/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/content/09-week9/","section":"content","summary":"Models - More than 2 groups","tags":null,"title":"Week 9","type":"book"},{"authors":null,"categories":null,"content":"   Data restructuring is often a useful tool to have. By data restructuring, I mean transforming data from long to wide format or vice versa. For the most part, long format is much easier to use when plotting and computing summary statistics. A related topic, called tidy data, can be read about in more detail here: http://www.jstatsoft.org/v59/i10/paper.\nThe data we are going to use for this section of notes is called “LongitudinalEx.csv” and can be found on the course website. The data are loaded directly from the web, but these could be loaded from a downloaded data file (see )\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.6 ✓ dplyr 1.0.7 ## ✓ tidyr 1.2.0 ✓ stringr 1.4.0 ## ✓ readr 2.1.2 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() long_data \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf-6250-blogdown/main/data/LongitudinalEx.csv\u0026quot;) ## Rows: 27 Columns: 7 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## dbl (7): id, wave, agegrp, age, piat, agegrp.c, age.c ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Long/Stacked Data The data read in above is in a format that is commonly referred to as long or stacked data.\nlong_data ## # A tibble: 27 × 7 ## id wave agegrp age piat agegrp.c age.c ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4 1 6.5 6 18 0 -0.5 ## 2 4 2 8.5 8.5 31 2 2 ## 3 4 3 10.5 10.7 50 4 4.17 ## 4 27 1 6.5 6.25 19 0 -0.25 ## 5 27 2 8.5 9.17 36 2 2.67 ## 6 27 3 10.5 10.9 57 4 4.42 ## 7 31 1 6.5 6.33 18 0 -0.167 ## 8 31 2 8.5 8.83 31 2 2.33 ## 9 31 3 10.5 10.9 51 4 4.42 ## 10 33 1 6.5 6.33 18 0 -0.167 ## # … with 17 more rows These data do not have one individual per row, instead each row is a individual by wave combination and are stacked for each individual (notice the three rows for id = 4). The variables in this case each have there own column in the data and all of them are time varying (change for each wave of data within an individual). This is also an example of “tidy data” from the paper linked to above, where each row is a unique observation (id, wave pair), variables are in the columns, and each cell of the data is a value.\nThe primary functions within the tidyr package are the following:\n pivot_longer(): for making the data longer, this replaces the gather() function from tidyr. pivot_wider(): for making the data wider, this replaces the spread() function from tidyr.  These two functions are relatively new and they still may change as they continue to be developed. I believe the old functions will be deprecated, but not removed entirely. This means they will not be actively developed any longer, but will remain in the tidyr package for the forseeable future.\nIt should also be noted there are mutiple R packages for data restructuring, including reshape and reshape2. The syntax for these has always been difficult for me to fully process and internalize, but they are incredibly powerful. I believe almost every data restructuring task can be accomplished by pivot_longer() and pivot_wider() (with some companion functions), but it may take a few extra steps to get to the desired structure.\n Extra Long Data To progress through data restructuring, we first need to transform this data is extra long format. This format is not entirely useful by itself, however it will help use show the use of a few functions from the tidyr package. To go to extra long data, we will make use of the pivot_longer() and unite functions.\nextra_long \u0026lt;- long_data %\u0026gt;% pivot_longer(agegrp:age.c, names_to = \u0026#39;variable\u0026#39;, values_to = \u0026#39;value\u0026#39;) %\u0026gt;% unite(var_wave, variable, wave) extra_long  ## # A tibble: 135 × 3 ## id var_wave value ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4 agegrp_1 6.5 ## 2 4 age_1 6 ## 3 4 piat_1 18 ## 4 4 agegrp.c_1 0 ## 5 4 age.c_1 -0.5 ## 6 4 agegrp_2 8.5 ## 7 4 age_2 8.5 ## 8 4 piat_2 31 ## 9 4 agegrp.c_2 2 ## 10 4 age.c_2 2 ## # … with 125 more rows You’ll notice now that there are only three columns in the data and that there are now 135 rows in data. This extra long data format gathered all of the variables into two columns, one that identify the variable and wave and the other that simply lists the value.\n Wide Data We can now take the extra long data and turn this into wide data. Wide data is characterized by one row per individual with columns representing the variable and wave combinations.\nwide \u0026lt;- extra_long %\u0026gt;% pivot_wider(names_from = var_wave, values_from = value) wide ## # A tibble: 9 × 16 ## id agegrp_1 age_1 piat_1 agegrp.c_1 age.c_1 agegrp_2 age_2 piat_2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4 6.5 6 18 0 -0.5 8.5 8.5 31 ## 2 27 6.5 6.25 19 0 -0.25 8.5 9.17 36 ## 3 31 6.5 6.33 18 0 -0.167 8.5 8.83 31 ## 4 33 6.5 6.33 18 0 -0.167 8.5 8.92 34 ## 5 41 6.5 6.33 18 0 -0.167 8.5 8.75 28 ## 6 49 6.5 6.5 19 0 0 8.5 8.75 32 ## 7 69 6.5 6.67 26 0 0.167 8.5 9.17 47 ## 8 77 6.5 6.83 17 0 0.333 8.5 8.08 19 ## 9 87 6.5 6.92 22 0 0.417 8.5 9.42 49 ## # … with 7 more variables: agegrp.c_2 \u0026lt;dbl\u0026gt;, age.c_2 \u0026lt;dbl\u0026gt;, agegrp_3 \u0026lt;dbl\u0026gt;, ## # age_3 \u0026lt;dbl\u0026gt;, piat_3 \u0026lt;dbl\u0026gt;, agegrp.c_3 \u0026lt;dbl\u0026gt;, age.c_3 \u0026lt;dbl\u0026gt; You’ll notice from the data above, there are now only 9 rows, but now 16 columns in the data. Each variable except for id now also has a number appended to it to represent the wave of the data.\nThis data structure is common, particularly for users of SPSS or Excel for data entry or processing. Unfortunately, when working with data in R (and in general), data in wide format is often difficult to work with. Therefore it is common to need to restructure the data from wide to long format.\n Back to Long Format Fortunately, we can use the same functions as we used above, but now in inverse to get from wide to long format.\nwide %\u0026gt;% pivot_longer(-id, names_to = \u0026quot;variable\u0026quot;, values_to = \u0026quot;value\u0026quot;) %\u0026gt;% separate(variable, into = c(\u0026#39;variable\u0026#39;, \u0026#39;wave\u0026#39;), sep = \u0026quot;_\u0026quot;) %\u0026gt;% arrange(id, wave) %\u0026gt;% pivot_wider(names_from = variable, values_from = value) ## # A tibble: 27 × 7 ## id wave agegrp age piat agegrp.c age.c ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4 1 6.5 6 18 0 -0.5 ## 2 4 2 8.5 8.5 31 2 2 ## 3 4 3 10.5 10.7 50 4 4.17 ## 4 27 1 6.5 6.25 19 0 -0.25 ## 5 27 2 8.5 9.17 36 2 2.67 ## 6 27 3 10.5 10.9 57 4 4.42 ## 7 31 1 6.5 6.33 18 0 -0.167 ## 8 31 2 8.5 8.83 31 2 2.33 ## 9 31 3 10.5 10.9 51 4 4.42 ## 10 33 1 6.5 6.33 18 0 -0.167 ## # … with 17 more rows This now is identical to the first data that we had. I would recommend working through the steps above to see what the data structure looks like in each intermediate step along the way. In addition, it is often not of interest to save the extra long data format, below is the code that would go directly from long to wide.\nlong_data %\u0026gt;% pivot_longer(agegrp:age.c, names_to = \u0026#39;variable\u0026#39;, values_to = \u0026#39;value\u0026#39;) %\u0026gt;% unite(var_wave, variable, wave) %\u0026gt;% pivot_wider(names_from = var_wave, values_from = value) ## # A tibble: 9 × 16 ## id agegrp_1 age_1 piat_1 agegrp.c_1 age.c_1 agegrp_2 age_2 piat_2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4 6.5 6 18 0 -0.5 8.5 8.5 31 ## 2 27 6.5 6.25 19 0 -0.25 8.5 9.17 36 ## 3 31 6.5 6.33 18 0 -0.167 8.5 8.83 31 ## 4 33 6.5 6.33 18 0 -0.167 8.5 8.92 34 ## 5 41 6.5 6.33 18 0 -0.167 8.5 8.75 28 ## 6 49 6.5 6.5 19 0 0 8.5 8.75 32 ## 7 69 6.5 6.67 26 0 0.167 8.5 9.17 47 ## 8 77 6.5 6.83 17 0 0.333 8.5 8.08 19 ## 9 87 6.5 6.92 22 0 0.417 8.5 9.42 49 ## # … with 7 more variables: agegrp.c_2 \u0026lt;dbl\u0026gt;, age.c_2 \u0026lt;dbl\u0026gt;, agegrp_3 \u0026lt;dbl\u0026gt;, ## # age_3 \u0026lt;dbl\u0026gt;, piat_3 \u0026lt;dbl\u0026gt;, agegrp.c_3 \u0026lt;dbl\u0026gt;, age.c_3 \u0026lt;dbl\u0026gt; Exercises Using the following data generation code, convert these data to long format.  set.seed(10) messy \u0026lt;- data.frame( id = 1:4, trt = sample(rep(c(\u0026#39;control\u0026#39;, \u0026#39;treatment\u0026#39;), each = 2)), work.T1 = runif(4), home.T1 = runif(4), work.T2 = runif(4), home.T2 = runif(4) ) Once successfully converted to long format, convert back to wide format.    ","date":1613606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613606400,"objectID":"d8787013f8cab391469dc0637f2c6584","permalink":"https://psqf6250.brandonlebeau.org/rcode/data_restructuring/","publishdate":"2021-02-18T00:00:00Z","relpermalink":"/rcode/data_restructuring/","section":"rcode","summary":"Data Restructuring","tags":null,"title":"Data Restructuring","type":"book"},{"authors":null,"categories":null,"content":"   Reproducible Rmarkdown Document: 10 pts\nDue: February 13th, 2021 - No penalty for late submissions, but due no later than May 8th.\nFor this assignment, you will create your first reproducible R Markdown document. The source file (the .Rmd) file will be turned in as well as the compiled version (html). The general form of the R code to be included in the document will be given to you. You may be asked to manipulate simple R commands. Submit completed assignment, including Rmd and html to ICON.\nR Markdown Setup Using RStudio, open up a new template for an R Markdown file. To do this, go to File \u0026gt; New File \u0026gt; R Markdown. A new window should open up, type in the details, and ensure that HTML Output is clicked. When finished entering in details, click the ‘okay’ button. Upon hitting the ‘okay’ button, a document template should show up. Read the elements in the template, then once comfortable with its contents, you can delete it and continue to the questions below.   Questions Using Markdown syntax, create a header that says Question 1. Note, you can pick any level of header you wish. Create subsequent headers for each question below. 1 pt\n Create an unordered list that lists your research interests (please list at least 2 interests here). 1 pt\n Create a hyperlink (link to a webpage) that links to the main R project website. Add the link in two ways, one that show the actual link and two, one that has the link embedded within text. You can pick whatever you wish for the link text. 1 pt\n Add a R code chunk to the document. Within this code chunk add the following R code: summary(iris). Ensure to give the chunk a unique name. 1 pt\n Using output from the R command used in the code chunk from question 4, create a table that summarizes the first four variables from the output. The syntax to create a table manually can be found in the Extended Markdown Syntax. More specifically, create a simple table by hand that describes the first row or two from the output of question 4. 1 pt\n Add a new R code chunk. Inside this code chunk add the following R code: hist(iris$Sepal.Length). Ensure to give the chunk a unique name and also add the chunk option to omit the code from being shown in the output. 1 pt\n Create an ordered list with Markdown that ranks your top vacations you have taken. Within each of your top vacations, add a nested item (e.g. https://commonmark.org/help/tutorial/10-nestedLists.html or https://www.markdownguide.org/basic-syntax#lists-1) to each of your vacations that states the top activities you did while on vaction. 1 pt\n Write some text that specifies the correlation between the continuous variables from the iris data. Place the following two bits of code inline in their relevant positions. The two correlations can be calculated with the following bit of code: round(cor(iris$Sepal.Length, iris$Sepal.Width), 2) and round(cor(iris$Petal.Length, iris$Petal.Width), 2) 1 pt\n Let’s now create another figure with the following R code, plot(iris$Sepal.Length, iris$Sepal.Width). Similar to above, give the chunk a unique name and also add the chunk option to omit the code from being shown in the output. For this figure, explore the knitr chunk options (https://yihui.name/knitr/options/) to add a figure caption to the figure. 1 pt\n Add one last code chunk (ensure this chunk has a unique name). Ensure through chunk options that the code is not evaluated, but the code is returned. 1 pt\n  x \u0026lt;- rnorm(100) y \u0026lt;- runif(100, min = 3, max = 8) mean(x) mean(y)  ","date":1609977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609977600,"objectID":"0125028e0a0739cc5b3cae5982c0f491","permalink":"https://psqf6250.brandonlebeau.org/assignments/assignment/assignment1/","publishdate":"2021-01-07T00:00:00Z","relpermalink":"/assignments/assignment/assignment1/","section":"assignments","summary":"Reproducible Rmarkdown Document: 10 pts\nDue: February 13th, 2021 - No penalty for late submissions, but due no later than May 8th.\nFor this assignment, you will create your first reproducible R Markdown document.","tags":null,"title":"Assignment 1","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week covers miscellaneous model related topics including model assumptions, non-linear models, and a brief introduction to generalized linear models.\nObjectives After completing this module, students will be able to:\n Define key model assumptions Identify model assumption violations Explore non-linear trends Create generalized linear models  Weekly Videos  Interactions - Continuous and Categorical    Statistical Assumption Checking with lm()    Modeling Non-Linear Trends    glm() Function   R Syntax  Advanced Modeling Model Assumptions Misc Modeling Topics  Assignments To come \u0026hellip;\n","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647216000,"objectID":"5404afacfa18580e60775ac443c64293","permalink":"https://psqf6250.brandonlebeau.org/content/10-week10/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/content/10-week10/","section":"content","summary":"Other Models","tags":null,"title":"Week 10","type":"book"},{"authors":null,"categories":null,"content":"   Graphics and Data Munging Practice: 10 pts\nDue: February 27th, 2022 - No penalty for late submissions, but due no later than May 8th.\nFor this assignment, you will build upon the skills you learned in the first reproducible R Markdown document created in the first assignment. In this assignment, you will explore some data graphically to inform a few research questions using data from the fivethirtyeight package. The source file (the .Rmd) file will be turned in as well as the compiled version (html). Note, please create a new Rmd document for this assignment rather than continue the one from the first assignment. Submit completed assignment, including Rmd and html to ICON.\nAll graphics should be of high quality, this includes formatting of axes, axes labels, etc. If none of the graphics are of high quality, a 2 pt penalty will apply over and above any item-specific reductions.\nResearch Questions The following research questions will be used to guide the assignment, but you do not need to answer these directly. The questions below will reference these questions.\nUsing the college_recent_grads data from the fivethirtyeight package, which majors are the most popular? Using the college_recent_grads data from the fivethirtyeight package, which major categories (not individual majors, but bigger major categories, major_category) are most unisex (i.e., have an equal number of males/females in them.)? Related to #2, which major categories have the most disproportionate number of males or females with that major category? Is there any evidence of a relationship between unemployment rate and the popularity of a major? What about median salary (shown by median) and the popularity of a major?   Questions Using an appropriate verb from dplyr, which majors are the most popular? Don’t print all the data in the output file to answer this question, keep this summary concise. 1 pt\n Which majors are the least popular? Don’t print all the data in the output file to answer this question, keep this summary concise. 1 pt\n Explore the distribution of the variable/attribute sharewomen visually. Summarize characteristics of this variable in a few sentences. Be sure to include any figure(s) or statistics as evidence to support your description. 1 pt\n Which major categories are the most unisex and which major categories are the most disproportionate? Similar to #1 and #2, please don’t print all of the majors, just highlight a few in each category. 1 pt\n Create a figure that effectively shows which major categories are the most unisex and disproportionate in a single figure. Discuss briefly why this figure is effective at answering research question 2 and 3. 1 pt\n Create a figure that explores if there is a relationship between the popularity of a major and the unemployment rate. Discuss briefly how you defined popularity and why this figure helps to show the relationship between the two attributes. Be sure in your discussion to also state your interpretation of the figure. 1 pt\n Building off of #6, create a figure that explores if there is a relationship between the popularity of a major and the median salary (shown with the median attribute). Discuss how you defined popularity and why this figure helps to show the relationship between the two attributes. Be sure in your discussion to also state your interpretation of the figure. 1 pt\n Create a figure that explores if there is a relationship between the popularity of a major category and the unemployment rate. Discuss why this figure helps to show the relationship between the two attributes and also discuss how this figure may differ from the one created in #6. What additional features did you need to consider to make an effective visualization of this relationship given the data structure. Be sure in your discussion to also state your interpretation of the figure. Note, you may wish to use dplyr and ggplot2 to help with this question. 1 pt\n Create a figure that explores if there is a relationship between the popularity of a major category and the median salary (shown with the median attribute). Discuss why this figure helps to show the relationship between the two attributes and also discuss how this figure may differ from the one created in #6. What additional features did you need to consider to make an effective visualization of this relationship given the data structure. Be sure in your discussion to also state your interpretation of the figure. Note, you may wish to use dplyr and ggplot2 to help with this question. 1 pt\n Identify a new research question from the college_recent_grads that interests you. State this research question, then create a figure that highlights/explores the research question. Discuss briefly why this figure does a good job of exploring the research question. What challenges did you have creating the figure? 1 pt\n   ","date":1644364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644364800,"objectID":"9a379d77e6562030f2ffc376339cf992","permalink":"https://psqf6250.brandonlebeau.org/assignments/assignment/assignment2/","publishdate":"2022-02-09T00:00:00Z","relpermalink":"/assignments/assignment/assignment2/","section":"assignments","summary":"Graphics and Data Munging Practice: 10 pts\nDue: February 27th, 2022 - No penalty for late submissions, but due no later than May 8th.\nFor this assignment, you will build upon the skills you learned in the first reproducible R Markdown document created in the first assignment.","tags":null,"title":"Assignment 2","type":"book"},{"authors":null,"categories":null,"content":"   Another common data manipulation task is to join multiple data sources into a single data file for an analysis. This task is most easily accomplished using a set of join functions found in the dplyr package. In this set of notes we are going to focus on mutating joins and filtering joins. There is another class of joins called set operations. I use these much less frequently, but for those interested, see the text in the R for Data Science book http://r4ds.had.co.nz/relational-data.html.\nFor this set of notes, we are going to make use of two packages:\nlibrary(tidyverse) # install.packages(\u0026#39;Lahman\u0026#39;) library(Lahman) The Lahman package contains data from the Major League Baseball (MLB), a professional baseball association in the United States. For this section, we are going to focus on the following three data tables, Teams, Salaries, and Managers. I print the first ten rows of the data for each table below.\nhead(Teams, n = 10) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin ## 1 1871 NA BS1 BNA \u0026lt;NA\u0026gt; 3 31 NA 20 10 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 2 1871 NA CH1 CNA \u0026lt;NA\u0026gt; 2 28 NA 19 9 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 3 1871 NA CL1 CFC \u0026lt;NA\u0026gt; 8 29 NA 10 19 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 4 1871 NA FW1 KEK \u0026lt;NA\u0026gt; 7 19 NA 7 12 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 5 1871 NA NY2 NNA \u0026lt;NA\u0026gt; 5 33 NA 16 17 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 6 1871 NA PH1 PNA \u0026lt;NA\u0026gt; 1 28 NA 21 7 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Y ## 7 1871 NA RC1 ROK \u0026lt;NA\u0026gt; 9 25 NA 4 21 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 8 1871 NA TRO TRO \u0026lt;NA\u0026gt; 6 29 NA 13 15 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 9 1871 NA WS3 OLY \u0026lt;NA\u0026gt; 4 32 NA 15 15 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 10 1872 NA BL1 BLC \u0026lt;NA\u0026gt; 2 58 NA 35 19 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV ## 1 \u0026lt;NA\u0026gt; 401 1372 426 70 37 3 60 19 73 16 NA NA 303 109 3.55 22 1 3 ## 2 \u0026lt;NA\u0026gt; 302 1196 323 52 21 10 60 22 69 21 NA NA 241 77 2.76 25 0 1 ## 3 \u0026lt;NA\u0026gt; 249 1186 328 35 40 7 26 25 18 8 NA NA 341 116 4.11 23 0 0 ## 4 \u0026lt;NA\u0026gt; 137 746 178 19 8 2 33 9 16 4 NA NA 243 97 5.17 19 1 0 ## 5 \u0026lt;NA\u0026gt; 302 1404 403 43 21 1 33 15 46 15 NA NA 313 121 3.72 32 1 0 ## 6 \u0026lt;NA\u0026gt; 376 1281 410 66 27 9 46 23 56 12 NA NA 266 137 4.95 27 0 0 ## 7 \u0026lt;NA\u0026gt; 231 1036 274 44 25 3 38 30 53 10 NA NA 287 108 4.30 23 1 0 ## 8 \u0026lt;NA\u0026gt; 351 1248 384 51 34 6 49 19 62 24 NA NA 362 153 5.51 28 0 0 ## 9 \u0026lt;NA\u0026gt; 310 1353 375 54 26 6 48 13 48 13 NA NA 303 137 4.37 32 0 0 ## 10 \u0026lt;NA\u0026gt; 617 2571 753 106 31 14 29 28 53 18 NA NA 434 166 2.90 48 1 1 ## IPouts HA HRA BBA SOA E DP FP name ## 1 828 367 2 42 23 243 24 0.834 Boston Red Stockings ## 2 753 308 6 28 22 229 16 0.829 Chicago White Stockings ## 3 762 346 13 53 34 234 15 0.818 Cleveland Forest Citys ## 4 507 261 5 21 17 163 8 0.803 Fort Wayne Kekiongas ## 5 879 373 7 42 22 235 14 0.840 New York Mutuals ## 6 747 329 3 53 16 194 13 0.845 Philadelphia Athletics ## 7 678 315 3 34 16 220 14 0.821 Rockford Forest Citys ## 8 750 431 4 75 12 198 22 0.845 Troy Haymakers ## 9 846 371 4 45 13 218 20 0.850 Washington Olympics ## 10 1548 573 3 63 77 432 22 0.830 Baltimore Canaries ## park attendance BPF PPF teamIDBR teamIDlahman45 ## 1 South End Grounds I NA 103 98 BOS BS1 ## 2 Union Base-Ball Grounds NA 104 102 CHI CH1 ## 3 National Association Grounds NA 96 100 CLE CL1 ## 4 Hamilton Field NA 101 107 KEK FW1 ## 5 Union Grounds (Brooklyn) NA 90 88 NYU NY2 ## 6 Jefferson Street Grounds NA 102 98 ATH PH1 ## 7 Agricultural Society Fair Grounds NA 97 99 ROK RC1 ## 8 Haymakers\u0026#39; Grounds NA 101 100 TRO TRO ## 9 Olympics Grounds NA 94 98 OLY WS3 ## 10 Newington Park NA 106 102 BAL BL1 ## teamIDretro ## 1 BS1 ## 2 CH1 ## 3 CL1 ## 4 FW1 ## 5 NY2 ## 6 PH1 ## 7 RC1 ## 8 TRO ## 9 WS3 ## 10 BL1 head(Salaries, n = 10) ## yearID teamID lgID playerID salary ## 1 1985 ATL NL barkele01 870000 ## 2 1985 ATL NL bedrost01 550000 ## 3 1985 ATL NL benedbr01 545000 ## 4 1985 ATL NL campri01 633333 ## 5 1985 ATL NL ceronri01 625000 ## 6 1985 ATL NL chambch01 800000 ## 7 1985 ATL NL dedmoje01 150000 ## 8 1985 ATL NL forstte01 483333 ## 9 1985 ATL NL garbege01 772000 ## 10 1985 ATL NL harpete01 250000 head(Managers, n = 10) ## playerID yearID teamID lgID inseason G W L rank plyrMgr ## 1 wrighha01 1871 BS1 NA 1 31 20 10 3 Y ## 2 woodji01 1871 CH1 NA 1 28 19 9 2 Y ## 3 paborch01 1871 CL1 NA 1 29 10 19 8 Y ## 4 lennobi01 1871 FW1 NA 1 14 5 9 8 Y ## 5 deaneha01 1871 FW1 NA 2 5 2 3 8 Y ## 6 fergubo01 1871 NY2 NA 1 33 16 17 5 Y ## 7 mcbridi01 1871 PH1 NA 1 28 21 7 1 Y ## 8 hastisc01 1871 RC1 NA 1 25 4 21 9 Y ## 9 pikeli01 1871 TRO NA 1 4 1 3 6 Y ## 10 cravebi01 1871 TRO NA 2 25 12 12 6 Y Inner Join The most basic join is the inner join. This join takes two tables and returns values if key variables match in both tables. If rows do not match on the key variables, these observations are removed. Suppose for example, we wanted to select the rows that matched between the Teams and Salaries data. This would be useful for example if we wished to calculate the average salary of the players for each team for every year.\nThis join could be done with the inner_join function.\nteam_salary \u0026lt;- inner_join(Teams, Salaries) ## Joining, by = c(\u0026quot;yearID\u0026quot;, \u0026quot;lgID\u0026quot;, \u0026quot;teamID\u0026quot;) head(team_salary, n = 10) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin ## 1 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 2 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 3 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 4 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 5 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 6 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 7 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 8 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 9 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 10 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV ## 1 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 2 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 3 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 4 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 5 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 6 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 7 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 8 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 9 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 10 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## IPouts HA HRA BBA SOA E DP FP name ## 1 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 2 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 3 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 4 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 5 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 6 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 7 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 8 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 9 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 10 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## park attendance BPF PPF teamIDBR teamIDlahman45 ## 1 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 2 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 3 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 4 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 5 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 6 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 7 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 8 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 9 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 10 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## teamIDretro playerID salary ## 1 ATL barkele01 870000 ## 2 ATL bedrost01 550000 ## 3 ATL benedbr01 545000 ## 4 ATL campri01 633333 ## 5 ATL ceronri01 625000 ## 6 ATL chambch01 800000 ## 7 ATL dedmoje01 150000 ## 8 ATL forstte01 483333 ## 9 ATL garbege01 772000 ## 10 ATL harpete01 250000 You’ll notice that there is only data from 1985 onward, the data in the Teams data from before 1985 have automatically been removed due to no matching data in the Salaries data. You may have also noticed, that I did not specify the variables to join by above, for interactive work this can be okay, but to be more reproducible, specifying the variables to join on would be better. The function call above can be modified to include this information.\nteam_salary \u0026lt;- inner_join(Teams, Salaries, by = c(\u0026#39;yearID\u0026#39;, \u0026#39;teamID\u0026#39;, \u0026#39;lgID\u0026#39;)) head(team_salary, n = 10) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin ## 1 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 2 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 3 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 4 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 5 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 6 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 7 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 8 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 9 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 10 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV ## 1 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 2 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 3 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 4 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 5 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 6 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 7 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 8 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 9 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 10 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## IPouts HA HRA BBA SOA E DP FP name ## 1 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 2 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 3 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 4 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 5 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 6 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 7 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 8 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 9 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 10 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## park attendance BPF PPF teamIDBR teamIDlahman45 ## 1 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 2 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 3 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 4 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 5 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 6 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 7 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 8 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 9 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 10 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## teamIDretro playerID salary ## 1 ATL barkele01 870000 ## 2 ATL bedrost01 550000 ## 3 ATL benedbr01 545000 ## 4 ATL campri01 633333 ## 5 ATL ceronri01 625000 ## 6 ATL chambch01 800000 ## 7 ATL dedmoje01 150000 ## 8 ATL forstte01 483333 ## 9 ATL garbege01 772000 ## 10 ATL harpete01 250000 We could then use other dplyr verbs to calculate the average salary for every team by year and plot these.\nteam_salary %\u0026gt;% group_by(yearID, teamID) %\u0026gt;% summarise(avg_salary = mean(salary, na.rm = TRUE)) %\u0026gt;% ggplot(aes(x = yearID, y = avg_salary)) + geom_line(size = 1) + facet_wrap(~teamID) ## `summarise()` has grouped output by \u0026#39;yearID\u0026#39;. You can override using the ## `.groups` argument. Below is a diagram of the inner join found in the R for Data Science text, inner join diagram.\n Left Join This is by far the most common join I perform. Left join is more formally part of a group of operations called outer joins. Outer joins are useful when you want to use one data table as a base data set in which variables will be added to this data if the keys match. It is likely best shown with an example.\nSuppose we wish to add the salary information to the Teams data. However, instead of using a inner_join, let’s use left_join to see the difference. Note: I print only 10 rows of data with the head() function. This part of the code below would generally not be used.\nleft_join(Teams, Salaries) %\u0026gt;% head(n = 10) ## Joining, by = c(\u0026quot;yearID\u0026quot;, \u0026quot;lgID\u0026quot;, \u0026quot;teamID\u0026quot;) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin ## 1 1871 NA BS1 BNA \u0026lt;NA\u0026gt; 3 31 NA 20 10 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 2 1871 NA CH1 CNA \u0026lt;NA\u0026gt; 2 28 NA 19 9 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 3 1871 NA CL1 CFC \u0026lt;NA\u0026gt; 8 29 NA 10 19 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 4 1871 NA FW1 KEK \u0026lt;NA\u0026gt; 7 19 NA 7 12 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 5 1871 NA NY2 NNA \u0026lt;NA\u0026gt; 5 33 NA 16 17 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 6 1871 NA PH1 PNA \u0026lt;NA\u0026gt; 1 28 NA 21 7 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Y ## 7 1871 NA RC1 ROK \u0026lt;NA\u0026gt; 9 25 NA 4 21 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 8 1871 NA TRO TRO \u0026lt;NA\u0026gt; 6 29 NA 13 15 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 9 1871 NA WS3 OLY \u0026lt;NA\u0026gt; 4 32 NA 15 15 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 10 1872 NA BL1 BLC \u0026lt;NA\u0026gt; 2 58 NA 35 19 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV ## 1 \u0026lt;NA\u0026gt; 401 1372 426 70 37 3 60 19 73 16 NA NA 303 109 3.55 22 1 3 ## 2 \u0026lt;NA\u0026gt; 302 1196 323 52 21 10 60 22 69 21 NA NA 241 77 2.76 25 0 1 ## 3 \u0026lt;NA\u0026gt; 249 1186 328 35 40 7 26 25 18 8 NA NA 341 116 4.11 23 0 0 ## 4 \u0026lt;NA\u0026gt; 137 746 178 19 8 2 33 9 16 4 NA NA 243 97 5.17 19 1 0 ## 5 \u0026lt;NA\u0026gt; 302 1404 403 43 21 1 33 15 46 15 NA NA 313 121 3.72 32 1 0 ## 6 \u0026lt;NA\u0026gt; 376 1281 410 66 27 9 46 23 56 12 NA NA 266 137 4.95 27 0 0 ## 7 \u0026lt;NA\u0026gt; 231 1036 274 44 25 3 38 30 53 10 NA NA 287 108 4.30 23 1 0 ## 8 \u0026lt;NA\u0026gt; 351 1248 384 51 34 6 49 19 62 24 NA NA 362 153 5.51 28 0 0 ## 9 \u0026lt;NA\u0026gt; 310 1353 375 54 26 6 48 13 48 13 NA NA 303 137 4.37 32 0 0 ## 10 \u0026lt;NA\u0026gt; 617 2571 753 106 31 14 29 28 53 18 NA NA 434 166 2.90 48 1 1 ## IPouts HA HRA BBA SOA E DP FP name ## 1 828 367 2 42 23 243 24 0.834 Boston Red Stockings ## 2 753 308 6 28 22 229 16 0.829 Chicago White Stockings ## 3 762 346 13 53 34 234 15 0.818 Cleveland Forest Citys ## 4 507 261 5 21 17 163 8 0.803 Fort Wayne Kekiongas ## 5 879 373 7 42 22 235 14 0.840 New York Mutuals ## 6 747 329 3 53 16 194 13 0.845 Philadelphia Athletics ## 7 678 315 3 34 16 220 14 0.821 Rockford Forest Citys ## 8 750 431 4 75 12 198 22 0.845 Troy Haymakers ## 9 846 371 4 45 13 218 20 0.850 Washington Olympics ## 10 1548 573 3 63 77 432 22 0.830 Baltimore Canaries ## park attendance BPF PPF teamIDBR teamIDlahman45 ## 1 South End Grounds I NA 103 98 BOS BS1 ## 2 Union Base-Ball Grounds NA 104 102 CHI CH1 ## 3 National Association Grounds NA 96 100 CLE CL1 ## 4 Hamilton Field NA 101 107 KEK FW1 ## 5 Union Grounds (Brooklyn) NA 90 88 NYU NY2 ## 6 Jefferson Street Grounds NA 102 98 ATH PH1 ## 7 Agricultural Society Fair Grounds NA 97 99 ROK RC1 ## 8 Haymakers\u0026#39; Grounds NA 101 100 TRO TRO ## 9 Olympics Grounds NA 94 98 OLY WS3 ## 10 Newington Park NA 106 102 BAL BL1 ## teamIDretro playerID salary ## 1 BS1 \u0026lt;NA\u0026gt; NA ## 2 CH1 \u0026lt;NA\u0026gt; NA ## 3 CL1 \u0026lt;NA\u0026gt; NA ## 4 FW1 \u0026lt;NA\u0026gt; NA ## 5 NY2 \u0026lt;NA\u0026gt; NA ## 6 PH1 \u0026lt;NA\u0026gt; NA ## 7 RC1 \u0026lt;NA\u0026gt; NA ## 8 TRO \u0026lt;NA\u0026gt; NA ## 9 WS3 \u0026lt;NA\u0026gt; NA ## 10 BL1 \u0026lt;NA\u0026gt; NA The first thing to notice is that now there are years in the yearID variable from before 1985, this was not the case in the above data joined using inner_join. If you scroll over to explore variables to the right, there are missing values for the salary variable. What left_join does when it doesn’t find a match in the table is to produce NA values, so all records within the joined data will be NA before 1985.\nThis is the major difference between outer joins and inner joins. Outer joins will preserve data in the keyed data that do not match and NA values are returned for non-matching values. For inner joins, any keys that do not match are removed.\n Right Join A right join is similar to a left join, except the keyed table is the second one specified (the rightmost data). For example, if we wished for the salary information to be the keyed table, we could do that same specification as above, but use right_join instead of left_join. Note: I print only 10 rows of data with the head() function. This part of the code below would generally not be used.\nright_join(Teams, Salaries) %\u0026gt;% head(n = 10) ## Joining, by = c(\u0026quot;yearID\u0026quot;, \u0026quot;lgID\u0026quot;, \u0026quot;teamID\u0026quot;) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin ## 1 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 2 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 3 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 4 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 5 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 6 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 7 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 8 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 9 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 10 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV ## 1 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 2 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 3 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 4 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 5 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 6 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 7 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 8 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 9 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 10 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## IPouts HA HRA BBA SOA E DP FP name ## 1 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 2 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 3 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 4 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 5 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 6 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 7 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 8 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 9 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 10 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## park attendance BPF PPF teamIDBR teamIDlahman45 ## 1 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 2 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 3 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 4 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 5 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 6 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 7 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 8 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 9 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 10 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## teamIDretro playerID salary ## 1 ATL barkele01 870000 ## 2 ATL bedrost01 550000 ## 3 ATL benedbr01 545000 ## 4 ATL campri01 633333 ## 5 ATL ceronri01 625000 ## 6 ATL chambch01 800000 ## 7 ATL dedmoje01 150000 ## 8 ATL forstte01 483333 ## 9 ATL garbege01 772000 ## 10 ATL harpete01 250000 This data is very similar (although not identical) to the one from the inner join above. Can you spot what is different?\n Full Join Full join is the last type of outer join and this will return all values from both tables and NAs will be given for those keys that do not match. For example,\nfull_join(Teams, Salaries) %\u0026gt;% head(n = 10) ## Joining, by = c(\u0026quot;yearID\u0026quot;, \u0026quot;lgID\u0026quot;, \u0026quot;teamID\u0026quot;) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin ## 1 1871 NA BS1 BNA \u0026lt;NA\u0026gt; 3 31 NA 20 10 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 2 1871 NA CH1 CNA \u0026lt;NA\u0026gt; 2 28 NA 19 9 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 3 1871 NA CL1 CFC \u0026lt;NA\u0026gt; 8 29 NA 10 19 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 4 1871 NA FW1 KEK \u0026lt;NA\u0026gt; 7 19 NA 7 12 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 5 1871 NA NY2 NNA \u0026lt;NA\u0026gt; 5 33 NA 16 17 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 6 1871 NA PH1 PNA \u0026lt;NA\u0026gt; 1 28 NA 21 7 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Y ## 7 1871 NA RC1 ROK \u0026lt;NA\u0026gt; 9 25 NA 4 21 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 8 1871 NA TRO TRO \u0026lt;NA\u0026gt; 6 29 NA 13 15 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 9 1871 NA WS3 OLY \u0026lt;NA\u0026gt; 4 32 NA 15 15 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## 10 1872 NA BL1 BLC \u0026lt;NA\u0026gt; 2 58 NA 35 19 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; N ## WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV ## 1 \u0026lt;NA\u0026gt; 401 1372 426 70 37 3 60 19 73 16 NA NA 303 109 3.55 22 1 3 ## 2 \u0026lt;NA\u0026gt; 302 1196 323 52 21 10 60 22 69 21 NA NA 241 77 2.76 25 0 1 ## 3 \u0026lt;NA\u0026gt; 249 1186 328 35 40 7 26 25 18 8 NA NA 341 116 4.11 23 0 0 ## 4 \u0026lt;NA\u0026gt; 137 746 178 19 8 2 33 9 16 4 NA NA 243 97 5.17 19 1 0 ## 5 \u0026lt;NA\u0026gt; 302 1404 403 43 21 1 33 15 46 15 NA NA 313 121 3.72 32 1 0 ## 6 \u0026lt;NA\u0026gt; 376 1281 410 66 27 9 46 23 56 12 NA NA 266 137 4.95 27 0 0 ## 7 \u0026lt;NA\u0026gt; 231 1036 274 44 25 3 38 30 53 10 NA NA 287 108 4.30 23 1 0 ## 8 \u0026lt;NA\u0026gt; 351 1248 384 51 34 6 49 19 62 24 NA NA 362 153 5.51 28 0 0 ## 9 \u0026lt;NA\u0026gt; 310 1353 375 54 26 6 48 13 48 13 NA NA 303 137 4.37 32 0 0 ## 10 \u0026lt;NA\u0026gt; 617 2571 753 106 31 14 29 28 53 18 NA NA 434 166 2.90 48 1 1 ## IPouts HA HRA BBA SOA E DP FP name ## 1 828 367 2 42 23 243 24 0.834 Boston Red Stockings ## 2 753 308 6 28 22 229 16 0.829 Chicago White Stockings ## 3 762 346 13 53 34 234 15 0.818 Cleveland Forest Citys ## 4 507 261 5 21 17 163 8 0.803 Fort Wayne Kekiongas ## 5 879 373 7 42 22 235 14 0.840 New York Mutuals ## 6 747 329 3 53 16 194 13 0.845 Philadelphia Athletics ## 7 678 315 3 34 16 220 14 0.821 Rockford Forest Citys ## 8 750 431 4 75 12 198 22 0.845 Troy Haymakers ## 9 846 371 4 45 13 218 20 0.850 Washington Olympics ## 10 1548 573 3 63 77 432 22 0.830 Baltimore Canaries ## park attendance BPF PPF teamIDBR teamIDlahman45 ## 1 South End Grounds I NA 103 98 BOS BS1 ## 2 Union Base-Ball Grounds NA 104 102 CHI CH1 ## 3 National Association Grounds NA 96 100 CLE CL1 ## 4 Hamilton Field NA 101 107 KEK FW1 ## 5 Union Grounds (Brooklyn) NA 90 88 NYU NY2 ## 6 Jefferson Street Grounds NA 102 98 ATH PH1 ## 7 Agricultural Society Fair Grounds NA 97 99 ROK RC1 ## 8 Haymakers\u0026#39; Grounds NA 101 100 TRO TRO ## 9 Olympics Grounds NA 94 98 OLY WS3 ## 10 Newington Park NA 106 102 BAL BL1 ## teamIDretro playerID salary ## 1 BS1 \u0026lt;NA\u0026gt; NA ## 2 CH1 \u0026lt;NA\u0026gt; NA ## 3 CL1 \u0026lt;NA\u0026gt; NA ## 4 FW1 \u0026lt;NA\u0026gt; NA ## 5 NY2 \u0026lt;NA\u0026gt; NA ## 6 PH1 \u0026lt;NA\u0026gt; NA ## 7 RC1 \u0026lt;NA\u0026gt; NA ## 8 TRO \u0026lt;NA\u0026gt; NA ## 9 WS3 \u0026lt;NA\u0026gt; NA ## 10 BL1 \u0026lt;NA\u0026gt; NA Note: I print only 10 rows of data with the head() function. This part of the code below would generally not be used.\nThis data is very similar to the left join above, but not identical, can you tell the difference again?\nBelow is a diagram of the differences between the three outer joins from the R for Data Science text, outer joins diagram.\n Filtering Joins I tend to not use filtering joins, however, these are useful to connect summary data back to the original rows in the data. For example, using the team_salary data created above, let’s select only the top 10 teams in terms of average salary from the year 2015.\ntop_salary_15 \u0026lt;- team_salary %\u0026gt;% group_by(yearID, teamID) %\u0026gt;% summarise(avg_salary = mean(salary, na.rm = TRUE)) %\u0026gt;% filter(yearID == 2015) %\u0026gt;% arrange(desc(avg_salary)) %\u0026gt;% head(10) ## `summarise()` has grouped output by \u0026#39;yearID\u0026#39;. You can override using the ## `.groups` argument. top_salary_15 ## # A tibble: 10 × 3 ## # Groups: yearID [1] ## yearID teamID avg_salary ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2015 LAN 7441103. ## 2 2015 NYA 7336274. ## 3 2015 DET 6891390 ## 4 2015 SFN 6100056. ## 5 2015 BOS 5659481. ## 6 2015 WAS 5365085. ## 7 2015 SEA 4888348 ## 8 2015 TEX 4791426. ## 9 2015 SLN 4586212. ## 10 2015 SDN 4555435. Although not impossible, it would be a bit more difficult to connect these teams and years back to the original data in the team_salary data. This can be done simply with a filtering join, namely a semi join.\nteam_salary %\u0026gt;% semi_join(top_salary_15) %\u0026gt;% head(n = 10) ## Joining, by = c(\u0026quot;yearID\u0026quot;, \u0026quot;teamID\u0026quot;) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin ## 1 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 2 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 3 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 4 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 5 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 6 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 7 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 8 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 9 2015 AL BOS BOS E 5 162 81 78 84 N N N ## 10 2015 AL BOS BOS E 5 162 81 78 84 N N N ## WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV ## 1 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 2 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 3 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 4 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 5 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 6 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 7 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 8 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 9 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## 10 N 748 5640 1495 294 33 161 478 1148 71 27 46 42 753 694 4.31 3 10 40 ## IPouts HA HRA BBA SOA E DP FP name park ## 1 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 2 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 3 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 4 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 5 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 6 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 7 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 8 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 9 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## 10 4345 1486 178 478 1218 97 148 0.984 Boston Red Sox Fenway Park II ## attendance BPF PPF teamIDBR teamIDlahman45 teamIDretro playerID salary ## 1 2880694 104 107 BOS BOS BOS barnema01 508500 ## 2 2880694 104 107 BOS BOS BOS bettsmo01 514500 ## 3 2880694 104 107 BOS BOS BOS bogaexa01 543000 ## 4 2880694 104 107 BOS BOS BOS bradlja02 528000 ## 5 2880694 104 107 BOS BOS BOS breslcr01 2000000 ## 6 2880694 104 107 BOS BOS BOS buchhcl01 12000000 ## 7 2880694 104 107 BOS BOS BOS castiru01 11271000 ## 8 2880694 104 107 BOS BOS BOS cecchga01 508500 ## 9 2880694 104 107 BOS BOS BOS craigal01 5500000 ## 10 2880694 104 107 BOS BOS BOS hanigry01 3500000 Note: I print only 10 rows of data with the head() function. This part of the code below would generally not be used.\nThis operation selected only the rows that had the matching keys from the first table (note that the columns were not touched).\nThe opposite operation is to use an anti join, in this type of join, the rows that do not match will be returned.\nteam_salary %\u0026gt;% anti_join(top_salary_15) %\u0026gt;% head(n = 10) ## Joining, by = c(\u0026quot;yearID\u0026quot;, \u0026quot;teamID\u0026quot;) ## yearID lgID teamID franchID divID Rank G Ghome W L DivWin WCWin LgWin ## 1 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 2 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 3 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 4 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 5 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 6 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 7 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 8 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 9 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## 10 1985 NL ATL ATL W 5 162 81 66 96 N \u0026lt;NA\u0026gt; N ## WSWin R AB H X2B X3B HR BB SO SB CS HBP SF RA ER ERA CG SHO SV ## 1 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 2 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 3 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 4 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 5 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 6 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 7 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 8 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 9 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## 10 N 632 5526 1359 213 28 126 553 849 72 52 22 41 781 679 4.19 9 9 29 ## IPouts HA HRA BBA SOA E DP FP name ## 1 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 2 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 3 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 4 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 5 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 6 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 7 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 8 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 9 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## 10 4372 1512 134 642 776 159 197 0.976 Atlanta Braves ## park attendance BPF PPF teamIDBR teamIDlahman45 ## 1 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 2 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 3 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 4 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 5 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 6 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 7 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 8 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 9 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## 10 Atlanta-Fulton County Stadium 1350137 105 106 ATL ATL ## teamIDretro playerID salary ## 1 ATL barkele01 870000 ## 2 ATL bedrost01 550000 ## 3 ATL benedbr01 545000 ## 4 ATL campri01 633333 ## 5 ATL ceronri01 625000 ## 6 ATL chambch01 800000 ## 7 ATL dedmoje01 150000 ## 8 ATL forstte01 483333 ## 9 ATL garbege01 772000 ## 10 ATL harpete01 250000 Note: I print only 10 rows of data with the head() function. This part of the code below would generally not be used.\nThe sum of the number of rows in these two tables should equal the number of rows from the entire team_salary data table\nanti_rows \u0026lt;- team_salary %\u0026gt;% anti_join(top_salary_15) %\u0026gt;% nrow() ## Joining, by = c(\u0026quot;yearID\u0026quot;, \u0026quot;teamID\u0026quot;) semi_rows \u0026lt;- team_salary %\u0026gt;% semi_join(top_salary_15) %\u0026gt;% nrow() ## Joining, by = c(\u0026quot;yearID\u0026quot;, \u0026quot;teamID\u0026quot;) anti_rows + semi_rows == nrow(team_salary) ## [1] TRUE Exercises Using the Teams and Managers data, join the two tables and only keep the matching observations in both tables. Note, you may need to specify the column names directly you wish to join by. What happens to the columns that have the same names but are not keys? Using the same data tables from #1, add all the Managers variables to the Teams data while retaining all the rows for the Teams data.    ","date":1613606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613606400,"objectID":"c7fda7714c6a4e635ab0cdebf8f263b5","permalink":"https://psqf6250.brandonlebeau.org/rcode/data_joins/","publishdate":"2021-02-18T00:00:00Z","relpermalink":"/rcode/data_joins/","section":"rcode","summary":"Joining Data","tags":null,"title":"Joining Data","type":"book"},{"authors":null,"categories":null,"content":"Exploratory Data Analysis and Data Restructuring: 10 pts\nDue: Around March 27th, 2021 - No penalty for late submissions, but due no later than May 8th.\n In this assignment, you will perform exploratory data analysis using the gss_cat data from the forcats package. The source file (the .Rmd) file will be turned in as well as the compiled version (html). Create a new .Rmd file for this assignment. You also do not need to explicitly answer the research question, rather this is used to guide the first part of the assignment.\nYou also do not need to include the code for most questions in the compiled (html) version since the Rmd will be turned in as well. If a question asks for code or there is particular elements of code you\u0026rsquo;d like to share, you can definitely include those in the final compiled version, but please limit those sections to specific elements rather than long chunks of code. Long chunks of code are best left for the Rmd document.\nThe source file (the .Rmd) file will be turned in as well as the compiled version (html). Note, please create a new Rmd document for this assignment rather than continue the one from the first assignment. Submit completed assignment, including Rmd and html to ICON.\nAll graphics should be of high quality, this includes formatting of axes, axes labels, etc. If none of the graphics are of high quality, a 2 pt penalty will apply over and above any item-specific reductions.\nResearch Questions The following research questions will be used to guide the assignment, but you do not need to answer these directly. The questions below will reference these questions.\n How is age related to the amount of television a person reports watching?  Questions   Does there appear to be patterns in the missing data from the variable tvhours by different income and age levels? Provide evidence for your reasoning. 1 pt\n  Provide a descriptive analysis exploring the research question above. Does age appear to be related to the amount of time a person spends watching television? Ensure in your discussion you provide justification for why you feel one way or the other and be as descriptive as possible. 1 pt\n   Data Import, Restructuring, Joining\nIn this part of the assignment, you will import data and perform data manipulations on this data file.\nQuestions   Read in the \u0026ldquo;ECLS_6250.csv\u0026rdquo; data file from the course website.\n Using the head() function, print the first few rows of the data and using the dim() function, print out the dimensions of the data. 1 pt    Verify that the data that are indeed missing are read in as missing values. Use the \u0026ldquo;ECLS_6250.pdf\u0026rdquo; codebook (found on the course website) to confirm values of missing data for each variable, these are listed for each variable directly in the codebook.\n Compute the mean of the following two variables: \u0026ldquo;C4R4MSCL\u0026rdquo; and \u0026ldquo;W3SESL\u0026rdquo;. 1 pt    Using the tidyr package, convert this data into an extra long format where the variables CHILDID, KURBAN_R, GENDER, and RACE are the id variables. The other data attributes would all represent data values to be restructured.\n Using the head() function, print the first few rows of the data and using the dim() function, print out the dimensions of the data. Note, you should have 6 columns when you are done with this step. 1 pt    Using the restructured data from #3 above, create three new variables that represent the type of variable (S = School, C = Child, W = family), the wave number, and remaining information. The type of variable and the wave number are the first and second characters of the variable names that were restructured (i.e. stacked) in #3. Hint, using the separate() function would be useful for this step and exploring some random rows of the data may be helpful using head() or tail() or sample_n().\n Using the head() function, print the first few rows of the data and using the dim() function, print out the dimensions of the data. Note, you should have 8 columns when you are done with this step. 1 pt    Using the restructured data from #4, combine the variable type (i.e., the attribute that is either S, C, or W) and left-over names. This new variable should be similar to the original variable names from when reading in the data from #1, but should not include the wave information in it. Hint, the unite() function should be helpful for this step.\n Using the head() function, print the first few rows of the data and using the dim() function, print out the dimensions of the data. Note, you should have 7 columns when you are done with this step. 1 pt    Finally, using the restructured data from #5, widen the data set to create a variable for each unique value from the newly created variable from #5.\n Using the head() function, print the first few rows of the data and using the dim() function, print out the dimensions of the data. Note, you should have 9 columns and fewer rows than #5 when you are done with this step. 1 pt    Read in the \u0026ldquo;ECLS_6250_school.csv\u0026rdquo; data file found on the course website. Using the restructured data created in #6, merge the school data imported in question #7 into the child level data. Use the type of join where the number of rows for the child level data are not changed. More specifically, the final data should have the same number of rows as #6, but will add 5 new columns.\n Using the head() function, print the first few rows of the merged data and using the dim() function, print out the dimensions of the merged data. 1 pt    This question has a number of steps which are highlighted below in more detail.\n Identify the 25 schools at wave 1 that have the highest proportion of female students and create a new data file that has has the school ID and proportion of female students in the school. Note: Use the codebook to identify which code represents males and females. It may also be helpful, although not necessary, to create a new variable for gender. Perform a filtering join that returns only the rows from the child level data (i.e., the final data from #7) at wave 1 that belong to the 25 schools that had the highest proportion of females at wave 1 (from step one of this question).   Using the head() function, print the first few rows of the data and using the dim() function, print out the dimensions of the data. 1 pt    ","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"6e81801222a7818e1b83b93e282b1f92","permalink":"https://psqf6250.brandonlebeau.org/assignments/assignment/assignment3/","publishdate":"2022-02-21T00:00:00Z","relpermalink":"/assignments/assignment/assignment3/","section":"assignments","summary":"Exploratory Data Analysis and Data Restructuring: 10 pts\nDue: Around March 27th, 2021 - No penalty for late submissions, but due no later than May 8th.\n In this assignment, you will perform exploratory data analysis using the gss_cat data from the forcats package.","tags":null,"title":"Assignment 3","type":"book"},{"authors":null,"categories":null,"content":"   To date I have ignored factor variables and how these are implemented in R. Much of this is due to the greater flexibility of character vectors instead of factors. Also, if using the readr or readxl packages to read in data, the variables are also read in as character strings instead of factors. However, there are situations when factors are useful. Most of these uses are for readability when creating output formats for a report or paper.\nThis set of notes will make use of the following three packages:\nlibrary(tidyverse) library(forcats) library(fivethirtyeight) Uses for Factors To see a few of the benefits of a factor, assume we have a variable that represents the levels of a survey question with five possible responses and we only saw three of those response categories.\nresp \u0026lt;- c(\u0026#39;Disagree\u0026#39;, \u0026#39;Agree\u0026#39;, \u0026#39;Neutral\u0026#39;) This type of variable has a natural order, namely the disagree side of the scale (i.e. strongly disagree) to the agree side of the scale (i.e. strongly agree) with neutral belonging in the middle. However, if we sort this variable, this ordering will not be taken into account with a character string.\nsort(resp) ## [1] \u0026quot;Agree\u0026quot; \u0026quot;Disagree\u0026quot; \u0026quot;Neutral\u0026quot; Notice, these are actually in alphabetical order, likely not what we wanted. This can be fixed by defining this variable as a factor with levels of the variable specified.\nscale_levels \u0026lt;- c(\u0026#39;Strongly Disagree\u0026#39;, \u0026#39;Disagree\u0026#39;, \u0026#39;Neutral\u0026#39;, \u0026#39;Agree\u0026#39;, \u0026#39;Strongly Agree\u0026#39;) resp_fact \u0026lt;- factor(resp, levels = scale_levels) resp_fact ## [1] Disagree Agree Neutral ## Levels: Strongly Disagree Disagree Neutral Agree Strongly Agree sort(resp_fact) ## [1] Disagree Neutral Agree ## Levels: Strongly Disagree Disagree Neutral Agree Strongly Agree Another benefit, if values that are not found in the levels of the factor variable, these will be replaced with NAs. For example,\nfactor(c(\u0026#39;disagree\u0026#39;, \u0026#39;Agree\u0026#39;, \u0026#39;Strongly Agree\u0026#39;), levels = scale_levels) ## [1] \u0026lt;NA\u0026gt; Agree Strongly Agree ## Levels: Strongly Disagree Disagree Neutral Agree Strongly Agree We can also explore valid levels of a variables with the levels function.\nlevels(resp_fact) ## [1] \u0026quot;Strongly Disagree\u0026quot; \u0026quot;Disagree\u0026quot; \u0026quot;Neutral\u0026quot; ## [4] \u0026quot;Agree\u0026quot; \u0026quot;Strongly Agree\u0026quot; Exercises How are factors stored internally by R? To explore this, use the str function on a factor variable and see what it looks like? To further this idea from #1, what happens when you do each of the following commands? Why is this happening?  as.numeric(resp) as.numeric(resp_fact)   Common Factor Manipulations In addition to setting the levels of the variable, there are two common tasks useful with factors.\nReorder factor levels for plotting or table creation Change the levels of the factor (i.e. collapse levels)  Examples of each of these will be given with the weather_check data from the fivethirtyeight package.\nweather_check ## # A tibble: 928 × 9 ## respondent_id ck_weather weather_source weather_source_… ck_weather_watch ## \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;ord\u0026gt; ## 1 3887201482 TRUE The default weath… \u0026lt;NA\u0026gt; Very likely ## 2 3887159451 TRUE The default weath… \u0026lt;NA\u0026gt; Very likely ## 3 3887152228 TRUE The default weath… \u0026lt;NA\u0026gt; Very likely ## 4 3887145426 TRUE The default weath… \u0026lt;NA\u0026gt; Somewhat likely ## 5 3887021873 TRUE A specific websit… Iphone app Very likely ## 6 3886937140 TRUE A specific websit… AccuWeather App Somewhat likely ## 7 3886923931 TRUE The Weather Chann… \u0026lt;NA\u0026gt; Very unlikely ## 8 3886913587 TRUE \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 9 3886889048 TRUE The Weather Chann… \u0026lt;NA\u0026gt; Very likely ## 10 3886848806 TRUE The default weath… \u0026lt;NA\u0026gt; Very likely ## # … with 918 more rows, and 4 more variables: age \u0026lt;fct\u0026gt;, female \u0026lt;lgl\u0026gt;, ## # hhold_income \u0026lt;ord\u0026gt;, region \u0026lt;chr\u0026gt; Reorder Factor Variables To show examples of this operation, suppose we calculated the proportion of respondents that checked the weather daily by region of the country. We could use dplyr for this:\nprop_check_weather \u0026lt;- weather_check %\u0026gt;% group_by(region) %\u0026gt;% summarise(prop = mean(ck_weather)) prop_check_weather ## # A tibble: 10 × 2 ## region prop ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 East North Central 0.858 ## 2 East South Central 0.927 ## 3 Middle Atlantic 0.885 ## 4 Mountain 0.792 ## 5 New England 0.942 ## 6 Pacific 0.697 ## 7 South Atlantic 0.740 ## 8 West North Central 0.815 ## 9 West South Central 0.904 ## 10 \u0026lt;NA\u0026gt; 0.548 This would be a bit easier to view if we plotted this data:\nggplot(prop_check_weather, aes(prop, region)) + geom_point() This plot is difficult to read, primarily due to the way the points are ordered. Showing the regions in alphabetical order makes it more difficult to discern the trend. Instead, we would likely wish to reorder this variable by the ascending order of the proportion that check the weather. We will use the fct_reorder function from the forcats package. Note, I also omit the NA category here.\nggplot(na.omit(prop_check_weather), aes(prop, fct_reorder(region, prop))) + geom_point() Need to be a bit careful with this operation however. For example:\nweather_check %\u0026gt;% group_by(hhold_income) %\u0026gt;% summarise(prop = mean(ck_weather)) %\u0026gt;% na.omit() %\u0026gt;% ggplot(aes(prop, fct_reorder(hhold_income, prop))) + geom_point() Instead, this is the proper way to show this relationship:\nweather_check %\u0026gt;% group_by(hhold_income) %\u0026gt;% summarise(prop = mean(ck_weather)) %\u0026gt;% na.omit() %\u0026gt;% ggplot(aes(prop, hhold_income)) + geom_point() Exercises Using data from the fivethirtyeight package called flying, explore the proportion of respondents that believe the reclining the chair while flying should be eliminated (the variable is recline_eliminate). Do these proportions differ by the location? Create a graphic that captures this relationship, you may wish to reorder the columns to more appropriately represent the relationship.    Rename Factor Levels These operations are useful to collapse categories or rename levels for publication. The primary function we will use for this operation is fct_recode from the forcats package.\nAgain, using the weather_check data, suppose we wished to change the levels of the age variable. The levels currently are:\nlevels(weather_check$age) ## [1] \u0026quot;18 - 29\u0026quot; \u0026quot;30 - 44\u0026quot; \u0026quot;45 - 59\u0026quot; \u0026quot;60+\u0026quot; Suppose we wished to better represent these as words. We can use this with mutate from dplyr combined with fct_recode:\nweather_check %\u0026gt;% mutate(age_recode = fct_recode(age, \u0026#39;18 to 29\u0026#39; = \u0026#39;18 - 29\u0026#39;, \u0026#39;30 to 44\u0026#39; = \u0026#39;30 - 44\u0026#39;, \u0026#39;45 to 59\u0026#39; = \u0026#39;45 - 59\u0026#39; )) %\u0026gt;% count(age_recode) ## # A tibble: 5 × 2 ## age_recode n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 18 to 29 176 ## 2 30 to 44 204 ## 3 45 to 59 278 ## 4 60+ 258 ## 5 \u0026lt;NA\u0026gt; 12 We could also collapse categories by assigning many levels to the same new level. For example, suppose we wished to collapse the ck_weather_watch variable to unlikely and likely instead of the very unlikely to very likely.\nlevels(weather_check$ck_weather_watch) ## [1] \u0026quot;Very unlikely\u0026quot; \u0026quot;Somewhat unlikely\u0026quot; \u0026quot;Somewhat likely\u0026quot; ## [4] \u0026quot;Very likely\u0026quot; weather_check %\u0026gt;% mutate(watch_recode = fct_recode(ck_weather_watch, \u0026#39;Unlikely\u0026#39; = \u0026#39;Very unlikely\u0026#39;, \u0026#39;Unlikely\u0026#39; = \u0026#39;Somewhat unlikely\u0026#39;, \u0026#39;Likely\u0026#39; = \u0026#39;Somewhat likely\u0026#39;, \u0026#39;Likely\u0026#39; = \u0026#39;Very likely\u0026#39; )) %\u0026gt;% count(watch_recode) ## # A tibble: 3 × 2 ## watch_recode n ## \u0026lt;ord\u0026gt; \u0026lt;int\u0026gt; ## 1 Unlikely 281 ## 2 Likely 636 ## 3 \u0026lt;NA\u0026gt; 11 Finally, one last option that may be useful is to lump together categories that are too small to report independently. This functionality is implemented with the function fct_lump. For example, suppose we want to lump the region variable together to have only 5 regions.\nweather_check %\u0026gt;% mutate(region = fct_lump(region, n = 5)) %\u0026gt;% count(region, sort = TRUE) ## # A tibble: 7 × 2 ## region n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Other 219 ## 2 Pacific 185 ## 3 South Atlantic 154 ## 4 East North Central 141 ## 5 Middle Atlantic 104 ## 6 West South Central 94 ## 7 \u0026lt;NA\u0026gt; 31 Exercises Again, using the flying data from the fivethirtyeight package, is there a relationship between the proportion of respondents who have a children under 18 years old and if they believe it is rude to bring a baby on a plane? For this question, collapse the baby variable to two levels, no and yes.     ","date":1614297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614297600,"objectID":"eb6a2c88e0f9f8f457a24e245df3b571","permalink":"https://psqf6250.brandonlebeau.org/rcode/factors/","publishdate":"2021-02-26T00:00:00Z","relpermalink":"/rcode/factors/","section":"rcode","summary":"Factors","tags":null,"title":"Factors","type":"book"},{"authors":null,"categories":null,"content":"Inferential Statistics with R: 6 pts\nDue around April 10th, 2022 - No penalty for late submissions, but due no later than May 8th.\n In this assignment, you will explore research questions from an inferential framework using R. The analyses will be more open ended and will likely contain data preparation steps as well. Please turn in the source file (the .Rmd) file as well as the compiled version (html). Create a new .Rmd file for this assignment. If a question asks for code or there is particular elements of code you\u0026rsquo;d like to share, you can definitely include those in the final compiled version, but please limit those sections to specific elements rather than long chunks of code. Long chunks of code are best left for the Rmd document.\nThe data for this assignment can be found on GitHub and ICON and is named \u0026ldquo;cal-fire-9-10-2020\u0026rdquo;, data on GitHub. A description of the variables is also provided on ICON. Use this data to explore the questions below. Note, the questions below will guide you through the research questions. You do not need to answer the research questions explicitly, instead you can focus on the specific questions, but keeping the overall research questions in mind may be helpful as you complete the assignment.\nAny graphics you create should be of high quality, this includes formatting of axes, axes labels, etc. If none of the graphics are of high quality, a 2 pt penalty will apply.\nResearch Questions  Is there evidence that the number of acres burned has increased for fires in more recent years? Does month help explain variation in the number of acres burned over and above the year of the fire? When the number of days the fire has burned in added to the model, are year or month the fire started still useful predictors?  Questions   Using text processing, create three new variables in the data that represents:\n the year the fire started the month the fire started Create a new variable that represents the length of time a fire burned. This can be created by using the difftime() function within R. Look at the help page to try to figure out how this function can compute differences in two dates.\nThe code is sufficient for this question. 1 pt    Are there any data points that are extreme values or outliers that you feel should be removed from this analysis? Discuss briefly, why you feel these values may impact the analysis and should therefore be removed. Be as specific as you can why any values should or should not be removed. If you identify data that are suspect, remove them from further analysis (i.e. use filter to remove the values). 1 pt\n  Fit two competing models to attempt to answer the 1st and 2nd research questions. Summarize briefly the results from the models with particular attention to answering the research question so that non-statistics/data science individuals could use the answer for their planning or decision making process. Note, consider carefully the best approach on how to include month and year in your models (i.e., continuous vs factor type variables). 1 pt\n Which model fits best or do you feel is the best model? Note, please do not include the output from the summary() function in your answer, instead pull out relevant information from the output to include in your description.    Check assumptions for the models from #3. Does there appear to be problems with meeting statistical assumptions? Provide rationale for why or why not. 1 pt\n  Fit another model to attempt to answer the third research question. Summarize briefly the results from the model with particular attention to answering the research question so that non-statistics/data science individuals could use the answer for their planning or decision making process. 1 pt\n Note, please do not include the output from the summary() function in your answer, instead pull out relevant information from the output to include in your description.    Finally, create a graphic that summarizes the results from your final model that you feel fits the data the best (i.e. this could be the model from #4 or #7). Discuss why you picked this model and describe why this figure does a good job of showing the takeaway message. 1 pt\n   Turning Tables into Graphics with R: 4 pts\nAny graphics you create should be of high quality, this includes formatting of axes, axes labels, etc. If none of the graphics are of high quality, a 1 pt penalty will apply.\nIntroduction Read the article Let\u0026rsquo;s Practice What We Preach: Turning Tables Into Graphs. In this article, Gelman, Pasarica, and Dodhia discuss the benefits of a graph instead of a table to succintly summarize statistical results.\nQuestions   Find a published table showing descriptive or inferential statistic results. Provide a copy of the table (a screen shot is fine for this purpose).\n  Turn the table from #1 above into a publication quality graphic using R. Ensure that this graphic conveys the same purpose as the original table. 1 pt\n  Create a different from the figure created in #3 above, and make an interactive graphic that attempts to convey the same message as the original table. Note, this figure should be an entirely different figure from that in #3. For example, if you created a bar chart in #3, create something other than a bar chart for this question. 1 pt\n  Briefly discuss whether you feel the graphs convey the message better or worse than the original table. Use specific examples from the table/graph and recommendations from the article in your discussion. Which figure do you feel does the best job in sharing the original purpose of the table? Be specific in your discussion. 1 pt\n  Take the data from the original table, to do this you may need to create an Excel file or use the function data.frame() to import the data from the original table. Use the kable() function and kableExtra package to create a reproducible table that looks as close as possible to the original table. 1 pt\n  ","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647216000,"objectID":"3ae8422c4e4b67c68c4c65e221f89597","permalink":"https://psqf6250.brandonlebeau.org/assignments/assignment/assignment4/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/assignments/assignment/assignment4/","section":"assignments","summary":"Inferential Statistics with R: 6 pts\nDue around April 10th, 2022 - No penalty for late submissions, but due no later than May 8th.\n In this assignment, you will explore research questions from an inferential framework using R.","tags":null,"title":"Assignment 4","type":"book"},{"authors":null,"categories":null,"content":"     An often useful task is to manipulate character string variables. This usually comes in the form of regular expressions. Regular expressions come as a part of the base R, however, the regular expressions found in the stringr package are a bit more consistent in their naming structure, so we will use them (they are simply wrappers around the base R regular expressions).\nThe following packages will be used in this section of notes.\nlibrary(tidyverse) # install.packages(\u0026quot;stringr\u0026quot;) library(stringr) Basic String Tasks This section will discuss three basic string functions that help with simple string manipulations. These functions include: str_length, str_c, and str_sub.\nstr_length The str_length function can be used to calculate the length of the string. For example:\nstring \u0026lt;- c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Cedar Rapids\u0026#39;, \u0026#39;Des Moines\u0026#39;, \u0026#39;IA\u0026#39;) str_length(string) ## [1] 9 12 10 2  str_c The str_c function allows you to combine strings together in different ways. One way to think about this is to think about pasting strings together. For example:\nstr_c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Cedar Rapids\u0026#39;, \u0026#39;Des Moines\u0026#39;, \u0026#39;IA\u0026#39;) ## [1] \u0026quot;Iowa CityCedar RapidsDes MoinesIA\u0026quot; Perhaps more useful:\nstr_c(c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Cedar Rapids\u0026#39;, \u0026#39;Des Moines\u0026#39;), \u0026#39;IA\u0026#39;) ## [1] \u0026quot;Iowa CityIA\u0026quot; \u0026quot;Cedar RapidsIA\u0026quot; \u0026quot;Des MoinesIA\u0026quot; More useful yet:\nstr_c(c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Cedar Rapids\u0026#39;, \u0026#39;Des Moines\u0026#39;), \u0026#39;IA\u0026#39;, sep = \u0026#39;, \u0026#39;) ## [1] \u0026quot;Iowa City, IA\u0026quot; \u0026quot;Cedar Rapids, IA\u0026quot; \u0026quot;Des Moines, IA\u0026quot; You can also collapse multiple vectors of strings into a single string using the collapse argument.\nstr_c(c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Cedar Rapids\u0026#39;, \u0026#39;Des Moines\u0026#39;), collapse = \u0026#39;, \u0026#39;) ## [1] \u0026quot;Iowa City, Cedar Rapids, Des Moines\u0026quot;  str_sub The str_sub function is useful for subsetting strings by location. For example:\nstr_sub(c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Cedar Rapids\u0026#39;, \u0026#39;Des Moines\u0026#39;), 1, 4) ## [1] \u0026quot;Iowa\u0026quot; \u0026quot;Ceda\u0026quot; \u0026quot;Des \u0026quot; You can use negative numbers to start from the end:\nstr_sub(c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Cedar Rapids\u0026#39;, \u0026#39;Des Moines\u0026#39;), -6, -1) ## [1] \u0026quot;a City\u0026quot; \u0026quot;Rapids\u0026quot; \u0026quot;Moines\u0026quot;   Regular Expressions Regular expressions are complicated and take awhile to master. This introduction is just going to cover the surface to get you started. To see the basics of regular expressions, we are going to use the str_view function to view text matches.\nThe most basic regular expression is simply to match literal text. For example:\nx \u0026lt;- c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Cedar Rapids\u0026#39;, \u0026#39;Des Moines\u0026#39;) str_view(x, \u0026#39;City\u0026#39;)  {\"x\":{\"html\":\"\\n Iowa City\\n Cedar Rapids\\n Des Moines\\n\"},\"evals\":[],\"jsHooks\":[]} Note that generally, regular expressions are case sensitive.\nstr_view(x, \u0026#39;city\u0026#39;)  {\"x\":{\"html\":\"\\n Iowa City\\n Cedar Rapids\\n Des Moines\\n\"},\"evals\":[],\"jsHooks\":[]} If you want the expression to ignore case, use the ignore_case argument in tandem with regex.\nstr_view(x, regex(\u0026#39;city\u0026#39;, ignore_case = TRUE))  {\"x\":{\"html\":\"\\n Iowa City\\n Cedar Rapids\\n Des Moines\\n\"},\"evals\":[],\"jsHooks\":[]} Two other useful regular expression tools are anchoring and repeating patterns. First, anchor refers to whether the match should occur anywhere (the default), match at the beginning of the string, or match at the end of the string. To match at the start of the string:\nx \u0026lt;- c(\u0026#39;Iowa City\u0026#39;, \u0026#39;Des Moines, Iowa\u0026#39;) str_view(x, \u0026#39;^Iowa\u0026#39;)  {\"x\":{\"html\":\"\\n Iowa City\\n Des Moines, Iowa\\n\"},\"evals\":[],\"jsHooks\":[]} Or to match at the end of a string:\nstr_view(x, \u0026#39;Iowa$\u0026#39;)  {\"x\":{\"html\":\"\\n Iowa City\\n Des Moines, Iowa\\n\"},\"evals\":[],\"jsHooks\":[]} There are three operators that are useful for matching repetitious strings.\n ? 0 or 1 match + 1 or more * 0 or more  Examples of these are given below:\nsounds \u0026lt;- c(\u0026#39;baaaa\u0026#39;, \u0026#39;ssss\u0026#39;, \u0026#39;moo\u0026#39;, \u0026#39;buzz\u0026#39;, \u0026#39;purr\u0026#39;) str_view(sounds, \u0026#39;a?\u0026#39;)  {\"x\":{\"html\":\"\\n baaaa\\n ssss\\n moo\\n buzz\\n purr\\n\"},\"evals\":[],\"jsHooks\":[]} str_view(sounds, \u0026#39;a+\u0026#39;)  {\"x\":{\"html\":\"\\n baaaa\\n ssss\\n moo\\n buzz\\n purr\\n\"},\"evals\":[],\"jsHooks\":[]} str_view(sounds, \u0026#39;rrr*\u0026#39;)  {\"x\":{\"html\":\"\\n baaaa\\n ssss\\n moo\\n buzz\\n purr\\n\"},\"evals\":[],\"jsHooks\":[]} str_view(sounds, \u0026#39;rrr+\u0026#39;)  {\"x\":{\"html\":\"\\n baaaa\\n ssss\\n moo\\n buzz\\n purr\\n\"},\"evals\":[],\"jsHooks\":[]} There are additional repetition operators using braces, {} that can be useful.\n {n} match exactly n {n, } match n or more {, m} match at most m {n, m} match between n and m  Exercises Using the str_view function and the sounds object created above, rewrite this regular expression using braces: str_view(sounds, 'rrr*'). Explore the str_trim function. What does this do? Test this function on the following string: string \u0026lt;- \"\\n\\nString with trailing and leading white space\\n\\n\"    Regular Expression Functions So far we have just visualized the regular expression match. This is useful for testing, however, commonly we would like to create a new variable based on information processed from text strings. The tools we will explore are: str_detect, str_count, str_extract, str_replace, and str_split.\nSuppose we have the following string:\nx \u0026lt;- c(\u0026#39;Iowa City, Iowa\u0026#39;, \u0026#39;Cedar Rapids, IA\u0026#39;, \u0026#39;Des Moines, Iowa\u0026#39;, \u0026#39;Waterloo, IA\u0026#39;, \u0026#39;Rochester, Minnesota\u0026#39;) x ## [1] \u0026quot;Iowa City, Iowa\u0026quot; \u0026quot;Cedar Rapids, IA\u0026quot; \u0026quot;Des Moines, Iowa\u0026quot; ## [4] \u0026quot;Waterloo, IA\u0026quot; \u0026quot;Rochester, Minnesota\u0026quot; Supose we were interested in knowing which cities are from Iowa in this text string, the str_detect function is useful for this.\nstr_detect(x, \u0026#39;Iowa$\u0026#39;) ## [1] TRUE FALSE TRUE FALSE FALSE This didn’t return all the correct matches due to formatting differences. There are two options to fix this. First, we could search for two strings:\nstr_detect(x, \u0026#39;Iowa$|IA$\u0026#39;) ## [1] TRUE TRUE TRUE TRUE FALSE We could then calculate the proportion of cities in the string directly:\nmean(str_detect(x, \u0026#39;Iowa$|IA$\u0026#39;)) ## [1] 0.8 Another useful related function to str_detect is str_count which instead of TRUE/FALSE, will tell you how many matches are in each string.\nstr_count(x, \u0026#39;Iowa$|IA$\u0026#39;) ## [1] 1 1 1 1 0 There are instances where you will need to be careful with this function as it will calculate number of matches.\nstr_count(x, \u0026#39;Iowa|IA\u0026#39;) ## [1] 2 1 1 1 0 Replace Text Above we solved the different formatting differences by searching for two text strings. This can be useful for a few different strings, however, for more complex searches, it can be useful to standardize the text to be the same across variables. This is the job for str_replace.\nstr_replace(x, \u0026#39;Iowa$\u0026#39;, \u0026#39;IA\u0026#39;) ## [1] \u0026quot;Iowa City, IA\u0026quot; \u0026quot;Cedar Rapids, IA\u0026quot; \u0026quot;Des Moines, IA\u0026quot; ## [4] \u0026quot;Waterloo, IA\u0026quot; \u0026quot;Rochester, Minnesota\u0026quot; This function takes two arguments, first the text to be matched and second the text the match should be changed to. If there are no matches the text is not changed. You need to be careful with this function too:\nstr_replace(x, \u0026#39;Iowa\u0026#39;, \u0026#39;IA\u0026#39;) ## [1] \u0026quot;IA City, Iowa\u0026quot; \u0026quot;Cedar Rapids, IA\u0026quot; \u0026quot;Des Moines, IA\u0026quot; ## [4] \u0026quot;Waterloo, IA\u0026quot; \u0026quot;Rochester, Minnesota\u0026quot; By default, the function will only replace the first match. If you’d like to replace all matches you need to use the str_replace_all function.\nstr_replace_all(x, \u0026#39;Iowa\u0026#39;, \u0026#39;IA\u0026#39;) ## [1] \u0026quot;IA City, IA\u0026quot; \u0026quot;Cedar Rapids, IA\u0026quot; \u0026quot;Des Moines, IA\u0026quot; ## [4] \u0026quot;Waterloo, IA\u0026quot; \u0026quot;Rochester, Minnesota\u0026quot; This operation is not useful here, but there are many places that this is a useful operation.\n Extract Text If you wished to extract text instead of replacing text, str_extract is useful for this. For example, if we wished to extract the Minnesota:\nstr_extract(x, \u0026#39;Minnesota\u0026#39;) ## [1] NA NA NA NA \u0026quot;Minnesota\u0026quot; You can build more complicated expressions using the str_extract function. For example, suppose we wished to extract only the city name.\nstr_extract(x, \u0026#39;^.*,\u0026#39;) ## [1] \u0026quot;Iowa City,\u0026quot; \u0026quot;Cedar Rapids,\u0026quot; \u0026quot;Des Moines,\u0026quot; \u0026quot;Waterloo,\u0026quot; ## [5] \u0026quot;Rochester,\u0026quot; This included the comma as well which may not be desired, we will show another way to achieve the same operation with the str_split function. One quick note about the above operation, I used a .. The . means to match any character (except a new line character). To match a literal ., you would need to escape this with \\\\..\n Split on Delimiter If you’d like to split a string based on a common delimiter, using the str_split function is useful. For example, if we wished to split the city from the state:\nstr_split(x, \u0026#39;, \u0026#39;) ## [[1]] ## [1] \u0026quot;Iowa City\u0026quot; \u0026quot;Iowa\u0026quot; ## ## [[2]] ## [1] \u0026quot;Cedar Rapids\u0026quot; \u0026quot;IA\u0026quot; ## ## [[3]] ## [1] \u0026quot;Des Moines\u0026quot; \u0026quot;Iowa\u0026quot; ## ## [[4]] ## [1] \u0026quot;Waterloo\u0026quot; \u0026quot;IA\u0026quot; ## ## [[5]] ## [1] \u0026quot;Rochester\u0026quot; \u0026quot;Minnesota\u0026quot; The str_split function will remove the delimiter that it used to split on. The function also allows you to simplify the structure:\nstr_split(x, \u0026#39;, \u0026#39;, simplify = TRUE) ## [,1] [,2] ## [1,] \u0026quot;Iowa City\u0026quot; \u0026quot;Iowa\u0026quot; ## [2,] \u0026quot;Cedar Rapids\u0026quot; \u0026quot;IA\u0026quot; ## [3,] \u0026quot;Des Moines\u0026quot; \u0026quot;Iowa\u0026quot; ## [4,] \u0026quot;Waterloo\u0026quot; \u0026quot;IA\u0026quot; ## [5,] \u0026quot;Rochester\u0026quot; \u0026quot;Minnesota\u0026quot; Now a matrix is returned.\n  Real World Example To give a sense of some real world applications of regular expressions, I’m going to use the “ufo.csv” data we used once previously.\nufo \u0026lt;- read_csv(\u0026#39;https://raw.githubusercontent.com/lebebr01/psqf-6250-blogdown/main/data/ufo.csv\u0026#39;) ## Rows: 8031 Columns: 7 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (7): Date / Time, City, State, Shape, Duration, Summary, Posted ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ufo ## # A tibble: 8,031 × 7 ## `Date / Time` City State Shape Duration Summary Posted ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 12/12/14 17:30 North Wales PA Triang… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; ## 2 12/12/14 12:40 Cartersville GA Unknown 3.6 min… \u0026quot;Looki… 12/12… ## 3 12/12/14 06:30 Isle of Man (UK/England) \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… ## 4 12/12/14 01:00 Miamisburg OH Changi… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… ## 5 12/12/14 00:00 Spotsylvania VA Unknown 1 minute \u0026quot;White… 12/12… ## 6 12/11/14 23:25 Kenner LA Chevron ~1 minu… \u0026quot;Stran… 12/12… ## 7 12/11/14 23:15 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… ## 8 12/11/14 20:04 Phoenix AZ Chevron 3 minut… \u0026quot;4 Ora… 12/12… ## 9 12/11/14 20:00 Franklin NC Disk 5 minut… \u0026quot;There… 12/12… ## 10 12/11/14 18:30 Longview WA Cylind… 10 seco… \u0026quot;Two c… 12/12… ## # … with 8,021 more rows A few things may be of interest here. First, we may wish to add columns that split the Duration variable into a time and metric variables.\nufo_duration \u0026lt;- str_split(ufo$Duration, \u0026#39; \u0026#39;, simplify = TRUE) cbind(ufo, ufo_duration) %\u0026gt;% head(n = 20) ## Date / Time City State Shape Duration ## 1 12/12/14 17:30 North Wales PA Triangle 5 minutes ## 2 12/12/14 12:40 Cartersville GA Unknown 3.6 minutes ## 3 12/12/14 06:30 Isle of Man (UK/England) \u0026lt;NA\u0026gt; Light 2 seconds ## 4 12/12/14 01:00 Miamisburg OH Changing \u0026lt;NA\u0026gt; ## 5 12/12/14 00:00 Spotsylvania VA Unknown 1 minute ## 6 12/11/14 23:25 Kenner LA Chevron ~1 minute ## 7 12/11/14 23:15 Eugene OR Disk 2 minutes ## 8 12/11/14 20:04 Phoenix AZ Chevron 3 minutes ## 9 12/11/14 20:00 Franklin NC Disk 5 minutes ## 10 12/11/14 18:30 Longview WA Cylinder 10 seconds ## 11 12/11/14 17:30 Markesan WI Light 10 minutes ## 12 12/11/14 16:40 Birmingham AL Fireball 20 minutes ## 13 12/11/14 06:00 West Milford NJ Fireball 10 seconds ## 14 12/11/14 00:00 Williamsburg VA Egg 10 minutes ## 15 12/10/14 20:30 Chandler AZ Sphere 1 hour ## 16 12/10/14 20:00 Maricopa AZ Formation 20-25 minutes ## 17 12/10/14 19:30 Litchfield Park AZ Formation 20 minutes ## 18 12/10/14 19:15 Flagler CO Light 1 minute ## 19 12/10/14 19:00 Garner NC Light 12 minutes ## 20 12/10/14 17:30 Ruidoso NM Fireball 20 minutes ## Summary ## 1 I heard an extremely loud noise outside, and went onto my balcony to investigate. I saw an very very large green light headed my direct ## 2 Looking up towards the west I noticed an object that flashed from white to green to red. ((NUFORC Note: Possible star?? PD)) ## 3 Over the Isle of Man, very fast moving light, diving then zooming. ## 4 Bright color changing and, shape shifting object seen over Miamisburg, OH. ((NUFORC Note: Possible \u0026quot;twinkling\u0026quot; star?? PD)) ## 5 White then orange orb gained a \u0026quot;tail of light\u0026quot; when chased off by a heli. ## 6 Strange, chevron-shaped, ufo moving east to west over Kenner. ## 7 Dual orange orbs in Eugene, Oregon. 12/11/2014. ## 8 4 Orange Lights Spotted South Of The Phoenix Area. ## 9 There were 5 or 6 lights in a row blinking, whites and reds. It was just sitting there over top the ridge of the mountains. ## 10 Two cylinder shaped objects that flew parallel in the sky. ## 11 Dark sky, large lights, nothing like an airplane, turning on and off in a pattern. ## 12 UFOs moving fast like fireballs or individual rockets. ## 13 Strange light across sky. ## 14 Bright light object with three clusters of light. ## 15 1-7 bright orange spheres seen for over an hour in Chandler, Arizona, near the Gila River Reservation. ## 16 Bright orange lights over Maricopa. ## 17 Multiple lights in the sky in Litchfield Park, Arizona. ## 18 Eastern Colorado lights. ## 19 Lights in distance quickly moving in every direction then shooting up at great speed. ## 20 1 lg. bright orange orb that split into 3 orbs. Fighter jets chased them \u0026amp; they disappeared. Mil. jets, helis, and a b2 followed. ## Posted 1 2 3 4 5 6 7 ## 1 \u0026lt;NA\u0026gt; 5 minutes ## 2 12/12/14 3.6 minutes ## 3 12/12/14 2 seconds ## 4 12/12/14 \u0026lt;NA\u0026gt; ## 5 12/12/14 1 minute ## 6 12/12/14 ~1 minute ## 7 12/12/14 2 minutes ## 8 12/12/14 3 minutes ## 9 12/12/14 5 minutes ## 10 12/12/14 10 seconds ## 11 12/12/14 10 minutes ## 12 12/12/14 20 minutes ## 13 12/12/14 10 seconds ## 14 12/12/14 10 minutes ## 15 12/12/14 1 hour ## 16 12/12/14 20-25 minutes ## 17 12/12/14 20 minutes ## 18 12/12/14 1 minute ## 19 12/12/14 12 minutes ## 20 12/12/14 20 minutes It could also be useful to count the number of times colors were mentioned in the summary text.\nufo %\u0026gt;% mutate( num_colors = str_count(Summary, \u0026#39;white|green|red|blue|orange|purple|yellow\u0026#39;) ) ## # A tibble: 8,031 × 8 ## `Date / Time` City State Shape Duration Summary Posted num_colors ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 12/12/14 17:30 North Wales PA Tria… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; 1 ## 2 12/12/14 12:40 Cartersville GA Unkn… 3.6 min… \u0026quot;Looki… 12/12… 3 ## 3 12/12/14 06:30 Isle of Man (U… \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… 0 ## 4 12/12/14 01:00 Miamisburg OH Chan… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… 0 ## 5 12/12/14 00:00 Spotsylvania VA Unkn… 1 minute \u0026quot;White… 12/12… 1 ## 6 12/11/14 23:25 Kenner LA Chev… ~1 minu… \u0026quot;Stran… 12/12… 0 ## 7 12/11/14 23:15 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… 1 ## 8 12/11/14 20:04 Phoenix AZ Chev… 3 minut… \u0026quot;4 Ora… 12/12… 0 ## 9 12/11/14 20:00 Franklin NC Disk 5 minut… \u0026quot;There… 12/12… 2 ## 10 12/11/14 18:30 Longview WA Cyli… 10 seco… \u0026quot;Two c… 12/12… 0 ## # … with 8,021 more rows  An Easier way to manipulate dates The lubridate package in R makes working with date vectors much simpler.\n# install.packages(\u0026quot;lubridate\u0026quot;) library(lubridate) First, we need to convert the Date/Time column in the ufo data to an actual date column. Note above that this column is actually a character vector. Fortunately, lubridate has some functions for common ways that dates and times are stored. The biggest hurdle to know which function to use, is to identify the pattern in the date/time column in our data. Below I print the first 6 rows of the date/time vector of data. Notice that the format is month/day/year then hour/minutes. We can use this information to parse the column to a date/time vector using lubridate’s built in date conversion tools.\nhead(ufo$`Date / Time`) ## [1] \u0026quot;12/12/14 17:30\u0026quot; \u0026quot;12/12/14 12:40\u0026quot; \u0026quot;12/12/14 06:30\u0026quot; \u0026quot;12/12/14 01:00\u0026quot; ## [5] \u0026quot;12/12/14 00:00\u0026quot; \u0026quot;12/11/14 23:25\u0026quot; The primary way to determine which conversion tool to use, is to understand lubridate’s shorthand notation. Below is a list showing these elements.\nFor date components, these are the shorthand notation. * y = year * m = month * d = day\nFor time components, these are the shorthand notation. * h = hours * m = minutes * s = seconds\nNote that “m” stands for both minute and month, but is used in context with either the date or time conversion. The lubridate package will handle this for us. Based on this table and the pattern depicted above, we can convert this with the following pattern and function: mdy_hm(). This can be read in English as, month, day, year followed by hour and minute.\nufo \u0026lt;- ufo %\u0026gt;% mutate(converted_date = mdy_hm(`Date / Time`)) ## Warning: 56 failed to parse. ufo ## # A tibble: 8,031 × 8 ## `Date / Time` City State Shape Duration Summary Posted converted_date ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; ## 1 12/12/14 17:30 North… PA Tria… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; 2014-12-12 17:30:00 ## 2 12/12/14 12:40 Carte… GA Unkn… 3.6 min… \u0026quot;Looki… 12/12… 2014-12-12 12:40:00 ## 3 12/12/14 06:30 Isle … \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… 2014-12-12 06:30:00 ## 4 12/12/14 01:00 Miami… OH Chan… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… 2014-12-12 01:00:00 ## 5 12/12/14 00:00 Spots… VA Unkn… 1 minute \u0026quot;White… 12/12… 2014-12-12 00:00:00 ## 6 12/11/14 23:25 Kenner LA Chev… ~1 minu… \u0026quot;Stran… 12/12… 2014-12-11 23:25:00 ## 7 12/11/14 23:15 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… 2014-12-11 23:15:00 ## 8 12/11/14 20:04 Phoen… AZ Chev… 3 minut… \u0026quot;4 Ora… 12/12… 2014-12-11 20:04:00 ## 9 12/11/14 20:00 Frank… NC Disk 5 minut… \u0026quot;There… 12/12… 2014-12-11 20:00:00 ## 10 12/11/14 18:30 Longv… WA Cyli… 10 seco… \u0026quot;Two c… 12/12… 2014-12-11 18:30:00 ## # … with 8,021 more rows The resulting output has the converted_date added to the original table. Note, we did get some warning messages, these basically say that there were some dates that could not be converted properly, these are likely due to missing data or different patterns in the conversion. We would want to inspect these in more detail to understand why those 56 date/times failed to parse.\nOnce the dates are now in a date/time format, we can now use additional functions from lubridate to pull out specific elements of the date or time. For example, we could use year(), month(), day() to extract the year, month or day from each element. There are also similar functions, hour(), minute(), and second(). These are shown in use below.\nufo %\u0026gt;% mutate( year = year(converted_date), month = month(converted_date), day = day(converted_date), hour = hour(converted_date), minute = minute(converted_date), month_label_abbr = month(converted_date, label = TRUE), wday_abbr = wday(converted_date, label = TRUE), month_label = month(converted_date, label = TRUE, abbr = FALSE), wday = wday(converted_date, label = TRUE, abbr = FALSE) ) ## # A tibble: 8,031 × 17 ## `Date / Time` City State Shape Duration Summary Posted converted_date ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; ## 1 12/12/14 17:30 North… PA Tria… 5 minut… \u0026quot;I hea… \u0026lt;NA\u0026gt; 2014-12-12 17:30:00 ## 2 12/12/14 12:40 Carte… GA Unkn… 3.6 min… \u0026quot;Looki… 12/12… 2014-12-12 12:40:00 ## 3 12/12/14 06:30 Isle … \u0026lt;NA\u0026gt; Light 2 secon… \u0026quot;Over … 12/12… 2014-12-12 06:30:00 ## 4 12/12/14 01:00 Miami… OH Chan… \u0026lt;NA\u0026gt; \u0026quot;Brigh… 12/12… 2014-12-12 01:00:00 ## 5 12/12/14 00:00 Spots… VA Unkn… 1 minute \u0026quot;White… 12/12… 2014-12-12 00:00:00 ## 6 12/11/14 23:25 Kenner LA Chev… ~1 minu… \u0026quot;Stran… 12/12… 2014-12-11 23:25:00 ## 7 12/11/14 23:15 Eugene OR Disk 2 minut… \u0026quot;Dual … 12/12… 2014-12-11 23:15:00 ## 8 12/11/14 20:04 Phoen… AZ Chev… 3 minut… \u0026quot;4 Ora… 12/12… 2014-12-11 20:04:00 ## 9 12/11/14 20:00 Frank… NC Disk 5 minut… \u0026quot;There… 12/12… 2014-12-11 20:00:00 ## 10 12/11/14 18:30 Longv… WA Cyli… 10 seco… \u0026quot;Two c… 12/12… 2014-12-11 18:30:00 ## # … with 8,021 more rows, and 9 more variables: year \u0026lt;dbl\u0026gt;, month \u0026lt;dbl\u0026gt;, ## # day \u0026lt;int\u0026gt;, hour \u0026lt;int\u0026gt;, minute \u0026lt;int\u0026gt;, month_label_abbr \u0026lt;ord\u0026gt;, ## # wday_abbr \u0026lt;ord\u0026gt;, month_label \u0026lt;ord\u0026gt;, wday \u0026lt;ord\u0026gt;  ","date":1614297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614297600,"objectID":"5e4c704a4cd83a539d7f28471a80a35e","permalink":"https://psqf6250.brandonlebeau.org/rcode/strings/","publishdate":"2021-02-26T00:00:00Z","relpermalink":"/rcode/strings/","section":"rcode","summary":"Working with Character Strings","tags":null,"title":"Working with Character Strings","type":"book"},{"authors":null,"categories":null,"content":"   This section of the notes is going to introduce you into the world of models in R. For the most part, we are going to stick with simple linear models and build up the various models using one function lm. The lm function is an extremely powerful function that can accommodate many different models in a single framework.\nThis section of notes is going to make use of four R packages:\nlibrary(tidyverse) library(modelr) # install.packages(\u0026quot;broom\u0026quot;) library(broom) library(fivethirtyeight) Simple Linear Regression First we need some data. We are going to explore the data from the fivethirtyeight package called fandango. Here are the first few rows:\nfandango ## # A tibble: 146 × 23 ## film year rottentomatoes rottentomatoes_… metacritic metacritic_user imdb ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Aveng… 2015 74 86 66 7.1 7.8 ## 2 Cinde… 2015 85 80 67 7.5 7.1 ## 3 Ant-M… 2015 80 90 64 8.1 7.8 ## 4 Do Yo… 2015 18 84 22 4.7 5.4 ## 5 Hot T… 2015 14 28 29 3.4 5.1 ## 6 The W… 2015 63 62 50 6.8 7.2 ## 7 Irrat… 2015 42 53 53 7.6 6.9 ## 8 Top F… 2014 86 64 81 6.8 6.5 ## 9 Shaun… 2015 99 82 81 8.8 7.4 ## 10 Love … 2015 89 87 80 8.5 7.8 ## # … with 136 more rows, and 16 more variables: fandango_stars \u0026lt;dbl\u0026gt;, ## # fandango_ratingvalue \u0026lt;dbl\u0026gt;, rt_norm \u0026lt;dbl\u0026gt;, rt_user_norm \u0026lt;dbl\u0026gt;, ## # metacritic_norm \u0026lt;dbl\u0026gt;, metacritic_user_nom \u0026lt;dbl\u0026gt;, imdb_norm \u0026lt;dbl\u0026gt;, ## # rt_norm_round \u0026lt;dbl\u0026gt;, rt_user_norm_round \u0026lt;dbl\u0026gt;, metacritic_norm_round \u0026lt;dbl\u0026gt;, ## # metacritic_user_norm_round \u0026lt;dbl\u0026gt;, imdb_norm_round \u0026lt;dbl\u0026gt;, ## # metacritic_user_vote_count \u0026lt;int\u0026gt;, imdb_user_vote_count \u0026lt;int\u0026gt;, ## # fandango_votes \u0026lt;int\u0026gt;, fandango_difference \u0026lt;dbl\u0026gt; These data have 146 rows and 23 columns.\nSuppose we were interested in exploring the relationship between ratings from rottentomatoes and metacritic. Note, we will not use the user rating for this exploration. A natural first step may be to look at a scatterplot of these data to explore the shape of the relationship.\nggplot(fandango, aes(rottentomatoes, metacritic)) + theme_bw() + geom_point(size = 3) To better explore the relationship, including a smoother can be useful:\nggplot(fandango, aes(rottentomatoes, metacritic)) + theme_bw() + geom_point(size = 3) + geom_smooth(method = \u0026#39;loess\u0026#39;, se = FALSE, size = 1.5) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; It may also be useful to calculate a correlation coefficient between these two variables.\nwith(fandango, cor(rottentomatoes, metacritic)) ## [1] 0.9573596 Fit Linear Regression Now we will attempt to fit a model to these data. Namely, the relationship appears to be mostly linear and suppose we wished to predict the metacritic review score with the rotten tomatoes score. To do this, we will use the lm function and the ~ that we used with facet_wrap and facet_grid.\nMore concretely, suppose we wished to fit the model: \\[ metacritic_{i} = b_{0} + b_{1} rottentomatoes_{i} + \\epsilon_{i} \\]\nIn this model, \\(metacritic_{i}\\) is the dependent or response variable and \\(rottentomatoes_{i}\\) is the independent, predictor, or covariate. In many traditional statistics courses, \\(metacritic_{i}\\) would be represented with \\(Y\\) and \\(rottentomatoes_{i}\\) would be represented with \\(X\\). It is often more descriptive to represent these with their variable names instead of \\(Y\\) or \\(X\\).\nTo fit this model, we simply need to replace the \\(=\\) sign found in the equation above with the ~. For example, the equation above would turn into:\nmeta_mod \u0026lt;- lm(metacritic ~ rottentomatoes, data = fandango) To see output from the model, we can take two approaches. One is to use summary and another is to use the tidy function from the broom package. I show each below in turn.\nsummary(meta_mod) ## ## Call: ## lm(formula = metacritic ~ rottentomatoes, data = fandango) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.7209 -4.1999 0.3855 3.7952 14.6662 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 21.12097 1.05710 19.98 \u0026lt;2e-16 *** ## rottentomatoes 0.61935 0.01558 39.77 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.658 on 144 degrees of freedom ## Multiple R-squared: 0.9165, Adjusted R-squared: 0.916 ## F-statistic: 1581 on 1 and 144 DF, p-value: \u0026lt; 2.2e-16 tidy(meta_mod) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 21.1 1.06 20.0 2.36e-43 ## 2 rottentomatoes 0.619 0.0156 39.8 1.54e-79 With the broom package, the results are reported in a tidier framework. We will see additional useful functions using the broom package later on.\nYou can also directly request confidence intervals with the tidy function:\ntidy(meta_mod, conf.int = TRUE) ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 21.1 1.06 20.0 2.36e-43 19.0 23.2 ## 2 rottentomatoes 0.619 0.0156 39.8 1.54e-79 0.589 0.650 Exercises Fit a new model using the fandango data that attempts to explain the metacritic ratings with the imdb rating. Fit another model using the fandango data that attempts to explain the metacritic ratings with the fandango_ratingvalue scores. Exploring the predictors of these two new models with the one fitted above with the rottentomatoes scores, which rating score best helps us predict the metacritic scores?    Workings Behind lm function To see what the lm function is doing behind the scenes, we will use the model_matrix function from the modelr package. For example, from the model above:\nmodel_matrix(fandango, metacritic ~ rottentomatoes) ## # A tibble: 146 × 2 ## `(Intercept)` rottentomatoes ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 74 ## 2 1 85 ## 3 1 80 ## 4 1 18 ## 5 1 14 ## 6 1 63 ## 7 1 42 ## 8 1 86 ## 9 1 99 ## 10 1 89 ## # … with 136 more rows This is often referred to as the design matrix in statistics text books and is one of the matrices that are used by lm to calculate the estimated parameters from above. Notice that is automatically included the intercept, normally this is of interest, if it is not, we can omit it directly by including a -1 in the formula. For example:\nmodel_matrix(fandango, metacritic ~ rottentomatoes - 1) ## # A tibble: 146 × 1 ## rottentomatoes ## \u0026lt;dbl\u0026gt; ## 1 74 ## 2 85 ## 3 80 ## 4 18 ## 5 14 ## 6 63 ## 7 42 ## 8 86 ## 9 99 ## 10 89 ## # … with 136 more rows tidy(lm(metacritic ~ rottentomatoes - 1, data = fandango)) ## # A tibble: 1 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 rottentomatoes 0.898 0.0134 67.3 3.14e-111 You need to be careful with this syntax as this is commonly not is what is desired when fitting a linear model.\n Categorical Predictors Suppose we were interested in the following research question:\n To what extent are there average differences in movie ratings between rottentomatoes and metacritic?  To answer this research question, we would need to transform our data to great a group variable and a rating variable.\nmeta_rotten \u0026lt;- fandango %\u0026gt;% select(film, year, rottentomatoes, metacritic) %\u0026gt;% gather(group, rating, rottentomatoes, metacritic) meta_rotten ## # A tibble: 292 × 4 ## film year group rating ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Avengers: Age of Ultron 2015 rottentomatoes 74 ## 2 Cinderella 2015 rottentomatoes 85 ## 3 Ant-Man 2015 rottentomatoes 80 ## 4 Do You Believe? 2015 rottentomatoes 18 ## 5 Hot Tub Time Machine 2 2015 rottentomatoes 14 ## 6 The Water Diviner 2015 rottentomatoes 63 ## 7 Irrational Man 2015 rottentomatoes 42 ## 8 Top Five 2014 rottentomatoes 86 ## 9 Shaun the Sheep Movie 2015 rottentomatoes 99 ## 10 Love \u0026amp; Mercy 2015 rottentomatoes 89 ## # … with 282 more rows Now we can work with this data to answer the question from above. More specifically, our dependent variable will be the rating variable and the independent variable will be the group (categorical) variable. This can be fitted within a linear model as follows:\ntidy(lm(rating ~ factor(group), data = meta_rotten)) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 58.8 2.10 28.0 2.50e-84 ## 2 factor(group)rottentomatoes 2.04 2.97 0.686 4.93e- 1 To see exactly what is happening, model_matrix may be useful. First I am going to arrange the data by the films in alphabetical order.\nmeta_rotten %\u0026gt;% arrange(film) ## # A tibble: 292 × 4 ## film year group rating ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 \u0026#39;71 2015 rottentomatoes 97 ## 2 \u0026#39;71 2015 metacritic 83 ## 3 5 Flights Up 2015 rottentomatoes 52 ## 4 5 Flights Up 2015 metacritic 55 ## 5 A Little Chaos 2015 rottentomatoes 40 ## 6 A Little Chaos 2015 metacritic 51 ## 7 A Most Violent Year 2014 rottentomatoes 90 ## 8 A Most Violent Year 2014 metacritic 79 ## 9 About Elly 2015 rottentomatoes 97 ## 10 About Elly 2015 metacritic 87 ## # … with 282 more rows meta_rotten %\u0026gt;% arrange(film) %\u0026gt;% model_matrix(rating ~ factor(group)) ## # A tibble: 292 × 2 ## `(Intercept)` `factor(group)rottentomatoes` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 ## 2 1 0 ## 3 1 1 ## 4 1 0 ## 5 1 1 ## 6 1 0 ## 7 1 1 ## 8 1 0 ## 9 1 1 ## 10 1 0 ## # … with 282 more rows You may be more familiar with using a t-test for this type of design. We can replicate the results above with a t-test using the t.test function.\nt.test(rating ~ factor(group), data = meta_rotten, var.equal = TRUE) ## ## Two Sample t-test ## ## data: rating by factor(group) ## t = -0.68638, df = 290, p-value = 0.493 ## alternative hypothesis: true difference in means between group metacritic and group rottentomatoes is not equal to 0 ## 95 percent confidence interval: ## -7.893918 3.811726 ## sample estimates: ## mean in group metacritic mean in group rottentomatoes ## 58.80822 60.84932 Exercises Compute descriptive means using the meta_rotten transformed data from above by the group variable. Do these means appear to be descriptively different? How do these means relate to the parameters estimated from the model above?    Evaulating Model fit There are many ways to evaluate model fit. Many of these are available using the summary function.\nsummary(lm(rating ~ factor(group), data = meta_rotten)) ## ## Call: ## lm(formula = rating ~ factor(group), data = meta_rotten) ## ## Residuals: ## Min 1Q Median 3Q Max ## -55.849 -19.089 0.671 22.161 39.151 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 58.808 2.103 27.967 \u0026lt;2e-16 *** ## factor(group)rottentomatoes 2.041 2.974 0.686 0.493 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 25.41 on 290 degrees of freedom ## Multiple R-squared: 0.001622, Adjusted R-squared: -0.001821 ## F-statistic: 0.4711 on 1 and 290 DF, p-value: 0.493 The unfortunate part of this is the fact that these are more difficult to pull out of the table programmatically (i.e. in a reproducible workflow). This is where the broom package helps with the use of the glance function.\nglance(lm(rating ~ factor(group), data = meta_rotten)) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.00162 -0.00182 25.4 0.471 0.493 1 -1358. 2722. 2733. ## # … with 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; These are now in a more tidy data frame and if you have multiple models in an exploratory analysis, these could then be much easier compared and combined programmatically to come to a final model.\nAnother useful function from the broom package is augment. This function will add additional information to the original data such as residuals, fitted (predicted) values, and other diagnostic statistics.\ndiagnostic \u0026lt;- augment(lm(rating ~ factor(group), data = meta_rotten)) diagnostic ## # A tibble: 292 × 7 ## rating `factor(group)` .fitted .hat .sigma .cooksd .std.resid ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 74 rottentomatoes 60.8 0.00685 25.4 0.000930 0.519 ## 2 85 rottentomatoes 60.8 0.00685 25.4 0.00314 0.954 ## 3 80 rottentomatoes 60.8 0.00685 25.4 0.00197 0.756 ## 4 18 rottentomatoes 60.8 0.00685 25.3 0.00988 -1.69 ## 5 14 rottentomatoes 60.8 0.00685 25.3 0.0118 -1.85 ## 6 63 rottentomatoes 60.8 0.00685 25.5 0.0000249 0.0849 ## 7 42 rottentomatoes 60.8 0.00685 25.4 0.00191 -0.744 ## 8 86 rottentomatoes 60.8 0.00685 25.4 0.00340 0.993 ## 9 99 rottentomatoes 60.8 0.00685 25.4 0.00783 1.51 ## 10 89 rottentomatoes 60.8 0.00685 25.4 0.00426 1.11 ## # … with 282 more rows These could then be plotted to explore more information about model fit. For example a histogram of the standardized residuals are often useful.\nggplot(diagnostic, aes(.std.resid)) + geom_histogram(bins = 30, color = \u0026#39;white\u0026#39;) + theme_bw() For this model, boxplots of the standardized residuals by the two groups can also be informative:\nggplot(diagnostic, aes(`factor(group)`, .std.resid)) + geom_boxplot() + geom_jitter() + coord_flip() + theme_bw() We will explore more details on predicted or fitted values later.\nLastly, the augment function can be useful, however I personally do not like the naming convention used by the function. I want to point you to two additional functions from the modelr package that can be useful for predicted (add_predictions) and residual values (add_residuals).\nFor example, to add the residuals to the original data:\nmeta_rotten %\u0026gt;% add_residuals(lm(rating ~ factor(group), data = meta_rotten)) ## # A tibble: 292 × 5 ## film year group rating resid ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Avengers: Age of Ultron 2015 rottentomatoes 74 13.2 ## 2 Cinderella 2015 rottentomatoes 85 24.2 ## 3 Ant-Man 2015 rottentomatoes 80 19.2 ## 4 Do You Believe? 2015 rottentomatoes 18 -42.8 ## 5 Hot Tub Time Machine 2 2015 rottentomatoes 14 -46.8 ## 6 The Water Diviner 2015 rottentomatoes 63 2.15 ## 7 Irrational Man 2015 rottentomatoes 42 -18.8 ## 8 Top Five 2014 rottentomatoes 86 25.2 ## 9 Shaun the Sheep Movie 2015 rottentomatoes 99 38.2 ## 10 Love \u0026amp; Mercy 2015 rottentomatoes 89 28.2 ## # … with 282 more rows Exercises Fit a new model using the fandango data that attempts to explain the metacritic ratings with the imdb rating. Explore the distribution of residuals. Does there appear to be problems with these residuals? Using the model from #1, create a scatterplot that displays the residuals by the predictor variable. Are there problems with this plot that we should be concerned with?     ","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"16e3d9d59cf33364836dd20ca5f50ec4","permalink":"https://psqf6250.brandonlebeau.org/rcode/model-intro/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/rcode/model-intro/","section":"rcode","summary":"Model Introduction","tags":null,"title":"Model Introduction","type":"book"},{"authors":null,"categories":null,"content":"   The last week has focused on building simple linear models with a single predictor. This week will evaluate these models and build them up with more complexity. Particularly, this week will focus on ways to build models with predictors that have more than two categories, alternative ways to code categorical predictors, mixing categorical and quantitative variables, and interactions.\nThis section of notes will use the following packages.\nlibrary(tidyverse) library(modelr) library(broom) library(fivethirtyeight) library(forcats) More than two categorical levels Last week we explored a linear model framework for a two sample t-test (and the homework has you explore fitting a one-sample t-test in a linear model framework). I now want to generalize this idea to more than two categorical levels. It is traditional to think about these types of models as analysis of variance (ANOVA) models, however, the same model can be fitted in a linear model framework as well.\nFor this set of notes, we are going to make use of the gss_cat data found in the forcats package. Below are the first few rows of the data:\ngss_cat ## # A tibble: 21,483 × 9 ## year marital age race rincome partyid relig denom tvhours ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 2000 Never married 26 White $8000 to 9999 Ind,near … Prot… Sout… 12 ## 2 2000 Divorced 48 White $8000 to 9999 Not str r… Prot… Bapt… NA ## 3 2000 Widowed 67 White Not applicable Independe… Prot… No d… 2 ## 4 2000 Never married 39 White Not applicable Ind,near … Orth… Not … 4 ## 5 2000 Divorced 25 White Not applicable Not str d… None Not … 1 ## 6 2000 Married 25 White $20000 - 24999 Strong de… Prot… Sout… NA ## 7 2000 Never married 36 White $25000 or more Not str r… Chri… Not … 3 ## 8 2000 Divorced 44 White $7000 to 7999 Ind,near … Prot… Luth… NA ## 9 2000 Married 44 White $25000 or more Not str d… Prot… Other 0 ## 10 2000 Married 47 White $25000 or more Strong re… Prot… Sout… 3 ## # … with 21,473 more rows Suppose we were interested in exploring the relationship between the marital status of an individual and how much tv they watch. For example, perhaps married couples watch more tv compared to those that are single or never married. To get an idea of the categories in the marital variable, we could use the count function within dplyr.\ngss_cat %\u0026gt;% count(marital) ## # A tibble: 6 × 2 ## marital n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 No answer 17 ## 2 Never married 5416 ## 3 Separated 743 ## 4 Divorced 3383 ## 5 Widowed 1807 ## 6 Married 10117 You’ll notice that there a few responses of “No Answer” and we may wish to treat these as missing values. This can be done with the fct_recode function as follows:\ngss_cat \u0026lt;- gss_cat %\u0026gt;% mutate(marital_miss = fct_recode(marital, NULL = \u0026#39;No answer\u0026#39; )) gss_cat %\u0026gt;% count(marital_miss) ## # A tibble: 6 × 2 ## marital_miss n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Never married 5416 ## 2 Separated 743 ## 3 Divorced 3383 ## 4 Widowed 1807 ## 5 Married 10117 ## 6 \u0026lt;NA\u0026gt; 17 We can now fit the model to this new data using the lm function.\nanova_mod \u0026lt;- lm(tvhours ~ marital_miss, data = gss_cat) summary(anova_mod) ## ## Call: ## lm(formula = tvhours ~ marital_miss, data = gss_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9120 -1.6504 -0.6504 0.8948 21.3496 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.10518 0.04679 66.366 \u0026lt; 2e-16 *** ## marital_missSeparated 0.44444 0.13738 3.235 0.00122 ** ## marital_missDivorced -0.01977 0.07680 -0.257 0.79687 ## marital_missWidowed 0.80682 0.09352 8.627 \u0026lt; 2e-16 *** ## marital_missMarried -0.45475 0.05879 -7.735 1.13e-14 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.561 on 11323 degrees of freedom ## (10155 observations deleted due to missingness) ## Multiple R-squared: 0.02141, Adjusted R-squared: 0.02107 ## F-statistic: 61.94 on 4 and 11323 DF, p-value: \u0026lt; 2.2e-16 To explore what the lm function is doing internally, the design matrix is a natural way to do this.\nmodel_matrix(gss_cat, tvhours ~ marital_miss) ## # A tibble: 11,328 × 5 ## `(Intercept)` marital_missSeparated marital_missDivorced marital_missWidowed ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0 0 0 ## 2 1 0 0 1 ## 3 1 0 0 0 ## 4 1 0 1 0 ## 5 1 0 0 0 ## 6 1 0 0 0 ## 7 1 0 0 0 ## 8 1 0 0 0 ## 9 1 0 0 0 ## 10 1 0 1 0 ## # … with 11,318 more rows, and 1 more variable: marital_missMarried \u0026lt;dbl\u0026gt; Writing out this model with equations, the model looks like this: \\[ tvhours_{i} = \\beta_{0} + \\beta_{1} Separated_{i} + \\beta_{2} Divorced_{I} + \\beta_{3} Widowed_{i} + \\beta_{4} Married_{i} + \\epsilon_{i} \\]\nIf you are more familiar with ANOVA terminology, you can get an ANOVA table using the anova function on the model object.\nanova(anova_mod) ## Analysis of Variance Table ## ## Response: tvhours ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## marital_miss 4 1624 406.12 61.941 \u0026lt; 2.2e-16 *** ## Residuals 11323 74239 6.56 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Here you’ll notice that the F statistic is the same from the lm and anova functions showing that these are equivalent model calls.\nExercises Using the martial_miss variable created above, what are the sample means of the five groups? How do these sample means relate back to the parameters estimates shown above? How could you visualize these models results? Attempt to create a visualization that captures the model results above.   Adjusting the reference group It is often of interest to adjust the reference group to make the intercept represent a specific group of interest. There are two approaches to take for this approach. The first I will show is using the forcats package to change the order of the levels of the variable.\nSuppose for example, we wish to make the widowed category the reference group. This is the job of fct_relevel from the forcats package.\ngss_cat \u0026lt;- gss_cat %\u0026gt;% mutate(marital_m_widow = fct_relevel( marital_miss, \u0026#39;Widowed\u0026#39; )) levels(gss_cat$marital_miss) ## [1] \u0026quot;Never married\u0026quot; \u0026quot;Separated\u0026quot; \u0026quot;Divorced\u0026quot; \u0026quot;Widowed\u0026quot; ## [5] \u0026quot;Married\u0026quot; levels(gss_cat$marital_m_widow) ## [1] \u0026quot;Widowed\u0026quot; \u0026quot;Never married\u0026quot; \u0026quot;Separated\u0026quot; \u0026quot;Divorced\u0026quot; ## [5] \u0026quot;Married\u0026quot; You’ll notice that in the new variable, the widowed category was moved to the beginning and the remaining order was not changed. We can now fit a new model with this newly releveled factor variable.\nsummary(lm(tvhours ~ marital_m_widow, data = gss_cat)) ## ## Call: ## lm(formula = tvhours ~ marital_m_widow, data = gss_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9120 -1.6504 -0.6504 0.8948 21.3496 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.91200 0.08097 48.313 \u0026lt; 2e-16 *** ## marital_m_widowNever married -0.80682 0.09352 -8.627 \u0026lt; 2e-16 *** ## marital_m_widowSeparated -0.36238 0.15245 -2.377 0.0175 * ## marital_m_widowDivorced -0.82659 0.10132 -8.159 3.75e-16 *** ## marital_m_widowMarried -1.26157 0.08845 -14.262 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.561 on 11323 degrees of freedom ## (10155 observations deleted due to missingness) ## Multiple R-squared: 0.02141, Adjusted R-squared: 0.02107 ## F-statistic: 61.94 on 4 and 11323 DF, p-value: \u0026lt; 2.2e-16 The second approach to modifying which group represents the reference group would be to create the indicator (dummy) variables manually. The logic follows from the design matrix above, namely that each variable should have a value of 1 if the marital status equals a specific category or 0 otherwise. For example, this can be created as follows:\ngss_cat \u0026lt;- gss_cat %\u0026gt;% mutate( separated = ifelse(marital_miss == \u0026#39;Separated\u0026#39;, 1, 0), never_married = ifelse(marital_miss == \u0026#39;Never married\u0026#39;, 1, 0), divorced = ifelse(marital_miss == \u0026#39;Divorced\u0026#39;, 1, 0), married = ifelse(marital_miss == \u0026#39;Married\u0026#39;, 1, 0) ) summary(lm(tvhours ~ separated + never_married + divorced + married, data = gss_cat)) ## ## Call: ## lm(formula = tvhours ~ separated + never_married + divorced + ## married, data = gss_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9120 -1.6504 -0.6504 0.8948 21.3496 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.91200 0.08097 48.313 \u0026lt; 2e-16 *** ## separated -0.36238 0.15245 -2.377 0.0175 * ## never_married -0.80682 0.09352 -8.627 \u0026lt; 2e-16 *** ## divorced -0.82659 0.10132 -8.159 3.75e-16 *** ## married -1.26157 0.08845 -14.262 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.561 on 11323 degrees of freedom ## (10155 observations deleted due to missingness) ## Multiple R-squared: 0.02141, Adjusted R-squared: 0.02107 ## F-statistic: 61.94 on 4 and 11323 DF, p-value: \u0026lt; 2.2e-16 Manually creating the variables has a few advantages, namely that there is a bit more flexibility on how the variables are created, but both approaches lead to the same model.\nExercises Combine the ‘Never married’ and ‘Divorced’ categories into one category. Fit a new model that combines these two categories. Does the model fit differ from the models shown above? Is this surprising?    Post Hoc Tests From the models fitted above, it may be of interest to conduct post hoc tests that compare all pairwise mean differences, particularly as the tests above are all compared to the reference group. This approach will be explored using the multcomp package and with defining linear contrasts.\n# install.packages(\u0026quot;multcomp\u0026quot;) library(multcomp) We first need to define linear contrasts based on the levels of the factor variable. For example, using the following model:\ngss_cat \u0026lt;- gss_cat %\u0026gt;% mutate(marital_m_widow = fct_recode(marital_m_widow, \u0026quot;Never_married\u0026quot; = \u0026quot;Never married\u0026quot; )) anova_mod \u0026lt;- lm(tvhours ~ marital_m_widow, data = gss_cat) levels(gss_cat$marital_m_widow) ## [1] \u0026quot;Widowed\u0026quot; \u0026quot;Never_married\u0026quot; \u0026quot;Separated\u0026quot; \u0026quot;Divorced\u0026quot; ## [5] \u0026quot;Married\u0026quot; We will use these level values to create linear contrasts that test all pairwise categories.\nmy_contrasts \u0026lt;- c(\u0026quot;Widowed - Never_married = 0\u0026quot;, \u0026quot;Widowed - Separated = 0\u0026quot;, \u0026quot;Widowed - Divorced = 0\u0026quot;, \u0026quot;Widowed - Married = 0\u0026quot;, \u0026quot;Never_married - Separated = 0\u0026quot;, \u0026quot;Never_married - Divorced = 0\u0026quot;, \u0026quot;Never_married - Married = 0\u0026quot;, \u0026quot;Separated - Divorced = 0\u0026quot;, \u0026quot;Separated - Married = 0\u0026quot;, \u0026quot;Divorced - Married = 0\u0026quot;) contr_results \u0026lt;- glht(anova_mod, linfct = mcp(marital_m_widow = my_contrasts)) summary(contr_results) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: lm(formula = tvhours ~ marital_m_widow, data = gss_cat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## Widowed - Never_married == 0 0.80682 0.09352 8.627 \u0026lt; 0.001 *** ## Widowed - Separated == 0 0.36238 0.15245 2.377 0.11077 ## Widowed - Divorced == 0 0.82659 0.10132 8.159 \u0026lt; 0.001 *** ## Widowed - Married == 0 1.26157 0.08845 14.262 \u0026lt; 0.001 *** ## Never_married - Separated == 0 -0.44444 0.13738 -3.235 0.00949 ** ## Never_married - Divorced == 0 0.01977 0.07680 0.257 0.99893 ## Never_married - Married == 0 0.45475 0.05879 7.735 \u0026lt; 0.001 *** ## Separated - Divorced == 0 0.46421 0.14280 3.251 0.00897 ** ## Separated - Married == 0 0.89919 0.13398 6.711 \u0026lt; 0.001 *** ## Divorced - Married == 0 0.43498 0.07054 6.166 \u0026lt; 0.001 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## (Adjusted p values reported -- single-step method) You can also specify different adjustment methods, such as the Benjamin-Hochberg method.\nsummary(contr_results, test = adjusted(\u0026quot;BH\u0026quot;)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: lm(formula = tvhours ~ marital_m_widow, data = gss_cat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## Widowed - Never_married == 0 0.80682 0.09352 8.627 \u0026lt; 2e-16 *** ## Widowed - Separated == 0 0.36238 0.15245 2.377 0.01941 * ## Widowed - Divorced == 0 0.82659 0.10132 8.159 1.48e-15 *** ## Widowed - Married == 0 1.26157 0.08845 14.262 \u0026lt; 2e-16 *** ## Never_married - Separated == 0 -0.44444 0.13738 -3.235 0.00152 ** ## Never_married - Divorced == 0 0.01977 0.07680 0.257 0.79687 ## Never_married - Married == 0 0.45475 0.05879 7.735 2.78e-14 *** ## Separated - Divorced == 0 0.46421 0.14280 3.251 0.00152 ** ## Separated - Married == 0 0.89919 0.13398 6.711 4.04e-11 *** ## Divorced - Married == 0 0.43498 0.07054 6.166 1.20e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## (Adjusted p values reported -- BH method) Although defining the linear contrasts manually is more flexible, for simple models, the multiple comparisons can be generated a bit more simply.\nsummary(glht(anova_mod, linfct = mcp(marital_m_widow = \u0026quot;Tukey\u0026quot;)), test = adjusted(\u0026quot;BH\u0026quot;)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: lm(formula = tvhours ~ marital_m_widow, data = gss_cat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## Never_married - Widowed == 0 -0.80682 0.09352 -8.627 \u0026lt; 2e-16 *** ## Separated - Widowed == 0 -0.36238 0.15245 -2.377 0.01941 * ## Divorced - Widowed == 0 -0.82659 0.10132 -8.159 1.48e-15 *** ## Married - Widowed == 0 -1.26157 0.08845 -14.262 \u0026lt; 2e-16 *** ## Separated - Never_married == 0 0.44444 0.13738 3.235 0.00152 ** ## Divorced - Never_married == 0 -0.01977 0.07680 -0.257 0.79687 ## Married - Never_married == 0 -0.45475 0.05879 -7.735 2.78e-14 *** ## Divorced - Separated == 0 -0.46421 0.14280 -3.251 0.00152 ** ## Married - Separated == 0 -0.89919 0.13398 -6.711 4.04e-11 *** ## Married - Divorced == 0 -0.43498 0.07054 -6.166 1.20e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## (Adjusted p values reported -- BH method) You can also generate simultaneous confidence intervals:\nci \u0026lt;- confint(summary(glht(anova_mod, linfct = mcp(marital_m_widow = \u0026quot;Tukey\u0026quot;)), test = adjusted(\u0026quot;BH\u0026quot;))) ci ## ## Simultaneous Confidence Intervals ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: lm(formula = tvhours ~ marital_m_widow, data = gss_cat) ## ## Quantile = 2.6896 ## 95% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## Never_married - Widowed == 0 -0.80682 -1.05835 -0.55529 ## Separated - Widowed == 0 -0.36238 -0.77241 0.04764 ## Divorced - Widowed == 0 -0.82659 -1.09910 -0.55409 ## Married - Widowed == 0 -1.26157 -1.49949 -1.02366 ## Separated - Never_married == 0 0.44444 0.07495 0.81394 ## Divorced - Never_married == 0 -0.01977 -0.22632 0.18678 ## Married - Never_married == 0 -0.45475 -0.61289 -0.29661 ## Divorced - Separated == 0 -0.46421 -0.84829 -0.08013 ## Married - Separated == 0 -0.89919 -1.25955 -0.53883 ## Married - Divorced == 0 -0.43498 -0.62471 -0.24525 These could then be visulized directly.\nSCI = data.frame( Contrast = 1:nrow(ci$confint), #Contrast number MD = ci$confint[, 1], #Mean difference LL = ci$confint[, 2], #Lower limit UL = ci$confint[, 3], #Upper limit Sig = c(\u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;, \u0026quot;Yes\u0026quot;, \u0026quot;Yes\u0026quot;, \u0026#39;Yes\u0026#39;, \u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;, \u0026#39;Yes\u0026#39;, \u0026#39;Yes\u0026#39;, \u0026#39;Yes\u0026#39;), #Statistically reliable? Alpha = c(1, .75, 1, 1, 1, .75, 1, 1, 1, 1), #Transparency value Names = rownames(ci$confint) # contrast label ) # Plot of the simultaneous intervals library(ggplot2) ggplot(data = SCI, aes(x = Contrast, y = MD, color = Sig)) + geom_point(size = 4) + geom_segment(aes(x = Contrast, xend = Contrast, y = LL, yend = UL, alpha = Alpha), lwd = 1.5) + geom_hline(yintercept = 0, lty = \u0026quot;dotted\u0026quot;) + scale_color_manual(values = c(\u0026quot;Black\u0026quot;, \u0026quot;Gold\u0026quot;)) + scale_x_continuous( name = \u0026quot;\u0026quot;, breaks = 1:10, labels = SCI$Names ) + ylab(\u0026quot;Mean Difference\u0026quot;) + coord_flip() + theme_bw() + theme( legend.position = \u0026quot;none\u0026quot;, panel.grid.minor = element_blank(), panel.grid.major = element_blank() ) Exercises Fit a model that explores mean differences in tvhours by the party affiliation (partyid variable). Do the means differ? Using the post-hoc tests, run post-hoc tests to test all pairwise differences.     ","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"15262da5fd3bebaef28aa39d6e71e570","permalink":"https://psqf6250.brandonlebeau.org/rcode/model-part2/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/rcode/model-part2/","section":"rcode","summary":"Building Upon Linear Models","tags":null,"title":"Building Upon Linear Models","type":"book"},{"authors":null,"categories":null,"content":"   This section of notes will use the following packages.\nlibrary(modelr) library(broom) library(forcats) library(stringr) library(tidyverse) Interactions Interactions are an important modeling concept that can greatly increase model fit, prediction accuracy, and explained variance. Interactions can be difficult to interpret, however, we will explore them in more detail here with particular attention to graphical displays of interactions and also exploring the design matrix for how interactions are included in the model fitting procedure.\nWe will use the heights data from the modelr package to explore interactions. The primary interactions that we will explore are between two (or more) categorical predictors and also the interaction between a categorical predictor and a continuous predictor. Interpretations are similar between two continuous predictors as well.\nHere are the first few rows of the data:\nheights ## # A tibble: 7,006 × 8 ## income height weight age marital sex education afqt ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 19000 60 155 53 married female 13 6.84 ## 2 35000 70 156 51 married female 10 49.4 ## 3 105000 65 195 52 married male 16 99.4 ## 4 40000 63 197 54 married female 14 44.0 ## 5 75000 66 190 49 married male 14 59.7 ## 6 102000 68 200 49 divorced female 18 98.8 ## 7 0 74 225 48 married male 16 82.3 ## 8 70000 64 160 54 divorced female 12 50.3 ## 9 60000 69 162 55 divorced male 12 89.7 ## 10 150000 69 194 54 divorced male 13 96.0 ## # … with 6,996 more rows Suppose we were interested in exploring the relationship between sex and afqt (armed forces qualifications test, in percentiles) and if this relationship is moderated by marital status. First, it may be useful to get a baseline to see the relationship between sex and afqt.\nafqt_sex \u0026lt;- lm(afqt ~ sex, data = heights) summary(afqt_sex) ## ## Call: ## lm(formula = afqt ~ sex, data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.876 -26.034 -4.429 24.046 59.406 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 41.8760 0.5093 82.221 \u0026lt;2e-16 *** ## sexfemale -1.2824 0.7074 -1.813 0.0699 . ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 29.03 on 6742 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.0004872, Adjusted R-squared: 0.000339 ## F-statistic: 3.287 on 1 and 6742 DF, p-value: 0.06989 Notice that there is a small effect, which is not significant if using an alpha value of 0.05. Also, notice the extremely small r-square value here, this is actually a good finding, we would hope there would be no statistical differences between males and females on this qualifications test. Now lets start adding in marital status. We can do this as follows (Note, I have combined separated and widowed into a single category due to relatively small sample sizes):\nheights \u0026lt;- heights %\u0026gt;% mutate( marital_comb = fct_recode(marital, \u0026#39;Other\u0026#39; = \u0026#39;separated\u0026#39;, \u0026#39;Other\u0026#39; = \u0026#39;widowed\u0026#39; ) ) afqt_sex_marital \u0026lt;- lm(afqt ~ sex + marital_comb, data = heights) summary(afqt_sex_marital) ## ## Call: ## lm(formula = afqt ~ sex + marital_comb, data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.957 -23.174 -4.386 22.389 74.002 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 31.7982 0.9093 34.970 \u0026lt; 2e-16 *** ## sexfemale -0.6819 0.6869 -0.993 0.320829 ## marital_combmarried 16.1587 0.9727 16.611 \u0026lt; 2e-16 *** ## marital_combOther -5.5953 1.5186 -3.685 0.000231 *** ## marital_combdivorced 6.2811 1.1232 5.592 2.33e-08 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 28.02 on 6739 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.069, Adjusted R-squared: 0.06845 ## F-statistic: 124.9 on 4 and 6739 DF, p-value: \u0026lt; 2.2e-16 This model only contains what are often referred to as main effects. Namely, these are only the additive effects of sex and marital variables. To get a sense as to what the design matrix looks like, we can use model_matrix.\nmodel_matrix(heights, afqt ~ sex + marital_comb) ## # A tibble: 6,744 × 5 ## `(Intercept)` sexfemale marital_combmarried marital_combOth… marital_combdiv… ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 1 0 0 ## 2 1 1 1 0 0 ## 3 1 0 1 0 0 ## 4 1 1 1 0 0 ## 5 1 0 1 0 0 ## 6 1 1 0 0 1 ## 7 1 0 1 0 0 ## 8 1 1 0 0 1 ## 9 1 0 0 0 1 ## 10 1 0 0 0 1 ## # … with 6,734 more rows To add the interaction between the two variables (multiplicative effects), we can add one additional term to the lm function call.\ninteract_mod \u0026lt;- lm(afqt ~ sex + marital_comb + sex:marital_comb, data = heights) summary(interact_mod) ## ## Call: ## lm(formula = afqt ~ sex + marital_comb + sex:marital_comb, data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.981 -22.978 -4.336 22.488 75.275 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 31.4205 1.1536 27.237 \u0026lt; 2e-16 *** ## sexfemale 0.1564 1.7184 0.091 0.928 ## marital_combmarried 16.5603 1.3273 12.477 \u0026lt; 2e-16 *** ## marital_combOther -2.5872 2.4677 -1.048 0.294 ## marital_combdivorced 6.2796 1.5814 3.971 7.24e-05 *** ## sexfemale:marital_combmarried -0.8856 1.9516 -0.454 0.650 ## sexfemale:marital_combOther -4.7414 3.1645 -1.498 0.134 ## sexfemale:marital_combdivorced -0.1494 2.2535 -0.066 0.947 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 28.02 on 6736 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.06936, Adjusted R-squared: 0.0684 ## F-statistic: 71.72 on 7 and 6736 DF, p-value: \u0026lt; 2.2e-16 There is also an anova function that gives more traditional anova and sum of squares information.\nanova(interact_mod) ## Analysis of Variance Table ## ## Response: afqt ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## sex 1 2769 2769 3.5267 0.06043 . ## marital_comb 3 389378 129793 165.3055 \u0026lt; 2e-16 *** ## sex:marital_comb 3 2058 686 0.8738 0.45380 ## Residuals 6736 5288896 785 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 In this model, there are not actually any significant results for the interaction, but lets explore the design matrix to see exactly what is happening.\nmodel_matrix(heights, afqt ~ sex + marital_comb + sex:marital_comb) ## # A tibble: 6,744 × 8 ## `(Intercept)` sexfemale marital_combmarried marital_combOth… marital_combdiv… ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 1 0 0 ## 2 1 1 1 0 0 ## 3 1 0 1 0 0 ## 4 1 1 1 0 0 ## 5 1 0 1 0 0 ## 6 1 1 0 0 1 ## 7 1 0 1 0 0 ## 8 1 1 0 0 1 ## 9 1 0 0 0 1 ## 10 1 0 0 0 1 ## # … with 6,734 more rows, and 3 more variables: ## # `sexfemale:marital_combmarried` \u0026lt;dbl\u0026gt;, `sexfemale:marital_combOther` \u0026lt;dbl\u0026gt;, ## # `sexfemale:marital_combdivorced` \u0026lt;dbl\u0026gt; This model specifically adds columns that literally are multiplications of other columns in the design matrix. This is why interactions are often depicted with the symbol “x”, e.g. marital x sex. R uses : as interactions. The interaction model can also be specified in an alternate more compact formula:\nsummary(lm(afqt ~ sex * marital_comb, data = heights)) ## ## Call: ## lm(formula = afqt ~ sex * marital_comb, data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.981 -22.978 -4.336 22.488 75.275 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 31.4205 1.1536 27.237 \u0026lt; 2e-16 *** ## sexfemale 0.1564 1.7184 0.091 0.928 ## marital_combmarried 16.5603 1.3273 12.477 \u0026lt; 2e-16 *** ## marital_combOther -2.5872 2.4677 -1.048 0.294 ## marital_combdivorced 6.2796 1.5814 3.971 7.24e-05 *** ## sexfemale:marital_combmarried -0.8856 1.9516 -0.454 0.650 ## sexfemale:marital_combOther -4.7414 3.1645 -1.498 0.134 ## sexfemale:marital_combdivorced -0.1494 2.2535 -0.066 0.947 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 28.02 on 6736 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.06936, Adjusted R-squared: 0.0684 ## F-statistic: 71.72 on 7 and 6736 DF, p-value: \u0026lt; 2.2e-16 Exercises Using the gss_cat data from the forcats package, fit a model that predicts age with marital status, partyid, and the interaction between the two. How well does this model fit? How is the intercept interpreted here? Do the results change when collapsing the partyid variable into the following three categories:  Republican Democrat Other     Visualize Model Results When exploring model results, visualizing the model results is often more useful than looking at a table of coefficients. In addition, if the model is simply attempting to predict means (ANOVA), plotting often simply involves computing means for the different categories. This section will also explore how best to visualize interactions.\nIf we simply would like to show the main effects from the model above predicting Armed Forces Qualifiactions Test Score with marital status and sex, we could calculate the means of these groups.\nmarital_means \u0026lt;- heights %\u0026gt;% group_by(marital_comb) %\u0026gt;% summarise(avg_afqt = mean(afqt, na.rm = TRUE), sd_afqt = sd(afqt, na.rm = TRUE), n = n()) %\u0026gt;% mutate(se_mean = sd_afqt/sqrt(n), group = \u0026#39;Marital\u0026#39;, levels = marital_comb) %\u0026gt;% ungroup() %\u0026gt;% dplyr::select(-marital_comb) sex_means \u0026lt;- heights %\u0026gt;% group_by(sex) %\u0026gt;% summarise(avg_afqt = mean(afqt, na.rm = TRUE), sd_afqt = sd(afqt, na.rm = TRUE), n = n()) %\u0026gt;% mutate(se_mean = sd_afqt/sqrt(n), group = \u0026#39;Sex\u0026#39;, levels = sex) %\u0026gt;% ungroup() %\u0026gt;% dplyr::select(-sex) comb_means \u0026lt;- bind_rows(marital_means, sex_means) comb_means ## # A tibble: 6 × 6 ## avg_afqt sd_afqt n se_mean group levels ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; ## 1 31.5 27.7 1124 0.828 Marital single ## 2 47.6 29.1 3806 0.472 Marital married ## 3 25.7 24.1 527 1.05 Marital Other ## 4 37.7 26.6 1549 0.676 Marital divorced ## 5 41.9 29.8 3402 0.511 Sex male ## 6 40.6 28.3 3604 0.472 Sex female The effects can now be shown in a figure.\nggplot(comb_means, aes(avg_afqt, fct_reorder(levels, avg_afqt))) + geom_point(size = 3) + facet_grid(group ~ ., scales = \u0026#39;free\u0026#39;, space = \u0026#39;free\u0026#39;) + ylab(\u0026quot;Groups\u0026quot;) + xlab(\u0026quot;Average Armed Forces Qualification Score\u0026quot;) + theme_bw() Since we computed standard errors, we could also add error bars using geom_errorbarh.\nggplot(comb_means, aes(avg_afqt, fct_reorder(levels, avg_afqt))) + geom_point(size = 3) + geom_errorbarh(aes(xmin = avg_afqt - se_mean*2, xmax = avg_afqt + se_mean*2), height = 0) + facet_grid(group ~ ., scales = \u0026#39;free\u0026#39;, space = \u0026#39;free\u0026#39;) + ylab(\u0026quot;Groups\u0026quot;) + xlab(\u0026quot;Average Armed Forces Qualification Score\u0026quot;) + theme_bw() Plotting the interaction happens in a similar fashion. Namely, we will now calculate means by using both variables in a single group_by statement.\nint_means \u0026lt;- heights %\u0026gt;% group_by(marital_comb, sex) %\u0026gt;% summarise(avg_afqt = mean(afqt, na.rm = TRUE), sd_afqt = sd(afqt, na.rm = TRUE), n = n()) %\u0026gt;% mutate(se_mean = sd_afqt/sqrt(n)) ## `summarise()` has grouped output by \u0026#39;marital_comb\u0026#39;. You can override using the ## `.groups` argument. int_means ## # A tibble: 8 × 6 ## # Groups: marital_comb [4] ## marital_comb sex avg_afqt sd_afqt n se_mean ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 single male 31.4 28.2 624 1.13 ## 2 single female 31.6 27.3 500 1.22 ## 3 married male 48.0 29.8 1905 0.683 ## 4 married female 47.3 28.5 1901 0.653 ## 5 Other male 28.8 25.7 177 1.93 ## 6 Other female 24.2 23.1 350 1.24 ## 7 divorced male 37.7 27.7 696 1.05 ## 8 divorced female 37.7 25.7 853 0.879 These means should now match the last model from the previous lecture. We can now plot these directly.\nggplot(int_means, aes(avg_afqt, fct_reorder(marital_comb, avg_afqt), shape = sex, linetype = sex, group = sex)) + geom_point(size = 3) + geom_line(size = 1) + ylab(\u0026quot;Groups\u0026quot;) + xlab(\u0026quot;Average Armed Forces Qualification Score\u0026quot;) + theme_bw() Standard error bars can be shown here, but may complicate the figure too much.\nggplot(int_means, aes(avg_afqt, fct_reorder(marital_comb, avg_afqt), shape = sex, linetype = sex, group = sex)) + geom_point(size = 3) + geom_line(size = 1) + geom_errorbarh(aes(xmin = avg_afqt - se_mean * 2, xmax = avg_afqt + se_mean * 2), height = 0) + ylab(\u0026quot;Groups\u0026quot;) + xlab(\u0026quot;Average Armed Forces Qualification Score\u0026quot;) + theme_bw() Exercises Using the gss_cat data from the forcats package, fit a model that predicts age with marital status, partyid, and the interaction between the two. Create a figure that explores the interaction between the two variables. Add a third variable to the model, race. Include this as a main effect, plus interacting with the other variables.    Interaction between continuous and categorical predictors Using again the heights data, suppose we wished to again predict the Armed Forces Qualifications Test score (afqt) with marital status and height. This can be done with the linear model.\nsummary(lm(afqt ~ marital_comb + height, data = heights)) ## ## Call: ## lm(formula = afqt ~ marital_comb + height, data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.150 -22.517 -4.248 22.097 78.355 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -28.99932 5.67144 -5.113 3.25e-07 *** ## marital_combmarried 16.13531 0.96385 16.741 \u0026lt; 2e-16 *** ## marital_combOther -4.63978 1.50160 -3.090 0.00201 ** ## marital_combdivorced 6.75490 1.11278 6.070 1.35e-09 *** ## height 0.89847 0.08329 10.787 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 27.78 on 6739 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.08467, Adjusted R-squared: 0.08413 ## F-statistic: 155.8 on 4 and 6739 DF, p-value: \u0026lt; 2.2e-16 You’ll notice here that the intercept is negative. Why is this negative? You can most easily see this by looking at a figure of the data. Note, here I am simply showing the dependent and the height variable ignoring the marital status, however, this will have a slightly different slope than the one above (Try it).\nggplot(heights, aes(x = height, y = afqt)) + geom_jitter(size = 2) + geom_abline(intercept = -26.02, slope = 1.002, size = 1, color = \u0026#39;blue\u0026#39;) + coord_cartesian(xlim = c(0, 90), ylim = c(-30, 105)) + ylab(\u0026quot;Average Armed Forces Qualification Score\u0026quot;) + xlab(\u0026quot;Height\u0026quot;) + theme_bw() ## Warning: Removed 262 rows containing missing values (geom_point). It may be better to mean center the height variable.\nsummary(lm(afqt ~ marital_comb + I(height - mean(heights$height)), data = heights)) ## ## Call: ## lm(formula = afqt ~ marital_comb + I(height - mean(heights$height)), ## data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.150 -22.517 -4.248 22.097 78.355 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 31.29186 0.84798 36.90 \u0026lt; 2e-16 *** ## marital_combmarried 16.13531 0.96385 16.74 \u0026lt; 2e-16 *** ## marital_combOther -4.63978 1.50160 -3.09 0.00201 ** ## marital_combdivorced 6.75490 1.11278 6.07 1.35e-09 *** ## I(height - mean(heights$height)) 0.89847 0.08329 10.79 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 27.78 on 6739 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.08467, Adjusted R-squared: 0.08413 ## F-statistic: 155.8 on 4 and 6739 DF, p-value: \u0026lt; 2.2e-16 Notice that the mean effects do not change, but rather just the location of the intercept. This is a traditional analysis of covariance model.\nInteractions are simple to add now, they follow the same syntax as the categorical predictors from above. For example, if we wanted to add an interaction between height and marital status, we could do this as follows.\nsummary(lm(afqt ~ marital_comb + I(height - mean(heights$height)) + marital_comb:I(height - mean(heights$height)), data = heights)) ## ## Call: ## lm(formula = afqt ~ marital_comb + I(height - mean(heights$height)) + ## marital_comb:I(height - mean(heights$height)), data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.768 -22.664 -4.183 21.972 78.762 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 31.32214 0.84911 ## marital_combmarried 16.08587 0.96531 ## marital_combOther -4.58940 1.53070 ## marital_combdivorced 6.66304 1.11478 ## I(height - mean(heights$height)) 0.76180 0.20923 ## marital_combmarried:I(height - mean(heights$height)) 0.22912 0.23745 ## marital_combOther:I(height - mean(heights$height)) 0.21641 0.37137 ## marital_combdivorced:I(height - mean(heights$height)) -0.02469 0.27510 ## t value Pr(\u0026gt;|t|) ## (Intercept) 36.888 \u0026lt; 2e-16 *** ## marital_combmarried 16.664 \u0026lt; 2e-16 *** ## marital_combOther -2.998 0.002725 ** ## marital_combdivorced 5.977 2.39e-09 *** ## I(height - mean(heights$height)) 3.641 0.000274 *** ## marital_combmarried:I(height - mean(heights$height)) 0.965 0.334624 ## marital_combOther:I(height - mean(heights$height)) 0.583 0.560092 ## marital_combdivorced:I(height - mean(heights$height)) -0.090 0.928481 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 27.79 on 6736 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.08494, Adjusted R-squared: 0.08399 ## F-statistic: 89.32 on 7 and 6736 DF, p-value: \u0026lt; 2.2e-16 or\nsummary(lm(afqt ~ marital_comb * I(height - mean(heights$height)), data = heights)) ## ## Call: ## lm(formula = afqt ~ marital_comb * I(height - mean(heights$height)), ## data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.768 -22.664 -4.183 21.972 78.762 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 31.32214 0.84911 ## marital_combmarried 16.08587 0.96531 ## marital_combOther -4.58940 1.53070 ## marital_combdivorced 6.66304 1.11478 ## I(height - mean(heights$height)) 0.76180 0.20923 ## marital_combmarried:I(height - mean(heights$height)) 0.22912 0.23745 ## marital_combOther:I(height - mean(heights$height)) 0.21641 0.37137 ## marital_combdivorced:I(height - mean(heights$height)) -0.02469 0.27510 ## t value Pr(\u0026gt;|t|) ## (Intercept) 36.888 \u0026lt; 2e-16 *** ## marital_combmarried 16.664 \u0026lt; 2e-16 *** ## marital_combOther -2.998 0.002725 ** ## marital_combdivorced 5.977 2.39e-09 *** ## I(height - mean(heights$height)) 3.641 0.000274 *** ## marital_combmarried:I(height - mean(heights$height)) 0.965 0.334624 ## marital_combOther:I(height - mean(heights$height)) 0.583 0.560092 ## marital_combdivorced:I(height - mean(heights$height)) -0.090 0.928481 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 27.79 on 6736 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.08494, Adjusted R-squared: 0.08399 ## F-statistic: 89.32 on 7 and 6736 DF, p-value: \u0026lt; 2.2e-16 What do these coefficients mean however? This is best explained with a picture.\nmodel_summary \u0026lt;- augment(lm(afqt ~ marital_comb * I(height - mean(heights$height)), data = heights)) model_summary \u0026lt;- rename(model_summary, \u0026#39;height_center\u0026#39; = `I(height - mean(heights$height))`) model_summary ## # A tibble: 6,744 × 9 ## .rownames afqt marital_comb height_center .fitted .hat .sigma .cooksd ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;I\u0026lt;dbl\u0026gt;\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 6.84 married -7.10 40.4 0.00115 27.8 2.09e-4 ## 2 2 49.4 married 2.90 50.3 0.000390 27.8 4.39e-8 ## 3 3 99.4 married -2.10 45.3 0.000360 27.8 1.70e-4 ## 4 4 44.0 married -4.10 43.3 0.000576 27.8 4.33e-8 ## 5 5 59.7 married -1.10 46.3 0.000301 27.8 8.70e-6 ## 6 6 98.8 divorced 0.896 38.6 0.000737 27.8 4.33e-4 ## 7 7 82.3 married 6.90 54.2 0.00100 27.8 1.28e-4 ## 8 8 50.3 divorced -3.10 35.7 0.000976 27.8 3.37e-5 ## 9 9 89.7 divorced 1.90 39.4 0.000884 27.8 3.63e-4 ## 10 10 96.0 divorced 1.90 39.4 0.000884 27.8 4.59e-4 ## # … with 6,734 more rows, and 1 more variable: .std.resid \u0026lt;dbl\u0026gt; ggplot(model_summary, aes(height_center, afqt)) + geom_jitter(alpha = .1) + geom_line(aes(x = height_center, y = .fitted, color = marital_comb), size = 2) + ylab(\u0026quot;Average Armed Forces Qualification Score\u0026quot;) + xlab(\u0026quot;Height\u0026quot;) + theme_bw() The figure for the traditional ANCOVA model initially explored would look like:\nmodel_summary \u0026lt;- augment(lm(afqt ~ marital_comb + I(height - mean(heights$height)), data = heights)) model_summary \u0026lt;- rename(model_summary, \u0026#39;height_center\u0026#39; = `I(height - mean(heights$height))`) model_summary ## # A tibble: 6,744 × 9 ## .rownames afqt marital_comb height_center .fitted .hat .sigma .cooksd ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;I\u0026lt;dbl\u0026gt;\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 6.84 married -7.10 41.0 0.000753 27.8 2.29e-4 ## 2 2 49.4 married 2.90 50.0 0.000337 27.8 2.99e-8 ## 3 3 99.4 married -2.10 45.5 0.000320 27.8 2.41e-4 ## 4 4 44.0 married -4.10 43.7 0.000439 27.8 9.09e-9 ## 5 5 59.7 married -1.10 46.4 0.000288 27.8 1.31e-5 ## 6 6 98.8 divorced 0.896 38.9 0.000684 27.8 6.38e-4 ## 7 7 82.3 married 6.90 53.6 0.000674 27.8 1.44e-4 ## 8 8 50.3 divorced -3.10 35.3 0.000736 27.8 4.31e-5 ## 9 9 89.7 divorced 1.90 39.7 0.000716 27.8 4.63e-4 ## 10 10 96.0 divorced 1.90 39.7 0.000716 27.8 5.88e-4 ## # … with 6,734 more rows, and 1 more variable: .std.resid \u0026lt;dbl\u0026gt; ggplot(model_summary, aes(height_center, afqt)) + geom_jitter(alpha = .1) + geom_line(aes(x = height_center, y = .fitted, color = marital_comb), size = 2) + ylab(\u0026quot;Average Armed Forces Qualification Score\u0026quot;) + xlab(\u0026quot;Height\u0026quot;) + theme_bw() Exercises Using the gss_cat data from the forcats package, fit a model that predicts age with marital status, tvhours, and the interaction between the two. Interpret the parameter estimates from this model? Is there evidence that the interaction is adding to the model? Create a figure that explores the interaction between the two variables. Fit another model that predicts age with marital status, party affiliation, and tvhours (main effects) as well as the interaction between marital status and tvhours and party afilliation and tvhours (two second order interactions). Interpret the effects for this model.    ","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"fa24f48469663a4873b73b9b38b6d445","permalink":"https://psqf6250.brandonlebeau.org/rcode/adv-model/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/rcode/adv-model/","section":"rcode","summary":"Advanced Modeling Topics","tags":null,"title":"Advanced Modeling Topics","type":"book"},{"authors":null,"categories":null,"content":"   I want to spend a little bit of time talking about model assumptions. These are important and should be checked for any analysis. The following packages will be used for this set of notes.\nlibrary(modelr) library(broom) library(forcats) library(tidyverse) We are also going to use this model from the advanced model section to explore model assumptions.\nheights2 \u0026lt;- heights %\u0026gt;% mutate( marital_comb = fct_recode(marital, \u0026#39;Other\u0026#39; = \u0026#39;separated\u0026#39;, \u0026#39;Other\u0026#39; = \u0026#39;widowed\u0026#39; ) ) afqt_mod \u0026lt;- lm(afqt ~ marital_comb + height, data = heights2) summary(afqt_mod) ## ## Call: ## lm(formula = afqt ~ marital_comb + height, data = heights2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.150 -22.517 -4.248 22.097 78.355 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -28.99932 5.67144 -5.113 3.25e-07 *** ## marital_combmarried 16.13531 0.96385 16.741 \u0026lt; 2e-16 *** ## marital_combOther -4.63978 1.50160 -3.090 0.00201 ** ## marital_combdivorced 6.75490 1.11278 6.070 1.35e-09 *** ## height 0.89847 0.08329 10.787 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 27.78 on 6739 degrees of freedom ## (262 observations deleted due to missingness) ## Multiple R-squared: 0.08467, Adjusted R-squared: 0.08413 ## F-statistic: 155.8 on 4 and 6739 DF, p-value: \u0026lt; 2.2e-16 plot function with model object One of the nicest features when model building within R, is that many diagnostic plots are very accessible using the plot function. For example:\nplot(afqt_mod) These plots are shown one at a time in an interactive R session, but allow you to check many common assumptions such as normal residuals, linearity, homogeneity of variance, and even leverage.\nYou can see from these figures that there is quite a bit to be desired from our model. First, the residuals are not normally distributed (very heavy tails) and more problematic is that the residuals have a trend to them. This suggests that we are missing an important variable in the model.\nSuppose we add some additional variables to the model keeping the model additive. Note, I am also mean centering many of the continuous variables to ease interpretation slightly.\nheights2 \u0026lt;- heights2 %\u0026gt;% mutate(income2 = ifelse(income == 0, .001, income), height2 = height - mean(height, na.rm = TRUE), education2 = education - mean(education, na.rm = TRUE), weight2 = weight - mean(weight, na.rm = TRUE), log_income = log(income2)) afqt_alt \u0026lt;- lm(afqt ~ marital_comb + height2 + education2 + log_income + weight2, data = heights2) summary(afqt_alt) ## ## Call: ## lm(formula = afqt ~ marital_comb + height2 + education2 + log_income + ## weight2, data = heights2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -80.466 -16.515 -2.036 16.088 101.730 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 32.645765 0.717463 45.502 \u0026lt; 2e-16 *** ## marital_combmarried 9.312248 0.802952 11.598 \u0026lt; 2e-16 *** ## marital_combOther -2.537031 1.231340 -2.060 0.0394 * ## marital_combdivorced 4.790976 0.913836 5.243 1.63e-07 *** ## height2 0.781474 0.077526 10.080 \u0026lt; 2e-16 *** ## education2 5.911616 0.111848 52.854 \u0026lt; 2e-16 *** ## log_income 0.395176 0.038720 10.206 \u0026lt; 2e-16 *** ## weight2 -0.027669 0.007074 -3.911 9.27e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 22.57 on 6637 degrees of freedom ## (361 observations deleted due to missingness) ## Multiple R-squared: 0.3969, Adjusted R-squared: 0.3963 ## F-statistic: 624 on 7 and 6637 DF, p-value: \u0026lt; 2.2e-16 plot(afqt_alt) We could continue to model this variable by including additional predictors or interactions between our current predictors to attempt to remove this strong trend in the residuals. I’m going to stop here however and move on to another topic.\n Exploring individual residuals Evaluating all of the residuals in a single step using the plot function can be useful initially to explore model quality. However, eventually it is useful to explore residuals for specific values to explore why these are large.\nThere are a few options to do this, one uses the broom package, the other uses the modelr package. I show both below.\naug_afqt \u0026lt;- augment(afqt_alt) aug_afqt ## # A tibble: 6,645 × 13 ## .rownames afqt marital_comb height2 education2 log_income weight2 .fitted ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 6.84 married -7.10 -0.218 9.85 -33.3 39.9 ## 2 2 49.4 married 2.90 -3.22 10.5 -32.3 30.2 ## 3 3 99.4 married -2.10 2.78 11.6 6.70 61.1 ## 4 4 44.0 married -4.10 0.782 10.6 8.70 47.3 ## 5 5 59.7 married -1.10 0.782 11.2 1.70 50.1 ## 6 6 98.8 divorced 0.896 4.78 11.5 11.7 70.6 ## 7 7 82.3 married 6.90 2.78 -6.91 36.7 60.0 ## 8 8 50.3 divorced -3.10 -1.22 11.2 -28.3 33.0 ## 9 9 89.7 divorced 1.90 -1.22 11.0 -26.3 36.8 ## 10 10 96.0 divorced 1.90 -0.218 11.9 5.70 42.2 ## # … with 6,635 more rows, and 5 more variables: .resid \u0026lt;dbl\u0026gt;, .hat \u0026lt;dbl\u0026gt;, ## # .sigma \u0026lt;dbl\u0026gt;, .cooksd \u0026lt;dbl\u0026gt;, .std.resid \u0026lt;dbl\u0026gt; height_resid \u0026lt;- heights2 %\u0026gt;% add_residuals(afqt_alt) %\u0026gt;% add_predictions(afqt_alt) height_resid ## # A tibble: 7,006 × 16 ## income height weight age marital sex education afqt marital_comb income2 ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 19000 60 155 53 married fema… 13 6.84 married 1.9 e+4 ## 2 35000 70 156 51 married fema… 10 49.4 married 3.5 e+4 ## 3 105000 65 195 52 married male 16 99.4 married 1.05e+5 ## 4 40000 63 197 54 married fema… 14 44.0 married 4 e+4 ## 5 75000 66 190 49 married male 14 59.7 married 7.5 e+4 ## 6 102000 68 200 49 divorc… fema… 18 98.8 divorced 1.02e+5 ## 7 0 74 225 48 married male 16 82.3 married 1 e-3 ## 8 70000 64 160 54 divorc… fema… 12 50.3 divorced 7 e+4 ## 9 60000 69 162 55 divorc… male 12 89.7 divorced 6 e+4 ## 10 150000 69 194 54 divorc… male 13 96.0 divorced 1.5 e+5 ## # … with 6,996 more rows, and 6 more variables: height2 \u0026lt;dbl\u0026gt;, ## # education2 \u0026lt;dbl\u0026gt;, weight2 \u0026lt;dbl\u0026gt;, log_income \u0026lt;dbl\u0026gt;, resid \u0026lt;dbl\u0026gt;, pred \u0026lt;dbl\u0026gt; The benefit to the augment function from the broom package is that it includes more information. The main benefit fo the add_residuals function from the modelr pacakge is that it includes all of the original data, not just those variables included in the final model. Therefore if you are fitting a model and want to explore trends in residuals based on other data characteristics not currently in the model, using the add_residuals function would likely be better for this use.\nFiltering and Plotting Residuals Now that we have these in a data frame, we could easily plot or filter residuals to explore cases in which the model is not adequatly fitting the dependent variable. For example, maybe we want to explore residuals greater than 50 in absolute value. This can easily be done with dplyr and the filter function.\nfilter(height_resid, abs(resid) \u0026gt; 50) ## # A tibble: 113 × 16 ## income height weight age marital sex education afqt marital_comb income2 ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 60000 69 162 55 divorc… male 12 89.7 divorced 6 e+4 ## 2 150000 69 194 54 divorc… male 13 96.0 divorced 1.5e+5 ## 3 45000 69 240 53 married male 12 98.8 married 4.5e+4 ## 4 0 66 195 50 married fema… 14 99.2 married 1 e-3 ## 5 93000 68 182 48 divorc… male 12 86.9 divorced 9.3e+4 ## 6 43000 64 145 51 divorc… fema… 12 94.8 divorced 4.3e+4 ## 7 52000 72 230 52 married male 12 93.1 married 5.2e+4 ## 8 35000 59 165 49 divorc… fema… 12 80.0 divorced 3.5e+4 ## 9 85000 61 140 55 married fema… 18 15.7 married 8.5e+4 ## 10 0 69 186 55 separa… fema… 12 88.8 Other 1 e-3 ## # … with 103 more rows, and 6 more variables: height2 \u0026lt;dbl\u0026gt;, education2 \u0026lt;dbl\u0026gt;, ## # weight2 \u0026lt;dbl\u0026gt;, log_income \u0026lt;dbl\u0026gt;, resid \u0026lt;dbl\u0026gt;, pred \u0026lt;dbl\u0026gt; You can also plot residuals directly or compared to other predictors.\nggplot(height_resid, aes(resid)) + geom_histogram() + theme_bw() + xlab(\u0026quot;Residuals\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 361 rows containing non-finite values (stat_bin). ggplot(height_resid, aes(resid, education2)) + geom_point(size = 3) + geom_smooth(size = 1, se = FALSE) + xlab(\u0026quot;Residuals\u0026quot;) + ylab(\u0026quot;Education (mean centered)\u0026quot;) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; ## Warning: Removed 361 rows containing non-finite values (stat_smooth). ## Warning: Removed 361 rows containing missing values (geom_point).   ","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"6abe3dd4de8607a0a5557c6954118349","permalink":"https://psqf6250.brandonlebeau.org/rcode/model-assumptions/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/rcode/model-assumptions/","section":"rcode","summary":"Model Assumptions","tags":null,"title":"Model Assumptions","type":"book"},{"authors":null,"categories":null,"content":"   I want to spend a little bit of time talking about ways to model non-linear trends within a linear model as well as show an example of conducting a logistic regression within R using the glm function. This set of notes will use the following packages:\nlibrary(modelr) library(broom) library(forcats) library(tidyverse) Modeling non-linear trends Modeling non-linear trends can be important and a great way to increase variance explained. There are many ways to model non-linear trends, including non-linear models, but I am going to focus on a linear modeling framework to include non-linear trends by adding quadratic terms.\nThese types of models are flexible and relatively easy to interpret, however have the drawback that prediction outside of the data at hand (extrapolation) can be problematic. Using the final model from the model assumptions lecture, lets see if we can improve model fit and the trend in the residuals by adding some non-linearity in the form of quadratic terms.\nIf you recall, here is the model that was used last time.\nheights2 \u0026lt;- heights %\u0026gt;% mutate( marital_comb = fct_recode(marital, \u0026#39;Other\u0026#39; = \u0026#39;separated\u0026#39;, \u0026#39;Other\u0026#39; = \u0026#39;widowed\u0026#39; ), income2 = ifelse(income == 0, .001, income), height2 = height - mean(height, na.rm = TRUE), education2 = education - mean(education, na.rm = TRUE), weight2 = weight - mean(weight, na.rm = TRUE), log_income = log(income2) ) afqt_alt \u0026lt;- lm(afqt ~ marital_comb + height2 + education2 + log_income + weight2, data = heights2) summary(afqt_alt) ## ## Call: ## lm(formula = afqt ~ marital_comb + height2 + education2 + log_income + ## weight2, data = heights2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -80.466 -16.515 -2.036 16.088 101.730 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 32.645765 0.717463 45.502 \u0026lt; 2e-16 *** ## marital_combmarried 9.312248 0.802952 11.598 \u0026lt; 2e-16 *** ## marital_combOther -2.537031 1.231340 -2.060 0.0394 * ## marital_combdivorced 4.790976 0.913836 5.243 1.63e-07 *** ## height2 0.781474 0.077526 10.080 \u0026lt; 2e-16 *** ## education2 5.911616 0.111848 52.854 \u0026lt; 2e-16 *** ## log_income 0.395176 0.038720 10.206 \u0026lt; 2e-16 *** ## weight2 -0.027669 0.007074 -3.911 9.27e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 22.57 on 6637 degrees of freedom ## (361 observations deleted due to missingness) ## Multiple R-squared: 0.3969, Adjusted R-squared: 0.3963 ## F-statistic: 624 on 7 and 6637 DF, p-value: \u0026lt; 2.2e-16 Let’s see if there are non-linear trends in the height, weight, or income variables. I am going to add these by specifically creating additional variables and using these as new predictors. You could also create these by using the insulate function I() to do the operation within the model syntax.\nheights2 \u0026lt;- heights2 %\u0026gt;% mutate( height2_quad = height2 ^ 2, weight2_quad = weight2 ^ 2, log_income_quad = log_income ^ 2, education2_quad = education2 ^ 2 ) afqt_alt \u0026lt;- lm(afqt ~ marital_comb + height2 + education2 + log_income + weight2 + education2_quad + height2_quad + weight2_quad + log_income_quad, data = heights2) summary(afqt_alt) ## ## Call: ## lm(formula = afqt ~ marital_comb + height2 + education2 + log_income + ## weight2 + education2_quad + height2_quad + weight2_quad + ## log_income_quad, data = heights2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -77.098 -16.265 -2.331 15.587 91.339 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.665e+01 1.598e+00 10.419 \u0026lt; 2e-16 *** ## marital_combmarried 8.340e+00 7.993e-01 10.434 \u0026lt; 2e-16 *** ## marital_combOther -2.604e+00 1.218e+00 -2.137 0.0326 * ## marital_combdivorced 4.262e+00 9.061e-01 4.703 2.61e-06 *** ## height2 7.488e-01 7.967e-02 9.399 \u0026lt; 2e-16 *** ## education2 5.467e+00 1.173e-01 46.609 \u0026lt; 2e-16 *** ## log_income -3.828e-01 8.307e-02 -4.609 4.12e-06 *** ## weight2 -4.652e-02 8.046e-03 -5.782 7.72e-09 *** ## education2_quad 5.535e-02 2.488e-02 2.224 0.0262 * ## height2_quad -3.018e-02 1.392e-02 -2.169 0.0302 * ## weight2_quad 4.135e-04 8.359e-05 4.947 7.73e-07 *** ## log_income_quad 2.177e-01 2.018e-02 10.791 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 22.33 on 6633 degrees of freedom ## (361 observations deleted due to missingness) ## Multiple R-squared: 0.4102, Adjusted R-squared: 0.4092 ## F-statistic: 419.3 on 11 and 6633 DF, p-value: \u0026lt; 2.2e-16 Alternatively, this model could look like:\nafqt_alt \u0026lt;- lm(afqt ~ marital_comb + height2 + education2 + log_income + weight2 + I(education2^2) + I(height2^2) + I(weight2^2) + I(log_income^2), data = heights2) summary(afqt_alt) ## ## Call: ## lm(formula = afqt ~ marital_comb + height2 + education2 + log_income + ## weight2 + I(education2^2) + I(height2^2) + I(weight2^2) + ## I(log_income^2), data = heights2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -77.098 -16.265 -2.331 15.587 91.339 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.665e+01 1.598e+00 10.419 \u0026lt; 2e-16 *** ## marital_combmarried 8.340e+00 7.993e-01 10.434 \u0026lt; 2e-16 *** ## marital_combOther -2.604e+00 1.218e+00 -2.137 0.0326 * ## marital_combdivorced 4.262e+00 9.061e-01 4.703 2.61e-06 *** ## height2 7.488e-01 7.967e-02 9.399 \u0026lt; 2e-16 *** ## education2 5.467e+00 1.173e-01 46.609 \u0026lt; 2e-16 *** ## log_income -3.828e-01 8.307e-02 -4.609 4.12e-06 *** ## weight2 -4.652e-02 8.046e-03 -5.782 7.72e-09 *** ## I(education2^2) 5.535e-02 2.488e-02 2.224 0.0262 * ## I(height2^2) -3.018e-02 1.392e-02 -2.169 0.0302 * ## I(weight2^2) 4.135e-04 8.359e-05 4.947 7.73e-07 *** ## I(log_income^2) 2.177e-01 2.018e-02 10.791 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 22.33 on 6633 degrees of freedom ## (361 observations deleted due to missingness) ## Multiple R-squared: 0.4102, Adjusted R-squared: 0.4092 ## F-statistic: 419.3 on 11 and 6633 DF, p-value: \u0026lt; 2.2e-16 Let’s see if this improved our model fit.\nplot(afqt_alt) We could also attempt to convert the dependent variable (in a percentile metric) to a z-score.\nheights2 \u0026lt;- heights2 %\u0026gt;% mutate( afqt2 = ifelse(afqt == 0, .00001, ifelse(afqt == 100, 99.9999999, afqt)), afqt3 = ifelse(afqt %in% c(0, 100), NA, afqt), afqt_z = qnorm(afqt2/100), afqt_z3 = qnorm(afqt3/100) ) afqt_alt2 \u0026lt;- lm(afqt_z3 ~ marital_comb + height2 + education2 + log_income + weight2 + education2_quad + height2_quad + weight2_quad + log_income_quad, data = heights2) summary(afqt_alt2) ## ## Call: ## lm(formula = afqt_z3 ~ marital_comb + height2 + education2 + ## log_income + weight2 + education2_quad + height2_quad + weight2_quad + ## log_income_quad, data = heights2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8910 -0.5105 -0.0064 0.4966 3.2479 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.119e+00 5.576e-02 -20.069 \u0026lt; 2e-16 *** ## marital_combmarried 2.942e-01 2.782e-02 10.576 \u0026lt; 2e-16 *** ## marital_combOther -9.070e-02 4.242e-02 -2.138 0.032536 * ## marital_combdivorced 1.701e-01 3.152e-02 5.395 7.09e-08 *** ## height2 2.506e-02 2.769e-03 9.051 \u0026lt; 2e-16 *** ## education2 1.933e-01 4.133e-03 46.769 \u0026lt; 2e-16 *** ## log_income -1.075e-02 2.897e-03 -3.710 0.000209 *** ## weight2 -1.640e-03 2.800e-04 -5.859 4.88e-09 *** ## education2_quad -5.433e-04 8.880e-04 -0.612 0.540682 ## height2_quad -1.316e-03 4.838e-04 -2.720 0.006545 ** ## weight2_quad 1.508e-05 2.906e-06 5.192 2.15e-07 *** ## log_income_quad 7.100e-03 7.042e-04 10.083 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.7745 on 6588 degrees of freedom ## (406 observations deleted due to missingness) ## Multiple R-squared: 0.4088, Adjusted R-squared: 0.4078 ## F-statistic: 414.1 on 11 and 6588 DF, p-value: \u0026lt; 2.2e-16 plot(afqt_alt2)  glm function The glm function behaves much like the lm function. The only major difference in model logistics is that we will now need to specify a family argument. This family argument will depend on the type of model being fitted. We are going to perform a logistic regression, therefore this family will be binomial.\nThe data we will use is from Kaggle and is data from Titanic, more specifically the data has characteristics on the passengers and whether they survived the shipwreck or not. You can get a sense of the variables from this website: https://www.kaggle.com/c/titanic/data.\nlibrary(titanic) titanic \u0026lt;- bind_rows(titanic_test, titanic_train) head(titanic) ## PassengerId Pclass Name Sex Age ## 1 892 3 Kelly, Mr. James male 34.5 ## 2 893 3 Wilkes, Mrs. James (Ellen Needs) female 47.0 ## 3 894 2 Myles, Mr. Thomas Francis male 62.0 ## 4 895 3 Wirz, Mr. Albert male 27.0 ## 5 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female 22.0 ## 6 897 3 Svensson, Mr. Johan Cervin male 14.0 ## SibSp Parch Ticket Fare Cabin Embarked Survived ## 1 0 0 330911 7.8292 Q NA ## 2 1 0 363272 7.0000 S NA ## 3 0 0 240276 9.6875 Q NA ## 4 0 0 315154 8.6625 S NA ## 5 1 1 3101298 12.2875 S NA ## 6 0 0 7538 9.2250 S NA Suppose we were interested in fitting a model that predicted whether a passenger survived or not. Using lm is not appropriate as the dependent variable is not continuous, rather it is dichotomous. Using logistic regression is more appropriate here.\nsurv_mod \u0026lt;- glm(Survived ~ factor(Pclass) + Fare + Sex + Age, data = titanic, family = binomial) summary(surv_mod) ## ## Call: ## glm(formula = Survived ~ factor(Pclass) + Fare + Sex + Age, family = binomial, ## data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7393 -0.6788 -0.3956 0.6486 2.4639 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 3.7225052 0.4645113 8.014 1.11e-15 *** ## factor(Pclass)2 -1.2765903 0.3126370 -4.083 4.44e-05 *** ## factor(Pclass)3 -2.5415762 0.3277677 -7.754 8.89e-15 *** ## Fare 0.0005226 0.0022579 0.231 0.817 ## Sexmale -2.5185052 0.2082017 -12.096 \u0026lt; 2e-16 *** ## Age -0.0367302 0.0077325 -4.750 2.03e-06 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 964.52 on 713 degrees of freedom ## Residual deviance: 647.23 on 708 degrees of freedom ## (595 observations deleted due to missingness) ## AIC: 659.23 ## ## Number of Fisher Scoring iterations: 5 It is common to interpret these in terms of probability, we can do this with the following bit of code:\nprob_mod \u0026lt;- titanic %\u0026gt;% select(Survived, Pclass, Fare, Sex, Age) %\u0026gt;% na.omit() %\u0026gt;% mutate( probability = predict(surv_mod, type = \u0026#39;response\u0026#39;) ) head(prob_mod, n = 15) ## Survived Pclass Fare Sex Age probability ## 419 0 3 7.2500 male 22 0.10509514 ## 420 1 1 71.2833 female 38 0.91404126 ## 421 1 3 7.9250 female 26 0.55726903 ## 422 1 1 53.1000 female 35 0.92162960 ## 423 0 3 8.0500 male 35 0.06793029 ## 425 0 1 51.8625 male 54 0.32031425 ## 426 0 3 21.0750 male 2 0.19781236 ## 427 1 3 11.1333 female 27 0.54860408 ## 428 1 2 30.0708 female 14 0.87516354 ## 429 1 3 16.7000 female 4 0.73937738 ## 430 1 1 26.5500 female 58 0.83285937 ## 431 0 3 8.0500 male 20 0.11224887 ## 432 0 3 31.2750 male 39 0.05987747 ## 433 0 3 7.8542 female 14 0.66168470 ## 434 1 2 16.0000 female 55 0.60685621 We could now plot these probabilities to explore the effects.\nggplot(prob_mod, aes(Age, probability, color = Sex, linetype = Sex)) + geom_line(size = 1) + facet_grid(. ~ Pclass) + theme_bw() + xlab(\u0026quot;Age\u0026quot;) + ylab(\u0026quot;Probability of Survival\u0026quot;)  ","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"def1faa6733c48f030283d2aaad0f315","permalink":"https://psqf6250.brandonlebeau.org/rcode/misc-model/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/rcode/misc-model/","section":"rcode","summary":"Miscellaneous Modeling Topics","tags":null,"title":"Miscellaneous Modeling Topics","type":"book"},{"authors":null,"categories":null,"content":"Quiz 1 can be taken on ICON, due January 30th, 2022. The quiz covers content from Week 1.\nQuiz 1 Link\n","date":1643068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643068800,"objectID":"ac83706d5690d70e2da1268514b156a4","permalink":"https://psqf6250.brandonlebeau.org/assignments/quizzes/quiz1/","publishdate":"2022-01-25T00:00:00Z","relpermalink":"/assignments/quizzes/quiz1/","section":"assignments","summary":"Quiz 1 can be taken on ICON, due January 30th, 2022. The quiz covers content from Week 1.\nQuiz 1 Link","tags":null,"title":"Quiz 1","type":"book"},{"authors":null,"categories":null,"content":"Quiz 2 can be taken on ICON, due February 6th, 2022. The quiz covers content from Week 1.\nQuiz 2 Link\n","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643673600,"objectID":"89e911fba74fe6a1dc69de07ae2db619","permalink":"https://psqf6250.brandonlebeau.org/assignments/quizzes/quiz2/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/assignments/quizzes/quiz2/","section":"assignments","summary":"Quiz 2 can be taken on ICON, due February 6th, 2022. The quiz covers content from Week 1.\nQuiz 2 Link","tags":null,"title":"Quiz 2","type":"book"},{"authors":null,"categories":null,"content":"Quiz 3 can be taken on ICON, due February 20th, 2022. The quiz covers content from Week 4.\nQuiz 3 Link\n","date":1644364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644364800,"objectID":"ffe02f7f4b85dd056013551821b59d03","permalink":"https://psqf6250.brandonlebeau.org/assignments/quizzes/quiz3/","publishdate":"2022-02-09T00:00:00Z","relpermalink":"/assignments/quizzes/quiz3/","section":"assignments","summary":"Quiz 3 can be taken on ICON, due February 20th, 2022. The quiz covers content from Week 4.\nQuiz 3 Link","tags":null,"title":"Quiz 3","type":"book"},{"authors":null,"categories":null,"content":"Quiz 4 can be taken on ICON, due March 13th, 2022. The quiz covers content from Week 5 and Week 6.\nQuiz 4 Link\n","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"c160bc7b80914ff7c52bbe5d6dff1a09","permalink":"https://psqf6250.brandonlebeau.org/assignments/quizzes/quiz4/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/assignments/quizzes/quiz4/","section":"assignments","summary":"Quiz 4 can be taken on ICON, due March 13th, 2022. The quiz covers content from Week 5 and Week 6.\nQuiz 4 Link","tags":null,"title":"Quiz 4","type":"book"},{"authors":null,"categories":null,"content":"Quiz 5 can be taken on ICON, due March 13th, 2022. The quiz covers content from Week 7 and Week 8.\nQuiz 5 Link\n","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"cda06e7833bad261249d58564bf0c9c1","permalink":"https://psqf6250.brandonlebeau.org/assignments/quizzes/quiz5/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/assignments/quizzes/quiz5/","section":"assignments","summary":"Quiz 5 can be taken on ICON, due March 13th, 2022. The quiz covers content from Week 7 and Week 8.\nQuiz 5 Link","tags":null,"title":"Quiz 5","type":"book"},{"authors":null,"categories":null,"content":"Course Project Proposal - 5 pts Due April 3rd, by 11:59 pm\nA short, one page at most, description of the project you intend to complete for this course. This project proposal should include the following aspects:\n Brief discussion of the data used Statement of research question(s) to explore The type of model used Rationale for the model (i.e. why will this model help answer the research questions?)  Although not necessary, it is encouraged to discuss your project with the instructor prior to submission.\nPlease also reference the project requirements as you plan the project proposal.\n","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643673600,"objectID":"33e546dddf65d9f08ab4f73b7d6e74b1","permalink":"https://psqf6250.brandonlebeau.org/assignments/project/project-proposal/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/assignments/project/project-proposal/","section":"assignments","summary":"Course Project Proposal - 5 pts Due April 3rd, by 11:59 pm\nA short, one page at most, description of the project you intend to complete for this course. This project proposal should include the following aspects:","tags":null,"title":"Course Project Proposal","type":"book"},{"authors":null,"categories":null,"content":"Course Project - 25 pts Due May 12th by 11:59 pm\nProject Details The goal of this project is to do an analysis from start to finish. The study can come from data/project you are already working on (for a thesis or project) or you can design a brand new analysis on a topic that you are interested in. For this project, you will find some data you are interested in, analyze the data, and do a short write-up of the methods and results. The main prerequisite is that the analysis should be conducted in R.\nProject Length The length of these will likely vary, but I envision this to be in the 3 to 8 page range.\nStatistical Output Do NOT include raw output from R in the final paper (e.g. do not include output directly from summary()). Rather, include statistics or figures you wish to include in formatted tables or directly in the text. Raw statistical output from R will result in a 10 point grade reduction.\nThings to include Below is a list of things to include in your write-up:\n  Brief literature review \u0026ndash; discuss why this topic interests you and any gaps in the literature. This is not intended to be a true literature review, I\u0026rsquo;m envisioning a few paragraphs giving a very broad overview, paying more attention to gaps in knowledge.\n  State your research question(s)\n  Plan out the methods to answer the research question(s). Talk about the dependent and independent variables, the analysis you are going to conduct, the sample and population, and hypotheses you are testing.\n  Report descriptive statistics (graphical and/or summary statistics). Talk about the distribution of the variables and interesting observations that came about from a descriptive analysis. For example, there may be discussion of missing data, extreme values that needed to be removed, variation across groups, or descriptive differences in location across groups.\n  Now perform your analysis. Make sure to talk about assumptions and if they were met for the method you performed. Report the results of the test in statistical terms.\n  Include at least one graphic summarizing the results of the inferential method you used to answer the research question.\n  Lastly, discuss the research conclusions. This may include how the results may impact practice, policy, or basic knowledge on the topic generally. Think about any limitations this study may have had and how this study may benefit the knowledge of the topic you are studying.\n  Please include a write up document (e.g. Word, HTML, PDF) and also include your analysis scripts (e.g. R script or Rmd) when uploading the final assignment to ICON.\n  ","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643673600,"objectID":"f5829e95fdc9745658527f73484a7d6c","permalink":"https://psqf6250.brandonlebeau.org/assignments/project/project/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/assignments/project/project/","section":"assignments","summary":"Course Project - 25 pts Due May 12th by 11:59 pm\nProject Details The goal of this project is to do an analysis from start to finish. The study can come from data/project you are already working on (for a thesis or project) or you can design a brand new analysis on a topic that you are interested in.","tags":null,"title":"Course Project","type":"book"}]